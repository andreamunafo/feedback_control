[
  {
    "objectID": "root_locus.html",
    "href": "root_locus.html",
    "title": "Root Locus",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport control"
  },
  {
    "objectID": "root_locus.html#typical-design-scenario",
    "href": "root_locus.html#typical-design-scenario",
    "title": "Root Locus",
    "section": "Typical design scenario",
    "text": "Typical design scenario\n\nSystem with multiple known parameters, and one unknown or varying parameter (K)\nThe unknown parameter makes it impossible to know if system is stable or if meets our design criteria\n\nFor example:\n\\[\nG(s) = \\frac{s^2 +s + 1}{s^3 + 4s^2 + Ks+1}\n\\]\n\nChanging \\(K\\) changes the locations of the poles\nPoles are in fact values of \\(s\\) s.t.: \\(s^3 + 4s^2 + Ks+1=0\\)\n\nAs control engineers we have two questions at this point: - How to design a system that meets the requirements: - What value of \\(K\\) should I choose to meet the requirements (i.e., that places the poles at the correct location in the \\(s\\) plane) - What is the effect of variations on a control system that has been already designed: - How sensitive is the system to a value of \\(K\\) that is slightly different than what we have estimated (or predicted).\nOne way: pick a random value of \\(K\\) and plot the poles in the \\(s\\) plane.\nFor example, if \\(K=0\\):\n\nK=0\npoles = np.roots([1, 4, K, 1])\nprint('Poles for K={}: {}'.format(K, poles))\n\nPoles for K=0: [-4.06064703+0.j         0.03032351+0.4953248j  0.03032351-0.4953248j]\n\n\n\nplt.plot(np.real(poles), np.imag(poles), marker='x', markersize=10, linestyle='', linewidth=3, markeredgewidth=3)\nplt.grid()\nplt.xlabel('Real');\nplt.ylabel('Imag');\n\n\n\n\nWe can do it for multiple values of K, calcolating the root of \\(s^3 + 4s^2 + Ks+1=0\\), for \\(K \\rightarrow \\infty\\) (or until we get to a high enough value of \\(K\\)).\n\npoles = []\nK_range = np.arange(0, 10, 0.5)\nfor K in K_range:\n    poles.append(np.roots([1, 4, K, 1]))\n    # print('Poles for K={}: {}'.format(K, poles[-1]))\n\nAnd plot it:\n\nplt.plot(np.real(poles), np.imag(poles), marker='x', markersize=10, linestyle='', color='k', markeredgewidth=3)\nplt.grid()\nplt.xlabel('Real')\nplt.ylabel('Imag');\n\n\n\n\nConnecting all these points together is a drawing of how all these poles move through the \\(s\\)-plane as we vary \\(K\\). This is what the Root Locus method does.\n\nInterpreting the location of the poles in the s-plane\n\nThe axis of the \\(s\\)-plane are: Real component (\\(\\sigma\\)) along the x-axis, and Imag component (\\(\\omega\\)) on the y-axis.\nEach location of the \\(s\\)-plane corresponds to a specific waveform in the time domain through equation: \\(e^{st}\\)\nIf \\(s\\) is real: this corresponds to exponential growth (\\(s\\)>0) or decay (\\(s\\)<0).\n\nThe further we are in the left half plane the faster the waveform decays\nThe further we are in the right half plane the faster the waveform grows\n\nIf \\(s\\) is complex: this corresponds to a pure oscillatory motion (sinusoidal) in the time domain.\n\nAlways in pair (complex conjugate of each other). This is due to having to deal with real time.\nThe further the are along the \\(\\omega\\) axis, the faster the oscillation\n\nIf \\(s\\) is a combination of real and imaginary values mix both exponential and sinusoidal motion.\n\nSee also notebook 05_System_response.\nWhen we anlayse a system: - Both poles and zeros are needed to characterise the forced response to an input - Only the poles are needed to dictate the unforced response of the system to a set of initial conditions - This links directly to the stability of the system\nKnowing the location of the poles in the \\(s\\)-plane: - Determines the stability, - Determines if the response is oscillatory (and at what frequency), or exponential (and at what rate) - And more: for example makes it possible to determine the damping ratio \\(\\zeta=cos(\\theta)\\): two poles on the same \\(\\theta\\) line would have the same damping ratio\n\n\n\n\n\n\n\n\n\n\n\nA note on the damping ratio\n\nNormalised unitless value (\\(\\zeta\\))\n\\(\\zeta=0\\): no dumping. The pole is on the imaginary axis\n\\(\\zeta=1\\): perfectly dumped. The pole is exponential and hence on the real axis.\n\\(\\zeta \\in (0, 1)\\): damped oscillations (the poles are somewhere in the s-plane).\n\\(\\zeta>1\\): over dumped (still on the real axis)."
  },
  {
    "objectID": "root_locus.html#damping-ratio-natural-frequency-and-pole-location",
    "href": "root_locus.html#damping-ratio-natural-frequency-and-pole-location",
    "title": "Root Locus",
    "section": "Damping ratio, natural frequency and pole location",
    "text": "Damping ratio, natural frequency and pole location\nLet’s consider a second order system:\n\\[\n\\frac{\\omega_n^2}{s^2+2\\zeta\\omega_n s+ \\omega_n^2}\n\\]\nThe characteristic equation is:\n\\[\ns^2+2\\zeta\\omega_n s+ \\omega_n^2 = 0\n\\]\nand when we solve it:\n\\[\ns_{1,2} = \\frac{-2\\zeta\\omega_n \\pm \\sqrt{4\\zeta^2\\omega_n^2-4\\omega_n^2}}{2} = -\\zeta\\omega_n \\pm \\omega_n\\sqrt{(\\zeta^2-1)}\n\\]\n\nif we assume \\(\\zeta < 1\\), we have complex roots\n\n\\[\ns_{1,2} = -\\zeta\\omega_n \\pm \\omega_n\\sqrt{(1-\\zeta^2)}j\n\\]\n\n<td> <img src=\"img/root-locus-damping-ratio-mit-cropped-scaling.png\" alt=\"root-locus-damping-ratio-mit.png\" style=\"width: 500px;\"/> </td>\n\n\n\n\n\n\n\n\n\nIf we hold \\(\\zeta\\) constant, the poles scale with \\(\\omega_n\\)\nWhat is the angle \\(\\theta\\) (sometime I will also call it \\(\\phi\\))?\n\n\\[\n\\sqrt{\\zeta^2\\omega_n^2 + \\omega_n^2(1-\\zeta^2)} cos(\\theta) = \\omega_n \\zeta\n\\]\n\\[ \\Downarrow \\]\n\\[\ncos(\\theta) = \\frac{\\omega_n \\zeta}{\\sqrt{\\zeta^2\\omega_n^2 + \\omega_n^2(1-\\zeta^2)} }\n\\]\n\\[ \\Downarrow \\]\n\\[\ncos(\\theta) = \\frac{\\omega_n \\zeta}{\\sqrt{\\omega_n^2(\\zeta^2 +1 -\\zeta^2)} } = \\frac{\\omega_n \\zeta}{\\omega_n} = \\zeta\n\\]\n\nRemember that we assumed $< 1 $\nWhen \\(\\zeta > 1\\), the two roots separate along the real axis: we have two first order real poles.\n\nWhy are we interested in this?\n\nRequirements most of the time come in the form of:\n\nDamping ratio (e.g. greater than some value)\nNatural frequency\nTime to half, time to rise, etc.\nOr any combination of these\n\n\nAnd these requirements translate into constraints on the position of the poles."
  },
  {
    "objectID": "root_locus.html#a-simple-design-problem",
    "href": "root_locus.html#a-simple-design-problem",
    "title": "Root Locus",
    "section": "A simple design problem",
    "text": "A simple design problem\nSuppose you need to design a spring-dumper-mass system:\n\n\n\n\n\n\n\n\n\n\nYou are given the spring (\\(k=1\\)) and the mass (\\(m=1\\))\nYou can choose the damper that is needed to achieve the requirements\n\nSystem shall have \\(\\xi \\ge 0.75\\).\n\n\nThe transfer function of the system is:\n\\[\n\\frac{Z(s)}{F(s)} = \\frac{1}{s^2+bs+1}\n\\]\nThe root locus method gives a method to verify how the poles move as we change the value of the parameter \\(b\\) from 0 to infinity.\nLet’s use the Python again to see what the Root Locus looks like in this case:\n\nfig, axs = plt.subplots(1,1, figsize=(10,5))\n\npoles = []\nb_range = np.arange(0, 10, 0.001)\nfor b in b_range:\n    poles.append(np.roots([1, b, 1]))\n    \nplt.plot(np.real(poles), np.imag(poles), marker='.', markersize=1, linestyle='', color='k', markeredgewidth=1)\nplt.plot(np.real(poles[0]), np.imag(poles[0]), marker='x', color='g', markeredgewidth=3, linestyle='')\nplt.plot(np.real(poles[-1]), np.imag(poles[-1]), marker='x', color='r', markeredgewidth=3, linestyle='')\n\n# desired damping lines\nphi = np.arccos(0.75)\nxi = 0.75\ntan_theta = np.sqrt(1-xi**2)/xi\n\nx_range = np.linspace(-2, 0)\nplt.plot(x_range, -tan_theta*x_range, linewidth=3, color='g')\nplt.plot(x_range, tan_theta*x_range, linewidth=3, color='g')\n\n# Choose b = 1.5\nb = 1.5\npoles_desired = np.roots([1, b, 1])\nprint('Desired poles (magenta):', poles_desired)\nplt.plot(np.real(poles_desired), np.imag(poles_desired), \n         marker='x', markersize=10, linestyle='', color='m', markeredgewidth=3)\n\nplt.xlabel('real');\nplt.ylabel('imag');\n\nDesired poles (magenta): [-0.75+0.66143783j -0.75-0.66143783j]\n\n\n\n\n\n\nWe start from the imaginary axis (\\(b=0\\)), as we would expect if we had no dumping: we expect oscillations\nAs we increase the damping the poles move in the left half plane and towards the real axis, until for the critical dumaping value they hit the real axis, where they split and separate on the real line\nWe can now plot the desired damping lines (\\(\\xi\\ge0.75\\))\n\nFor example, if we choose \\(b=1.5\\), we have poles: [-0.75+0.66143783j -0.75-0.66143783j], which meet our requirements.\nHowever…what happens if one of the parameters changes? For example, the spring might be sensitive to temperature variation: - Hot: 0.9 K_nominal - Cold: 1.1 K_nominal\nAnd we can again use the Root Locus method, but this time using:\n\\[\n\\frac{Z(s)}{F(s)} = \\frac{1}{s^2+1.5s+k}\n\\]\nfor \\(K \\in [0.9k, 1.1k]\\)\n\nfig, axs = plt.subplots(1,1, figsize=(10,5))\n\npoles = []\nk_range = np.arange(0.9, 1.1, 0.001)\nfor k in k_range:\n    poles.append(np.roots([1, 1.5, k]))\n    \nplt.plot(np.real(poles), np.imag(poles), marker='.', markersize=1, linestyle='', color='k', markeredgewidth=1)\nplt.plot(np.real(poles[0]), np.imag(poles[0]), marker='.', color='g', markeredgewidth=5, linestyle='', label='Hot Spring')\nplt.plot(np.real(poles[-1]), np.imag(poles[-1]), marker='.', color='r', markeredgewidth=5, linestyle='', label='Cold Spring')\nplt.legend()\n# desired damping lines\nphi = np.arccos(0.75)\nxi = 0.75\ntan_theta = np.sqrt(1-xi**2)/xi\n\nx_range = np.linspace(-1, -0.5)\nplt.plot(x_range, -tan_theta*x_range, linewidth=3, color='g')\nplt.plot(x_range, tan_theta*x_range, linewidth=3, color='g')\n\n# Choose b = 1.5\nb = 1.5\npoles_desired = np.roots([1, b, 1])\nprint(poles_desired)\nplt.plot(np.real(poles_desired), np.imag(poles_desired), \n         marker='x', markersize=10, linestyle='', color='m', markeredgewidth=3)\n\nplt.grid()\nplt.xlabel('real')\nplt.ylabel('imag');\n\n[-0.75+0.66143783j -0.75-0.66143783j]\n\n\n\n\n\n\nWhen the spring is cold we do not meet our requirements!\nWe might need to find a way to restrict the temperature variation, redesign our system or have a warning that performance might degrade depending on temperature. Depends on the application."
  },
  {
    "objectID": "root_locus.html#classic-set-up-of-the-root-locus-problem",
    "href": "root_locus.html#classic-set-up-of-the-root-locus-problem",
    "title": "Root Locus",
    "section": "Classic Set up of the Root Locus Problem:",
    "text": "Classic Set up of the Root Locus Problem:\n\n\n\n\n\n\n\n\n\n\nGain in the forward path\nUnity feedback\n\nThe closed transfer function is:\n\\[\nG_{cc} = \\frac{KG(s)}{1+KG(s)}\n\\]\n\nGiven that we are interested in finding the poles of the closed loop system we only need: \\(1+KG(s)=0\\)\nThis is the form of the characteristic equation that we need in order to apply the rules that we are going to discuss.\nNote: Matlab might draw the root locus starting from a slightly different form, and considers the gain in the feedback path. Note that the denominator does not change (the closed loop system will behave differently to forced inputs but this is not important for our objectives when we plot the root locus). To have the Matlab representation the same, we need to multiply the input by \\(K\\).\nFrom a stability perspective there is no difference\n\n\n\n\n\n\n\n\n\n\n\nNote: Before we analysed\n\n\\[\nG(s) = \\frac{s^2 +s + 1}{s^3 + 4s^2 + Ks+1}\n\\]\nwhich is not in the form that we would like.\n\nWe can translate it back to the form we need.\nWe are looking for the roots of the characteristic equation and how they mode as we change \\(K\\).\n\nSteps: - Write the characteristic equation: \\(s^3 + 4s^2 + Ks+1=0\\) - Group all \\(K\\) terms: \\(s^3 + 4s^2 +1 + (Ks) =0\\) - Divide by the non-\\(K\\) terms to obtain the form we need:\n\\[\ns^3 + 4s^2 +1 + (Ks) = 0 \\Rightarrow \\frac{s^3 + 4s^2 +1}{s^3 + 4s^2 +1} + K\\frac{s}{s^3 + 4s^2 +1} = 1 + K\\frac{s}{s^3 + 4s^2 +1}\n\\]\n\nNote that we are now considering a new \\(G(s)=\\frac{s}{s^3 + 4s^2 +1}\\)\nWe can use the Root Locus to analyse how any change in one of the parameters affect the results, not only the gain in the forward path. It is really powerful!.\n\n\nHow to sketch the Root Locus: Drawing Rules\n\nWe use 10 drawing rules\n\nStart from: \\[\n1+KG(s)=0 \\Rightarrow 1+K\\frac{Q(s)}{P(s)} = 0\n\\]\n\nWe have re-written \\(G(s)\\) as a ratio of two polynomials\nAlways start from this form.\n\n\n\nRULE 1\n\nThere are \\(n\\) lines (loci) in the Root Locus, where \\(n\\) the maximum between the degree of the denominator and of the numerator. - We have as many lines as the number of poles or zeros (whichever is greater)\n\n\ne.g. 3 poles and 2 zeros: 3 lines\n\n\n\nRULE 2\n\nAs \\(K\\) increases from 0 to \\(\\infty\\), the roots move from the poles of \\(G(s)\\) to the zeros of \\(G(s)\\).\n\nLet’s unpick this rule a bit and start from: \\[\n1+K\\frac{Q(s)}{P(s)} = 0 \\Rightarrow P(s)+KQ(s)=0\n\\]\n\nPoles are when \\(P(s)=0\\)\nZeros are when \\(Q(s)=0\\)\n\nWhen \\(K=0\\), the \\(P(s)+KQ(s)=0\\) we are left with \\(P(s)=0\\). So the poles of the closed loop system start at the poles of \\(G(s)\\).\nWhen \\(K\\rightarrow\\infty\\), we put more importance to \\(Q(s)\\), which dominates our characteristic equation: the root locus ends at the zeros of \\(G(s)\\).\n\nRule 2 says: As \\(K\\) increases, the closed loop poles move from the open loop poles to the open loop zeros.\nWhat happens when we have a different number of poles and zeros?\ncase 1: If \\(\\text{order}(Q(s))=\\text{order}(P(s))\\): each have a pair, and the roots travel from one pole to a zero\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, 4],[1, 1]))\n\n\n\n\n\ncase 2: If \\(\\text{order}(P(s))>\\text{order}(Q(s))\\), the lines from the extra poles would go to infinity.\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, -1],[1, -2, -8]))\n\n\n\n\n\ncase 3: If \\(\\text{order}(Q(s))>\\text{order}(P(s))\\), the extra lines come from infinity.\n\nFor example: \\[\nG(s) = \\frac{s^2-s}{s-3}\n\\]\n\n# [rlist, klist] = control.root_locus(control.tf([1, -1, 0], [1, -3]))  \n# For some reason python control lib does not plot it..\n# The figure below is done with Matlab:\n# >> rlocus(tf([1, -1, 0], [1, -3]))\n\n\n\n\n\n\n\n\n\n\n\n\nRULE 3\n\nWhen roots are complex they occur in conjugate pairs (we saw this already multiple times! - it is due to having real systems) and they move in pairs. - The Root Locus is symmetric around the real line\n\n\n\nRULE 4\n\nAt no time will the same root cross over its path (no loop): no value of \\(s\\) will correspond to more than one value of \\(K\\).\n\n\nNote that paths of two different roots are allowed to intersect.\n\nFor example if we have poles in \\(s=0\\) and \\(s=1\\):\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1], [1, 1, 0]))\n\n\n\n\n\n\nUsing the first 4 rules - example:\n\\[\nG(s) = \\frac{s}{(s-3)(s+2-2j)(s+2+2j)}\n\\]\n\nWe have 3 poles, so we have 3 lines\n3 poles, and 1 zeros: 2 lines go to infinity\nThe two complex conjugate roots move in pairs (mirrored about the real axis).\nEach root will not have a path that creates loop\n\nLet’s now plot the Root Locus with the Python Control Library\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, 0],[1, 1, -4, -24]))\n\n\n\n\nWe can check our roots:\n\nnp.roots([1, 1, -4, -24])\n\narray([-2.+2.j, -2.-2.j,  3.+0.j])\n\n\n\n\nRULE 5\n\nThe portion of the real axis to the left of an odd number of open loop poles and zeros (critical frequencies) are part of the loci.\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, +2],[1, 1]))\nplt.grid()\n\n\n\n\nNote that if we swap the location of the pole and zero, the root locus is still between them, but its direction is reversed: it is always from the open loop pole to the open loop zero.\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, 1],[1, 2]))\n\n\n\n\nIf we have three poles:\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, 6, 11, 6]))\n\n\n\n\nNote that: - To the left of the third pole, the root locus goes to infinity - Between the first and the second critical frequencies the lines come together and then they go off the real axis. We will see in a minute where they go.\n\n\nRULE 6\n\nLines leave (or break out) and enter (break in) the real line at \\(90^o\\) (in search of an open loop zero)\n\n\n\nRULE 7\n\nIf there are not enough poles or zeros to make a pair then the extra lines go to or come from infinity\n\nFor example:\n\nOne pole, no zero: 1 extra pole: the left side of the pole is part of the locus and it goes to infinity\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, 1]))\n\n\n\n\n\nTwo poles and no zeros, 2 extra poles: the left side of an odd number of poles in part of the locus and they leave the real axis at 90 deg towards infinity\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, 3, 2]))\n\n\n\n\n\nTwo poles and one zero, 1 extra pole\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, 4],[1, 3, 2]))\nplt.xlim((-10, 0));\n\n\n\n\n\nWhat happens when we have two extra zeros?\nTwo lines would be coming in from infinity\n\nGiven that we have lines going to (or coming from) infinity, at what angles do they do that?\n\n\nRULE 8\n\nLines go to infinity along asymptotes\n\n\nWe have as many asymptotes as many unmatched pole-zero pairs (\\(n-m\\))\nThe angles of the asymptotes is (in deg):\n\n\\[\n\\Phi_A = \\frac{2q+1}{n-m} \\cdot 180\n\\]\nWhere \\(n-m\\) is the number of unmatched pairs (#poles-#zeros), and \\(q=0, 1, 2, ..., (n-m-1)\\)\n\nThe centroid of the asymptotes is:\n\n\\[\nC = \\frac{\\sum{\\text{Finite poles}} - \\sum{\\text{Finite zeros}}}{n-m}\n\\]\n\nThis is the point on the real axis where all asymptotes come together.\n\nFor example:\n\n\\(n-m=1\\) (1 line that goes to infinity)\n\n\\[\n\\Phi_A = \\frac{2\\cdot 0 + 1}{1} \\cdot 180 = 180\n\\]\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, 1]))\n\n\n\n\nFor example:\n\n\\(n-m=2\\) (2 lines that go to infinity)\n\n\\[\n\\Phi_{A_0} = \\frac{2\\cdot 0 + 1}{2} \\cdot 180 = 90\n\\]\n\\[\n\\Phi_{A_1} = \\frac{2\\cdot 1 + 1}{2} \\cdot 180 = 270 = -90\n\\]\nwith centroid (this determines exactly where they go off to \\(\\pm\\) 90 deg):\n\\[\nC = \\frac{\\sum{\\text{Finite poles}} - \\sum{\\text{Finite zeros}}}{n-m} = \\frac{(-2-1)-0}{2} = -1.5\n\\]\n\nhalf way between the poles in this case where we only have two poles.\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, 3, 2]))\n\n\n\n\n\n\nWhat would you expect if we added one more pole and one more zero to the left of the -2 pole?\n\n\n!cat answers/solution_root_locus_1\n\n- Still two lines going to infinity\n- Asymptotes +- 90\n- Centroid would be shifted to the left\n\n\n\n\n\\(n-m=3\\) (3 lines that go to infinity)\n\n\\[\nG(s) = \\frac{1}{(s+1)(s+2)(s+3)}\n\\]\nCentroid:\n\\[\n\\frac{-3 -2 -1}{3} = -2\n\\]\nAngles:\n\\[\n\\Phi_{A_0} = 60\n\\]\n\\[\n\\Phi_{A_1} = 180\n\\]\n\\[\n\\Phi_{A_2} = -60\n\\]\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n\n[rlist, klist] = control.root_locus(control.tf([1],[1, 6, 11, 6]))\n\nx_range = np.linspace(-2, 0);\nplt.plot(x_range, np.tan(60/180*3.14)*x_range+np.tan(60/180*3.14)*(2), linestyle='--');\nplt.plot(x_range, np.tan(-60/180*3.14)*x_range+np.tan(-60/180*3.14)*(2), linestyle='--');\n\n\n\n\n\n\nRULE 9\n\nIf there is at least two lines to infinity, then the sum of all roots is constant\n\n\nThis does not help sketching the roots but helps us with intuition\n\nSuppose we have the system above:\n\\[\nG(s) = \\frac{1}{(s+1)(s+2)(s+3)}\n\\]\n\nThe sum of the roots is \\(-6\\). This is the sum of the roots when \\(K=0\\) (the roots start at the open loop roots).\nWhen increase \\(K\\) the roots starts too move, but their sum must still be \\(-6\\)\n\nFor example in the picture below, we can plot how the roots move for \\(K=0.29\\)\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n\n[rlist, klist] = control.root_locus(control.tf([1],[1, 6, 11, 6]))\n\nx_range = np.linspace(-2, 0)\nplt.plot(x_range, np.tan(60/180*3.14)*x_range+np.tan(60/180*3.14)*(2), linestyle='--')\nplt.plot(x_range, np.tan(-60/180*3.14)*x_range+np.tan(-60/180*3.14)*(2), linestyle='--')\n\nk_id=3\nprint('K=', klist[k_id])\nplt.plot(np.real(rlist[k_id]), np.imag(rlist[k_id]), marker='x', color='m', linestyle='', markeredgewidth=3)\n\nK= 0.28867513459481264\n\n\n\n\n\n\nWhat is important is that given that the sum is constant, we are destabilising the complex pair half as fast as how we move the real root\nThis is useful when trying to decide the gain of our system\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n\n[rlist, klist] = control.root_locus(control.tf([1],[1, 6, 11, 6]))\n\nx_range = np.linspace(-2, 0)\nplt.plot(x_range, np.tan(60/180*3.14)*x_range+np.tan(60/180*3.14)*(2), linestyle='--')\nplt.plot(x_range, np.tan(-60/180*3.14)*x_range+np.tan(-60/180*3.14)*(2), linestyle='--')\n\nfor k_id in range(3, 30, 2):\n    plt.plot(np.real(rlist[k_id]), np.imag(rlist[k_id]), marker='x', color='m', linestyle='', markeredgewidth=3)\n\n\n\n\n\nSo far we have always considered \\(K \\in [0, +\\infty]\\), let’s see instead when we would like to analyse a negative gain.\n\n\n\nRULE 10 - Negative K\n\nThe root locus when \\(K\\) goes from 0 to \\(-\\infty\\) can be obtained reversing RULE 5 and adding \\(+180^o\\) to the asymptote angles - For \\(K<0\\), the root locus is on the right of the odd critical frequencies\n\n\n\nA few more comments\nThese set of rules only provide a sketch of the root locus. For example we have not discussed where exactly is on the real axis the break in and break away points or the angle of departure for complex roots, or where the locus crosses the imaginary axis, or what is the gain associated to one specific point on the locus.\n\nUse references to go deeper in the topic (see 00_Syllabus)\nUse software tool such as the Matlab SISO tool or the Python Control Library."
  },
  {
    "objectID": "root_locus.html#a-simple-control-problem",
    "href": "root_locus.html#a-simple-control-problem",
    "title": "Root Locus",
    "section": "A simple control problem",
    "text": "A simple control problem\nWe have\n\\[\nG(s) = \\frac{1}{(s+1)(s-2)}\n\\]\nAnd the root locus is:\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1],[1, -1, -2]))\n\n\n\n\n\nSystem is unstable and there is no \\(K\\) that can stabilise it\nHow can we make this stable?\n\n\n\nGiven that we have two unmatched poles, they go off to inifinity with \\(\\pm90\\)\nHowever, we know that if have only one unmatched pair, the unmatched root would go to +180\nThis means that if we had one more zero to the left of our poles, we should obtain the desired behaviour.\n\nFor example, we could add a zero in \\(s=-2\\).\nLet’s use root_locus to confirm:\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\n[rlist, klist] = control.root_locus(control.tf([1, 2],[1, -1, -2]))\nplt.xlim((-10, 2));\n\n\n\n\n\nAdding an open loop zero and choosing the correct gain has stabilised the system\n\nTo be more precide we have started from this:\n\n\n\n\n\n\n\n\n\nand added a zero:\n\n\n\n\n\n\n\n\n\n\nThe controller now has a proportional controller \\((K)\\) and a derivative controller \\((s+2)\\)\nWe have designed a PD controller in the \\(s\\)-domain\n\n\nAdditional properties\n\nBrake-in and break-away\nCorrespond to those points along the real axis where the root locus goes in or away from the it. These points corresponds to multiple roots of the characteristic equation.\nLet’s write the characteristic equation \\(1+KG(s)=0\\) as:\n\\[\nf(s) = B(s) + KA(s) = 0\n\\]\n\\(f(s)\\) has multiple roots where \\(\\frac{d}{ds}f(s)=0\\)\nWe can see this condidering a root of order \\(r\\ge2\\): \\[\nf(s) = (s-s_1)^r(s-s_2)...\n\\]\nWe can then calculate \\(\\frac{d}{ds}f(s)=0\\) when \\(s=s_1\\) which gives us:\n\\[\n\\frac{d}{ds}f(s)\\Big|_{s=s_1}=0\n\\]\nThis means that the multiple roots of \\(f(s)\\) satisfy our equation.\nThis means:\n\\[\n\\frac{d}{ds}f(s) = \\frac{d}{ds}B(s) + K\\frac{d}{ds}A(s) = 0 \\rightarrow B'(s) + KA'(s) = 0\n\\]\nand if we plug this back in the characteristic equation:\n\\[\nB(s) + KA(s) = 0 \\rightarrow B(s) - \\frac{B'(s)}{A'(s)}A(s) = 0\n\\]\nwhich we can also write as:\n\\[\nB(s)A'(s) - B'(s)A(s) = 0\n\\]\nwhen we solve this equation with respect to \\(s\\) we obtain the points where multiple roots occur.\nLet’s also note that:\n\\[\nB(s) + KA(s) = 0 \\rightarrow K = -\\frac{B(s)}{A(s)}\n\\]\nand that:\n\\[\n\\frac{dK}{ds} = - \\frac{B(s)A'(s) - B'(s)A(s)}{A^2(s)}\n\\]\nIf \\(dK/ds\\) is set equal to zero, we get the same equation as before.\nIn summary to find the break-in/break-away points we can simply find the roots of:\n\\[\n\\frac{dK}{ds} = 0\n\\]\nNote that only those points that are on the root locus are actual breakaway/break-in points.\nStated differently, if at a point at which \\(dK/ds=0\\) the value of \\(K\\) takes a real positive value, then that point is an actual breakaway or break-in point (we have assumed \\(K>0\\)).\n\n\nIntersections with the imaginary axis\n\nThe points where the root locus intersects the imaginary axis can be determined using the Routh criterion: the parameters \\(K^*\\) and \\(\\omega^*\\) provided both the value of parameter \\(K = K^*\\) and the frequency \\(\\omega=\\omega^*\\) for which the root locus intersects the imaginary axis.\n\nAlternatively, we can also set \\(s=j\\omega\\) in the characteristic equation and set to zero the real and imaginary parts to solve for \\(\\omega\\) and \\(K\\).\n\n\n\n\nExample\nLet’s consider a negative feedback system:\n\n\n\n\n\n\n\n\n\nwhere \\[\nG(s) =  \\frac{K}{s(s+1)(s+2)}\n\\]\nwith \\(K\\ge0\\)\n\nDetermine the root loci on the real axis.\nDetermine the asymptotes of the root loci:\n\n\\[\n\\phi_A = \\frac{2q+1}{n-m} 180\n\\]\nwhere \\(q=0,1, n-m-1\\)\nWe obtain: \\(60\\), \\(180\\), \\(300\\)\nWith centroid:\n\\[\nc = \\frac{0-1-2}{3} = -1\n\\]\nThe asymptotes are almost parts of the root loci in regions very far from the origin.\n\nDetermine the breakaway points:\n\nLet’s consider the following characteristic equation \\(G(s)+1 = 0\\):\n\\[\nG(s)+1 = \\frac{K}{s(s+1)(s+2)}+1=0\n\\]\nThis means:\n\\[\nK = - (s^3 + 3s^2 + 2s)\n\\]\nSetting\n\\[\n\\frac{dK}{ds} = 0\n\\]\nwe obtain:\n\\[\n\\frac{dK}{ds} = -(3s^2+6s+2)=0\n\\]\nand:\n\\[\ns=-0.4226\\;\\;\\;\\;s=-1.5774\n\\]\n\nThe breakaway point must lie on a root locus between 0 and –1.\n\\(s=–0.4226\\) corresponds to the actual breakaway point.\n\\(s=–1.5774\\) is not on the root locus.\n\nEvaluation of the values of \\(K\\) corresponding:\n\n\\(K=0.3849\\), for \\(s=–0.4226\\)\n\\(K=-0.3849\\), for \\(s=–1.5774\\)\n\n\n## first point\ns = -0.4226\n\nK=-(s**3+3*s**2+2*s)\nprint(f'K={K} for s={s}')\n\n## second point\ns = -1.5774\n\nK=-(s**3+3*s**2+2*s)\nprint(f'K={K} for s={s}')\n\nK=0.38490017517599995 for s=-0.4226\nK=-0.3849001751759995 for s=-1.5774\n\n\nDetermine the intersection with the imaginary axis:\nThe characteristic equation is:\n\\[\ns^3 + 3s^2 + 2s + K = 0\n\\]\nWe can compute the Routh array as:\n\n\n\n\\(s^3\\)\n\\(1\\)\n\\(2\\)\n\n\n\\(s^2\\)\n\\(3\\)\n\\(K\\)\n\n\n\\(s^1\\)\n\\((6-K)/3\\)\n\n\n\n\\(s^0\\)\n\\(K\\)\n\n\n\n\nThe value of \\(K\\) that makes the \\(s^1\\) term in the first column equal zero is \\(K=6\\) (see notebook 09_Routh_hurwitz_criterion)\nThe crossing points can then be found solving the auxiliary polinomial obtained from the \\(s^2\\) row:\n\\[\n3s^2+K = 3s^2+6 = 0\n\\]\nwhich gives us:\n\\[\ns = \\pm\\sqrt{2}j\n\\]\n\nThe frequencies at the crossing points on the imaginary axis are thus \\(\\omega=\\pm\\sqrt{2}j\\).\nThe gain value corresponding to the crossing points is \\(K=6\\).\n\nAlternatively we can set \\(s=j\\omega\\) in the characteristic equation,e quate both the real part and the imaginary part to zero, and then solve for \\(\\omega\\) and \\(K\\).\nIn this case:\n\\[\n(j\\omega)^3 + 3(j\\omega)^2 + 2(j\\omega) + K = 0\n\\]\n\\[\n(K-3\\omega^2) + j(2\\omega-\\omega^3)=0\n\\]\n\\[\n(K-3\\omega^2) = 0, \\;\\;\\;\\; (2\\omega-\\omega^3) = 0\n\\]\nfrom which:\n\\(\\omega=\\pm\\sqrt{2}j\\) and \\(K=6\\)\nand\n\\(\\omega=0\\) and \\(K=0\\)\n\nsolve for system requirements\n\nWe can now determine a pair of dominant complex-conjugate closed loop poles such that the dumping ratio is \\(\\xi = 0.5\\).\n\nWe know that they must lie on a line passing through the origin and such that:\n\n\\[\ncos(\\theta) = \\xi \\rightarrow cos^{-1}(\\xi) = \\theta\n\\]\nfrom which:\n\\[\ncos^{-1}(\\xi) = 60^o\n\\]\nFrom the picture of the root locus we can obtain thatclosed-loop poles having \\(\\xi=0.5\\) are obtained approsimately as follows:\n\\[\ns=-0.3\\pm j0.58\n\\]\nWe can then determine the value of \\(K\\) that yields such poles from what is called the magnitude condition (see also end of notebook):\n\\[\nK = - (s^3 + 3s^2 + 2s)\n\\]\n\\[\n|K| = |- (s^3 + 3s^2 + 2s)|_{s=-0.3+ j0.58}\n\\]\nor \\(K=1.066\\)\n\ns=-0.3+0.58j\nnp.abs(s**3+3*s**2+2*s)\n\n1.0662593604484791\n\n\nKnowing \\(K=1.066\\) we can determine the third pole from:\n\\[\ns^3+3s^2+2s+1.066\n\\]\nas \\(s=-2.3399\\)\n\nnp.roots([1, 3, 2, 1.066])\n\narray([-2.33997629+0.j       , -0.33001185+0.5887719j,\n       -0.33001185-0.5887719j])\n\n\n\nNote that for \\(K=6\\), the dominant poles would be on the imaginary axis with \\(\\omega=\\pm j\\sqrt{2}\\) and the system will have sustained osciellations.\nFor \\(K>6\\) the dominant poles would be on the right-half plane and the system would be unstable."
  },
  {
    "objectID": "root_locus.html#yes-but-why-root-locus-development",
    "href": "root_locus.html#yes-but-why-root-locus-development",
    "title": "Root Locus",
    "section": "Yes, But Why? Root Locus Development",
    "text": "Yes, But Why? Root Locus Development\n\n\n\n\n\n\n\n\n\nRecall that the closed-loop characteristic equation is: \\[\n1 + KG(s) = 0\n\\]\nwhere \\(G(s) = G_{ol}(s)R(s)\\) is the open-loop transfer function.\n\nHow can we tell if an arbitrary point s = σ + jω lies on the root locus?\nWe seek conditions that determine whether s is a root of the characteristic equation.\n\\(s\\) is a root if \\(KG(s) = −1 + j0.\\)\n\nor in polar form:\n\\[\nKG(s) = |KG(s)| e^{j\\angle(KG(s))} = -1+j0=1\\cdot e^{j(2n+1)\\pi} \\;\\; \\text{for}\\;\\;  n=0,1,2,..\n\\]\nIn fact:\n\\[\n1+KG(s)=0 \\Rightarrow KG(s)=-1\n\\]\nSince this is a complex quantity, we can obtain a phase and a magnitude equation.\nThis tells us that for any point s = σ + jω on the root locus:\n\\[\n|KG(s)| = 1\n\\]\nand\n\\[\n\\angle (G(s)) = (2n + 1)\\pi\n\\]\nThis gives two important conditions:\n\nThe angle condition: \\(\\angle (G(s)) = (2n + 1)\\pi\\)\nThe magnitude condition: \\(|KG(s)|=1\\)\nThe angle condition is used to determine whether a point \\(s\\) lies on the root locus\nIf a point lies on the root locus, the magnitude condition is used to determine the gain \\(K\\) associated with that point, since \\(K = 1/ |G(s)|\\).\nValues of \\(s\\) that satisfies both the phase and magnitude equations are the roots of the characteristic equation, i.e., the closed loop poles.\nIf only the phase equation is satisfied we obtain the root locus.\n\nWhen \\(K < 0\\) (positive feedback) the phase relationship must be modified as:\n\\[\n\\angle (G(s)) = (2n)\\pi\n\\]\nNote that we can always use the angle condition to test if any point lies on the root locus\n\nDetermine the angle of departure from the complex-conjugate open-loop poles\nThe presence of a pair of complex-conjugate open-loop poles requires the determination of the angle of departure from these poles. Knowledge of this angle is important, since the root locus near a com- plex pole yields information as to whether the locus originating from the complex pole migrates toward the real axis or extends toward the asymptote.\nIf we choose a test point and move it in the very vicinity of the complex open-loop pole at \\(s=–p_1\\) , we find that the sum of the angular contributions from the pole at \\(s=p_2\\) and zero at \\(s=–z_1\\) to the test point can be considered remaining the same.\nIf the test point is to be on the root locus, then the sum of \\(\\phi'_1\\) (phase of the zero) , \\(–\\theta_1\\), and \\(–\\theta'_2\\) must be \\(\\pm 180^o(2k + 1)\\).\n\nThe angle of departure (or angle of arrival) of the root locus from a complex pole (or at a complex zero) can be found by subtracting from 180° the sum of all the angles of vectors from all other poles and zeros to the complex pole (or complex zero) in question, with appropriate signs included.\n\nAngle of departure from a complex pole=180° \\(–\\) (sum of the angles of vectors to a complex pole in question from other poles) \\(+\\) (sum of the angles of vectors to a complex pole in question from zeros)\nAngle of arrival at a complex zero=180° \\(–\\) (sum of the angles of vectors to a complex zero in question from other zeros) \\(+\\) (sum of the angles of vectors to a complex zero in question from poles)\nFor example:\n\\[\nG(s) = \\frac{K(s+2)}{s^2+2s+3}\n\\]\n\\(s=-1\\pm j\\sqrt{2}\\)\n\nThe real axis \\([-\\inf, -2]\\) is part of the loci\nSince there are two open-loop poles and one zero, there is one asymptote, which coincides with the negative real axis.\nThe angle of departure from the complex-conjugate poles is:\n\n\\[\n\\phi'_1 - (\\theta_1 + \\theta'_2) = \\pm 180^o(2k + 1)\n\\]\nor\n\\[\n\\theta_1 = 180 - \\theta'_2 + \\phi'_1 = 180 - \\theta_2 + \\phi_1 =\n\\]\nwhich is:\n\\[\n\\theta_1 = 180 - \\theta_2 + \\phi_1 = 180 - 90 + 55 = 145^o\n\\]\n\n\n\n\n\n\n\n\n\nFigure From Ogata, Modern Control Engineering\nBreak-in points:\n\\[\nK = -\\frac{s^2+2s+3}{s+2}\n\\]\n\\[\n\\frac{dK}{ds} = -\\frac{(2s+2)(s+2)-(s^2+2s+3)}{(s+2)^2}=0\n\\]\nwhich gives:\n\\[\ns^2+4s+1 = 0\n\\]\nfrom which:\n\\(s= -3.7320,\\;\\;\\;s=-0.2680\\)\n\\(s= -3.732\\) is on the root locus and hence it is a break-in point.\n\nTo determine accurate root loci, several points must be found by trial and error between the break- in point and the complex open-loop poles. (To facilitate sketching the root-locus plot, we should find the direction in which the test point should be moved by mentally summing up the changes on the angles of the poles and zeros.)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\\[\nG(s)H(s) = \\frac{1}{s(s+4)(s^2+4s+20)}\n\\]\n\n\n\n\n\n\n\n\n\n\nPoles:\n\n\\(s = 0, -4, -2\\pm4j\\)\n\n4 asymptotes:\n\nangles: 45, 135, 225, 315 (deg)\ncentroid: -2 (everything is symmetric)\n\nHow do the poles get to the asymptotes?\n\n\n\n\n\n\n\n\n\n\nLet’s use what the root locus looks like:\n\ns = control.tf([1, 0], [1])\n\nK = 1\nGH = 1/(s*(s+4)*(s**2+4*s+20))\n\nprint(GH)\n\n\n             1\n---------------------------\ns^4 + 8 s^3 + 36 s^2 + 80 s\n\n\n\n\ncontrol.pzmap(GH)\nplt.grid()\n\n\n\n\nAnd for the closed loop system:\n\n# Change K from 1 to 105\ncontrol.pzmap(control.feedback(GH*40, 1));\nplt.grid()\n\n\n\n\n\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\ncontrol.rlocus(GH);\n\n\n\n\n\ncontrol.rlocus parameters:\n\nkvect (list or ndarray, optional) – List of gains to use in computing diagram.\nprint_gain (bool) – If True (default), report mouse clicks when close to the root locus branches, calculate gain, damping and print.\n\n\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1,1,figsize=(7, 7))\ncontrol.rlocus(GH);\n\n\n\n\n\nThe MATLAB sisotool has also additional tools that can help you in your controller design."
  },
  {
    "objectID": "root_locus.html#comments-on-the-root-locus-plot",
    "href": "root_locus.html#comments-on-the-root-locus-plot",
    "title": "Root Locus",
    "section": "Comments on the Root Locus Plot",
    "text": "Comments on the Root Locus Plot\n\nA slight change in the pole–zero configuration may cause significant changes in the root-locus configurations.\nThe next two plots demonstrate the fact that a slight change in the location of a zero or pole will make the root-locus configuration look quite different.\n\n\ns = control.tf([1, 0], [1])\n\nK = 1\nGH = (s+2.5)/(s*(s+1)*(s**2+5*s+22))\n\nprint(GH)\n\n\n          s + 2.5\n---------------------------\ns^4 + 6 s^3 + 27 s^2 + 22 s\n\n\n\n\nfig, axs = plt.subplots(1,1,figsize=(10, 10))\ncontrol.rlocus(GH);\naxs.set_xlim((-6,3))\n\n(-6.0, 3.0)\n\n\n\n\n\n\ns = control.tf([1, 0], [1])\n\nK = 1\nGH = (s+2.6)/(s*(s+1)*(s**2+5*s+22))\n\nprint(GH)\n\n\n          s + 2.6\n---------------------------\ns^4 + 6 s^3 + 27 s^2 + 22 s\n\n\n\n\nfig, axs = plt.subplots(1,1,figsize=(10, 10))\ncontrol.rlocus(GH);\naxs.set_xlim((-6,3))\n\n(-6.0, 3.0)\n\n\n\n\n\n\nCancellations of Poles of G(s) with Zeros of R(s)\nThe more general case\n\n\n\n\n\n\nIt is important to note that if the denominator of \\(G(s)\\) and the numerator of \\(R(s)\\) involve common factors, then the corresponding open-loop poles and zeros will cancel each other, reducing the degree of the characteristic equation by one or more.\nThe root-locus plot of \\(G(s)R(s)\\) does not show all the roots of the characteristic equation, only the roots of the reduced equation.\nTo obtain the complete set of closed-loop poles, we must add the canceled pole of \\(G(s)R(s)\\) to those closed-loop poles obtained from the root-locus plot of \\(G(s)R(s)\\).\nThe important thing to remember is that the canceled pole of \\(G(s)R(s)\\) is a closed-loop pole of the system"
  },
  {
    "objectID": "root_locus.html#further-comments",
    "href": "root_locus.html#further-comments",
    "title": "Root Locus",
    "section": "Further comments",
    "text": "Further comments\n\nPlotting the damping ratio line given a desired overshoot \\(S\\) (e.g. \\(S=15\\%\\))\n\nWe know that:\n\\[\n\\xi = \\frac{|\\ln(S/100)|}{\\sqrt{\\pi^2 + \\ln^2(S/100)}}\n\\]\nand the angle \\(\\beta = \\cos^{-1}(\\xi)\\)\nLet’s see this in action:\n\n# Percentage Overshoot\nS = 15\n\n# Equivalent damping ratio (from the equation)\nxi = np.abs(np.log(S/100)/np.sqrt(np.pi**2+np.log(S/100)**2))\n\nprint('Damping ratio (xi): {:.4f}'.format(xi))\n\nDamping ratio (xi): 0.5169\n\n\n\n# desired damping lines\nphi = np.arccos(xi)\nprint('Angle phi: {:.4f} deg'.format(phi*180/np.pi))\n\nAngle phi: 58.8734 deg\n\n\nWe can plot it:\n\nfig, axs = plt.subplots(1,1, figsize=(10,5))\n\nx_range = np.linspace(-2, 0) # points on the constant damping ratio line\n\nplt.plot(x_range, -np.tan(phi)*x_range, \n         linewidth=3, color='g')\n\nplt.plot([x_range[1], x_range[10], x_range[-6]], \n          -np.tan(phi)* np.array([x_range[1], x_range[10], x_range[-6]]), \n         linewidth=3, linestyle='', color='r', marker='x', markeredgewidth=3, markersize=12)\nplt.plot(plt.xlim(), [0, 0], linewidth=3, color='k')\nplt.plot([0, 0], plt.ylim(), linewidth=3, color='k')\nplt.text(-0.7, 0.5, \"$\\phi$\", fontsize=20)\nplt.grid()\nplt.xlabel('real', fontsize=20)\nplt.ylabel('imag', fontsize=20);\n\n\n\n\n\nAll points along the line have the same damping ratio\n\nNow we can go deeper in our analysis and define a second order transfer function. - Note that to do this, we need to choose the natural frequency \\(\\omega_n\\). - Natural frequency however does not influence overshoot, we are free to select any value. For example: \\(\\omega_n=1\\) rad/s.\n\nw = 1\n\nDefine the transfer function using the Python Control Library.\nLet’s use another way to do it, creating a transfer function object ‘s’ first:\n\ns = control.tf([1, 0],[1])\nprint(s)\n\n\ns\n-\n1\n\n\n\nAnd now we can use the ‘s’ object to define more complex transfer functions more easily:\n\nG = w**2/(s**2+2*xi*w*s+w**2)\nprint(G)\n\n\n        1\n-----------------\ns^2 + 1.034 s + 1\n\n\n\nAnd we can plot the step response:\n\n[tout, yout] = control.step_response(G)\n\n\nfig, ax = plt.subplots(1,1, figsize=(10,5))\nax.plot(tout, yout)\naxin1 = ax.inset_axes(\n        [6, 0.2, 4, .4], transform=ax.transData)\naxin1.plot(tout[yout>1.01], yout[yout>1.01])\nax.indicate_inset_zoom(axin1, edgecolor=\"black\")\nplt.xlabel('time (s)')\nplt.ylabel('amplitude')\nplt.grid()\n\n\n\n\n\nAs expected there is \\(15\\%\\) overshoot\n\nLet’s now verify that the natural frequency does not impact overshoot:\n\nfig, ax = plt.subplots(1,1, figsize=(10,5))\nfor w in np.arange(0.5, 2.1, 0.5):\n    print('w', w)\n    G = w**2/(s**2+2*xi*w*s+w**2)\n    [tout, yout] = control.step_response(G)\n    plt.plot(tout, yout)\n    \nplt.xlabel('time (s)')\nplt.ylabel('amplitude')\nplt.grid()\n\nw 0.5\nw 1.0\nw 1.5\nw 2.0\n\n\n\n\n\n\nAll responses have exactly $15$ overshoot\nThe rise time and settling time are influenced by the natural frequency\n\nLet’s see how the poles are impacted:\n\nfig, ax = plt.subplots(1,1, figsize=(7,7))\n\nfor w in np.arange(0.5, 2.1, 0.5):\n    roots = np.roots([1,+2*xi*w,+w**2])\n    plt.plot(np.real(roots), np.imag(roots), \n             linestyle='', marker='x', markeredgewidth=3, markersize=12, label='w={:.2f}'.format(w))\n\nx_range = np.linspace(-1.2, 0)\nplt.plot(x_range, -np.tan(phi)*x_range, linewidth=3, color='g', linestyle='--')\nplt.plot(x_range, +np.tan(phi)*x_range, linewidth=3, color='g', linestyle='--')\n\nplt.legend(fontsize=12)\nplt.xlabel('real axis')\nplt.ylabel('imag axis')\nplt.grid()\n\n\n\n\n\nRequirements\n\nTime requirements\n\nRaise time\nPeak time\nSettle time\nOvershoot\nSteady state error\n\nFrequency requirements\n\nBandwidth\nGain Margin\nPhase Margin\n\nS-Domain\n\nPole/Zero locations\n\nObviously they are all related\nWe have tools in the s-domain or in the frequency domain that we do not have in the time domain (e.g. root locus)\n\nWhen focusing on S-Domain, the location of poles and zeros can be expressed: - rectangular coordinates: \\(x+yj\\) - polar coordinates: \\(\\omega_n, \\xi\\)\n\nfig, ax = plt.subplots(1,1, figsize=(7,7))\n\nxi = 0.5\nw  = 1\nphi = np.arccos(xi)\n\nour_roots = np.roots([1, 2*xi*w, w**2])\n\nfor xi in np.arange(0, 1, 0.05):\n    r = np.roots([1, 2*xi*w, w**2])\n    plt.plot(np.real(r), np.imag(r), \n         color='k', linestyle='', marker='.', markeredgewidth=1, markersize=1)\n\n# constant damping ratio line\nx_range = np.linspace(-1.2, 0)\nplt.plot(x_range, -np.tan(phi)*x_range, linewidth=3, color='g', linestyle='--')\nplt.plot(x_range, +np.tan(phi)*x_range, linewidth=3, color='g', linestyle='--')\n\n# poles\nplt.plot(np.real(our_roots), np.imag(our_roots), \n         color='r', linestyle='', marker='x', markeredgewidth=3, markersize=12)\n\n# axis\nplt.plot(plt.xlim(), [0, 0], linewidth=2, color='k')\nplt.plot([0, 0], plt.ylim(), linewidth=2, color='k')\n\nplt.text(-1.5, -1, 'x+yj')\nplt.text(-1.5, 1, '$\\omega_n$, $\\zeta$')\nplt.xlabel('real axis')\nplt.ylabel('imag axis')\nplt.axis('equal')\nplt.grid()\n\n\n\n\n\nDamping ratio and natural frequency are only defined for 2nd order system\nThey lose their meaning in higher order systems\nWe saw that many higher order systems exibit 2nd order behaviours (e.g. dominant poles)\nWhen we have a requirement expressed in terms of damping ratio or natural frequency we are assuming that the system is 2nd order\nPlay around with high order systems and verify what happens!"
  },
  {
    "objectID": "root_locus.html#solving-for-gain-in-a-root-locus-plot",
    "href": "root_locus.html#solving-for-gain-in-a-root-locus-plot",
    "title": "Root Locus",
    "section": "Solving for gain in a root locus plot",
    "text": "Solving for gain in a root locus plot\n\n\n\n\n\n\n\n\n\nLet’s consier an arbitrary \\(G(s)\\):\n\\[\nG(s) = \\frac{5}{(s+4)(s+2)}\n\\]\n\nDetermine what value of \\(K\\) gives a 15% overshoot\n\nLet’s start plotting the root locus\n\ns = control.tf([1, 0],[1])\n\n\nfig, axs = plt.subplots(1,1, figsize=(10, 5))\ncontrol.rlocus(5/((s+4)*(s+2)));\n\n# desired damping ratio line\nx_range = np.linspace(-4, 0)\nplt.plot(x_range, -np.tan(1.0276)*x_range, linestyle='--')\nplt.ylim(-5.5, 5.5)\n\n(-5.5, 5.5)\n\n\n\n\n\n\nWe saw that a 15% overshoot translates into:\n\n\\(\\zeta=0.5169\\)\n\\(\\theta=\\cos^{-1}(0.5169) = 1.0276\\) rad\n\nWe would like to have our poles where the root locus intersects the line of damping ratio desired (see orange line in the plot above)\n\nWe can calculate the desired position for the poles:\n\nreal component: \\(-3\\)\nimaginary component: \\(3\\tan(\\theta)=4.9683\\)\nDesired poles: \\(-3 \\pm 4.9683 j\\)\n\nThe closed loop transfer function is:\n\\[\n\\frac{KG(s)}{1+KG(s)}\n\\]\nWe can then find:\n\\[\nK = \\frac{-1}{|G(s)|}\\Big|_{s=-3 + 4.9683 j} = \\frac{-1}{\\frac{5}{(-3+4.97j+4)(-3+4.97j+2)}} = -\\frac{(1+4.97j)(-1+4.97j)}{5}=\\frac{1+4.97^2}{5}= 5.14\n\\]\nSteps: - Draw the loop locus - Figure out where you would like the poles to be - Plug the desired poles in the close loop characteristic equation and solve for \\(K\\)\n\nFor more complex cases it could become more complicated."
  },
  {
    "objectID": "loop-shaping.html",
    "href": "loop-shaping.html",
    "title": "Controller design: Loop analysis and shaping",
    "section": "",
    "text": "Bolzern, Chapter 12."
  },
  {
    "objectID": "loop-shaping.html#introduction",
    "href": "loop-shaping.html#introduction",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Introduction",
    "text": "Introduction\n\nFeedback Goals\n\nStability of the closed loop system\nPerformance while tracking inputs\n\ntracking error, typically specified in terms of steady state error to specific inputs:\n\ne.g., error to step inputs less than \\(5\\%\\)\n\nbehaviour of the transient, typically specified in terms of bandwidth, rise time, settling time, damping ratio, overshoot (these are requirements for the transfer function between \\(Y\\) and \\(Y_{ref}\\)).\n\nRobustness to noise measurement and disturbances\n\nWe can design the behavior of the closed loop system focusing on the open loop transfer function.\nObservation: > This approach was also used in studying stability using the Nyquist criterion: we plotted the Nyquist plot for the open loop transfer function to determine the stability of the closed loop system."
  },
  {
    "objectID": "loop-shaping.html#feedback-design-via-loop-shaping",
    "href": "loop-shaping.html#feedback-design-via-loop-shaping",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Feedback Design via Loop Shaping",
    "text": "Feedback Design via Loop Shaping\n\nOne advantage of the Nyquist stability theorem is that it is based on the loop transfer function \\(L = GR\\), which is the product of the transfer functions of the process and the controller.\nEasy to see how the controller influences the loop transfer function.\nFor example, to make an unstable system stable we simply have to bend the Nyquist curve away from the critical point.\nThis idea is the basis of several different design methods collectively called loop shaping: Choose a compensator that gives a loop transfer function with a desired shape.\nExample: Determine a loop transfer function that gives a closed loop system with the desired properties and to compute the controller as \\(R = L/G\\) or change the gain of the process transfer function to obtain the desired bandwidth, and then add poles and zeros until the desired shape is obtained.\n\nLet’s consider our usual closed loop system:\n\n\n\n\n\n\n\\(d\\): disturbance\n\\(n\\): noise\n\nWe want to design \\(R(s)\\): - Disturbance is rejected - Noise is attenuated - Stable and high performance of \\(G(s)\\) - Robustness in case our model of \\(G(s)\\) is different than reality\nWe will look at the loop transfer function \\(L(s) = G(s)R(s)\\) - From a design perspective, the use of loop analysis tools is very powerful - If we can specify the desired performance in terms of properties of \\(L(s)\\), we can directly see the impact of changes in the controller R(s) (since we know the plant \\(G(s)\\)).\nThis is much easier, for example, than trying to reason directly about the tracking response of the closed loop system - the close loop transfer function is much more complex and given by \\(H = GR/(1+GR)\\).\nNote - Designing a controller: designing the controller \\(R\\) means choosing poles, zeros and the static gain of the controller.\nReference - A good reference in general and for the loop shaping specifically is “Feedback Systems: An Introduction for Scientists and Engineers” available here freely to download (See Chapter 10 for details on Loop Shaping)\n\n\nSensitivity and Complementary Sensitivity Functions (Recap)\n(we talked about this in 08_Main_types_of_loops_and_transfer_functions - please refer to that for more details)\nDoing simple algebric manipulation we can express the output \\(y\\) as a function of all the inputs:\n\\[\nY(s) = D(s) + GR(Y_{ref}-Y+N) \\Rightarrow (I+GR) Y(s) = GR \\cdot Y_{ref}(s) + D(s) + GR\\cdot N(s)\n\\]\n\\[\nY(s) = \\frac{GR}{1+GR}Y_{ref}(s) + \\frac{1}{1+GR}D(s) + \\frac{GR}{1+GR}N(s)\n\\]\nWe want to design \\(R\\) so that all transfer functions have good properties - tracking (at freq. where this is important) - disturbance rejection (at freq. where this is important) - noise attenuation (at freq. where this is important)\nAnd we can explict the equation above as a set of transfer functions:\n\\[\nE(s) = \\frac{1}{1+GR}Y_{ref}(s), \\hspace{2cm} Y(s) = \\frac{1}{1+GR}D(s)\n\\]\n\\[\nY(s) = \\frac{GR}{1+GR}Y_{ref}(s), \\hspace{2cm} Y(s) = \\frac{GR}{1+GR}N(s)\n\\]\n\\[\nU(s) = \\frac{R}{1+GR}Y_{ref}(s) \\hspace{2cm} \\hspace{4cm}\n\\]\nWe called:\n\nSensitivity Function: \\(S = \\frac{1}{1+GR}\\)\nComplementary Sensitivity Function: \\(T = \\frac{GR}{1+GR}\\)\n\nWe can re-write these functions in terms of the loop transfer function \\(L\\):\n\nSensitivity Function: \\(S = \\frac{1}{1+L}\\)\nComplementary Sensitivity Function: \\(T = \\frac{L}{1+L}\\)\n\nand we also know that \\(S+T=I\\) for all frequencies"
  },
  {
    "objectID": "loop-shaping.html#design-considerations-suitable-shape",
    "href": "loop-shaping.html#design-considerations-suitable-shape",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Design Considerations: suitable shape",
    "text": "Design Considerations: suitable shape\nLet’s first discuss a suitable shape for the loop transfer function that gives good performance and good stability margins. - Remember that the objective is that of designing a suitable \\(R\\).\nLet’s re-write the previous equation expliciting the error term: \\(\\epsilon=y_{ref} - y +n\\)\n\\[\n\\epsilon = S Y_{ref}(s) - S D(s) - T N(s)\n\\]\n\nwe are not interested in the value of \\(y\\) so much (we want \\(y \\rightarrow y_{ref}\\))\nwe want \\(\\epsilon = 0\\) for any disturbance, noise and reference that are reasonable for my system\n\nGoals: - Good reference tracking for a sufficently slow change in reference hence for slow frequencies (e.g., think of a cruise control) - With want \\(|S|\\) small at low frequency - This is also true for low freq. disturbances\n\nNoise is generally high frequency\n\nWe need \\(|T|\\) small at high frequency\n\nSince \\(S+T=I\\) they cannot be “small” all the time, and we need to have a good compromise leveraging the complementary behaviour that we often have between low frequency signals (references and disturbances) and high frequency signals (noise):\n\n\n\n\n\n\n\n\n\n\n\nCross-over frequency \\(w_c\\):\n\nWhere the \\(S\\) begins to be large, and where the \\(T\\) begins to be small\n\\(w_c\\) makes it possible to choose where I think noise dominate vs disturbance dominate\nNote that \\(T\\) should be close to 1 in low frequency for \\(y=y_{ref}\\).\n\n\nGiven that we now know how we would like \\(S\\) and \\(T\\) to look like, we can also start defining how \\(L\\) should look like.\n\n\\(L(s)\\) includes the controller \\(R(s)\\) that I can design to give me a good \\(L\\).\n\n\n\n\\(L(s)=G(s)R(s)\\)\n\\(S(s)=\\frac{1}{1+L(s)}\\)\n\\(T(s)=\\frac{L}{1+L(s)}\\)\n\n\n\n\n\n\n\n\n\n\n\nI would like to design my controller \\(R(s)\\) so that the \\(L\\) loop transfer function looks like an integrator\nNote that moving \\(L\\) to the right or to the left means choosing the correct gain \\(K\\):\n\n\\[\nL(s)=\\frac{K}{s}\n\\]\n\nHigher \\(K\\) means higher frequency reference tracking and disturbance rejection\nSmaller \\(K\\) means lower bandwidth performance (e.g., noisy sensors at lower frequencies)\nNote that where the \\(L(s)=\\frac{K}{s}\\) crosses the \\(0\\) dB line is where we are dividing between low and high frequency\n\n\n\nGood performance requires that the loop transfer function is large for frequencies where we desire good tracking of reference signals and good attenuation of low-frequency load disturbances.\nSince \\(S = 1/(1 + L)\\): for frequencies where \\(|L| > 100\\) disturbances will be attenuated by approximately a factor of 100 or more and the tracking error is less than \\(1\\%\\).\n\nRemember that \\[\n  E(s) = \\frac{1}{1+GR}Y_{ref}(s)\n  \\]\n\nThe transfer function from measurement noise to control action is \\(C_S = R/(1 + L)\\). To avoid injecting too much measurement noise, which can create undesirable control actions, the controller transfer function \\(R\\) should have low gain at high frequencies (high-frequency roll-off).\nGiven these constraints, the loop transfer function \\(L=GR\\) should thus have roughly the shape:\n\n\n\n\n\n\n\n\n\n\n\nIt has unit gain at the gain crossover frequency (\\(|L(i\\omega_{gc})| = 1\\)), large gain for lower frequencies, and small gain for higher frequencies.\nThe crossover frequency \\(\\omega_{gc}\\) determines the attenuation of load disturbances, bandwidth, and response time of the closed loop system.\nThe slope \\(n_{gc}\\) of the gain curve of \\(L(s)\\) at the gain crossover frequency \\(\\omega_{gc}\\) determines the robustness of the closed loop systems.\nGood robustness requires good stability margins, which imposes requirements on the loop transfer function around the gain crossover frequency \\(\\omega_{gc}\\).\nDesirable to transition from high loop gain \\(|L(i\\omega))|\\) at low frequencies to low loop gain quickly\nRobustness requirements impose restrictions on how fast the gain can decrease.\n\nIt is possible to verify that, for a minimum-phase system (a system in which poles and zeros will not lie on the right side of the s-plane): $ n_{gc} + $, where \\(\\varphi_m\\) is the phase margin in degrees.\nA steeper slope thus gives a smaller phase margin.\nTime delays and poles and zeros in the right half-plane impose further restrictions\n\nAt low frequencies, a large magnitude of \\(L\\) provides good load disturbance rejection and reference tracking\nAt high frequencies a small loop gain avoids injecting too much measurement noise.\n\n\nLoop shaping\n\nIs a trial-and-error procedure\nWe typically start with a Bode plot of the process transfer function \\(G\\)\nChoosing the gain crossover frequency \\(\\omega_{gc}\\) is a major design decision: a compromise between attenuation of load disturbances and injection of measurement noise\nFinally shape the loop transfer function by changing the controller gain and adding poles and zeros to the controller transfer function\nThe controller gain at low frequencies can be increased by so-called lag compensation, and the behavior around the crossover frequency can be changed by so-called lead compensation.\n\n\n\nA simple example for cruise control\nModel of the car:\n\\[\n\\dot{x} = -x + u\\\\\ny = x\n\\]\nwith transfer function\n\\[\n\\frac{Y(s)}{U(s)} = G(s) = \\frac{1}{s+1}\n\\]\n\nWe would like to have \\(L(s)=G(s)R(s) = \\frac{K}{s}\\)\nWe know that \\(G(s) = \\frac{1}{s+1}\\)\n\n\\[\n\\frac{1}{s+1}R(s) = \\frac{K}{s} \\Rightarrow R(s)=\\frac{K(s+1)}{s}\n\\]\n\nThis solves the problem\n\n\n\n\n\n\n\n- Is there any problem with this approach? - We are inverting the plant \\(G(s)\\) to solve for the controller - Inverting the plant with a RHP pole (or zero) would imply a RHP zero (or pole): we are making the overall system unobservable exactly where our system is unstable (critical cancellation: the system is internally unstable and we would not even be able to tell measuring the output) - What happens if the real system is not exactly equal to my model \\(G(s)\\)? - Feedback helps, but we will need to make this more robust\n\n\n\n\ns + 1\n\n\n``` ::: :::\n\n\n::: {.cell} ``` {.python .cell-code} # L(s) desiredLoop = 10/s\n\n\n# Controller R = desiredLoop/car_tf print(R) ```\n\n\n::: {.cell-output .cell-output-stdout} ```\n\n\n10 s + 10\n\n\n\ns\n:::\n:::\n\n\nWe can then create our system:\n\n::: {.cell}\n``` {.python .cell-code}\nsysLoop = control.series(R, car_tf) # Loop T.F.\nsysCL = control.feedback(sysLoop, 1, -1) # Close loop T.F.\n:::\n\nt, y = control.step_response(sysCL, T=7)\n\nt_ol, y_ol = control.step_response(car_tf, T=7) # open loop - no control here\n\nplt.plot(t, y, color='b', label='sysCL')\nplt.plot(t_ol, y_ol, color='g', label='sysOL')\n\nplt.grid()\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nLet’s see what happens if we had a slight mismatch between the car and our model\n\nrealCar_tf = 1/(s+0.5)\nprint(realCar_tf)\n\n\n   1\n-------\ns + 0.5\n\n\n\n\nsysLoop = control.series(R, realCar_tf) # Loop T.F.\nsysCL = control.feedback(sysLoop, 1, -1) # Close loop T.F.\n\nt, y = control.step_response(sysCL, T=7)\n\nt_ol, y_ol = control.step_response(car_tf, T=7) # open loop - no control here\n\nplt.plot(t, y, color='b', label='sysCL')\nplt.plot(t_ol, y_ol, color='g', label='sysOL')\n\nplt.grid()\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\nLet’s see what the Loop Transfer Function looks like as a Bode Plot\n\nsysLoop = control.series(R, car_tf)\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\ncontrol.bode_plot(sysLoop, dB=True);\n\naxs[1].set_ylim(0, -100);\n\n\n\n\nWhich is an integrator,\nand the desired one:\n\nfig, axs = plt.subplots(2, 1, figsize=(10, 5))\ncontrol.bode_plot(desiredLoop, dB=True);\n\n\n\n\nAnd as expected, they are exactly the same.\n\n\nSensitivity and Robustness\n\nWe know that the sensitivity gives an indication of robustness\n\n\n\n\n\n\n\n\n\n\n\nWe saw this with Nyquist plots already.\n\nI would like my \\(L(s)\\) as far away as possible from the -1 point\nThe distance from -1 is the stability margin we have (gain and phase)\nSince \\(S=\\frac{1}{1+L}\\), the closer \\(L\\) is to -1, and hence the less robust it is, the bigger \\(S\\) is (at that frequency).\n\n\nNominal sensitivity peak \\[\nM_s = \\max_{0 \\leq \\omega \\leq \\infty} | S(j\\omega) | = \\max_{0 \\leq \\omega \\leq \\infty} \\Big| \\frac{1}{1+G(j\\omega)R(j\\omega)} \\Big|\n\\]\n\n\\(M_s\\) gives us an indication of how far \\(L\\) is from -1\nWe want \\(S\\) to have a small peak so that we can accept:\n\nlarger uncertainty in our model\ntime delays (e.g., slower than expected computation, or response)\nRHP zeros of P\n\ntime delays and RHP zeros of P limits how small the sensitiviy peak can be: fundamental lack of robustness of the system\nin this case we need to have a smaller bandwidth (less performance): this would level off the sensitivity function sooner\n\nIntuitively - if I have a time delay of 1s, I cannot control something that moves at 500Hz - A RHP zeros acts similarly to a time delay (for ex. look at the step response)\n\n\nAnalysis: More Comments\n\nWe want to understand the properties of the closed loop system \\(H\\) analysing the Bode plot of \\(L\\)\nFrom now on, we will assume that \\(L\\) is BIBO stable (we will accept to have up to 2 poles at the origin).\nWhat happens if \\(G\\) is unstable? We design a first regulator to stabilise it and a second one that meets performance requirements\n\n\n\n\n\n\n\n\n\n\nLet’s go back to our system once more:\n\n\n\n\n\n\n\n\n\n\\[\nH(s) = \\frac{G(s)R(s)}{1+G(s)R(s)} = \\frac{L(s)}{1+L(s)}\n\\]\n\nThe “+” sign at the denominstator depends on the summation node in the feedback loop\nHowever, \\(L(s)\\) has magnitude and phase: when the phase is < -180 there is a sign change, this is equivalent to summing reference input and measured output\nThe feedback loop can only sustain the change in sign if the \\(L(s)\\) < 1 (see also Nyquist)."
  },
  {
    "objectID": "loop-shaping.html#closed-loop-stability---case-1",
    "href": "loop-shaping.html#closed-loop-stability---case-1",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Closed loop stability - Case 1",
    "text": "Closed loop stability - Case 1\n\n\\(L(j\\omega)\\)\nGiven \\(\\omega_{gc} : |L(j\\omega_{gc})|=1\\)\n\\(|L(j\\omega)| > 1\\), for $< {gc} $, and \\(|L(j\\omega)| < 1\\), for $> {gc} $,\nif \\(\\angle L(j\\omega_{gc}) > -180\\), then \\(H=\\frac{L(s)}{1+L(s)}\\) is BIBO stable\nWhen the magnitude plot crosses 0dB then phase must be > -180\\(^o\\)\nWe are revisiting the phase margin: how much can the phase change before the system becomes unstable.\nNote that, the phase margin is a robustness metric with respect to fixed delays: delay \\(\\tau\\), means phase shift: \\(\\omega\\tau\\)."
  },
  {
    "objectID": "loop-shaping.html#closed-loop-stability---case-2",
    "href": "loop-shaping.html#closed-loop-stability---case-2",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Closed loop stability - Case 2",
    "text": "Closed loop stability - Case 2\n\n\\(L(j\\omega)\\)\nGiven \\(\\omega_{gc} : \\angle L(j\\omega)|=-180^o\\)\n\\(\\angle L(j\\omega) > -180\\), for $< {gc} $, and \\(\\angle L(j\\omega) < -180\\), for $> {gc} $,\nif \\(|L(j\\omega)| < 1\\), then \\(H=\\frac{L(s)}{1+L(s)}\\) is BIBO stable\nWhen the phase plot crosses -180\\(^o\\) then the magnitude must be < 1 (0dB)\nWe are revisiting the gain margin: how much can the gain can change before the system becomes unstable.\nNote that, the gain margin is a robustness metric with respect to uncertainties in the static gain of the loop transfer function \\(L\\)."
  },
  {
    "objectID": "loop-shaping.html#tracking-errors",
    "href": "loop-shaping.html#tracking-errors",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Tracking errors",
    "text": "Tracking errors\n\n\n\n\n\n\n\n\n\n\\[\nE(s) = \\frac{1}{1+RG}Y_{ref}(s) = \\frac{1}{1+L}Y_{ref}(s)\n\\]\n\nWe are interested in understanding how the error evolves over time, and when \\(t \\rightarrow \\infty\\)\nWith varying input signals. Typically, step, ramp, parabolic input, etc \\[u(t)=1(t); \\hspace{0.5cm} u(t)=t ; \\hspace{0.5cm} u(t)=\\frac{t^2}{2} \\]\nwhose transfer functions are: \\[U(s) = \\frac{1}{s} \\hspace{0.5cm} U(s) = \\frac{1}{s^2} ; \\hspace{0.5cm} U(s) = \\frac{1}{s^3} \\]\n\nOr in the frequency domain:\n\\[\nE(j\\omega) = \\frac{1}{1+L(j\\omega)}Y_{ref}(j\\omega)\n\\]\n\nAnalysing the behaviour of \\(e\\) for \\(t \\rightarrow \\infty\\), means analysing \\(E(j\\omega)\\) for $ $.\n\\(L(j\\omega)\\) should be as large as possible for low frequencies (as long as closed loop stability is maintained).\n\n\nSidenote:\n\nRemember that a system if of type 0 (1, 2, …) when its loop function \\(L(s)\\) has 0 (1, 2, …) poles at the origin.\n\n\nSystems of type 1 or 2: \\(L(j\\omega) \\rightarrow \\infty\\) when \\(\\omega \\rightarrow 0\\)\n\nRemember that a system has type \\(n\\) if its loop transfer function \\(L(s)\\) has \\(n\\) poles at the origin.\n\nIf the process \\(G\\) does not satisfy our requirements, we can act on the controller \\(R\\)\nNote: to analyse the steady state of a system we must have closed loop stability, or we do not even have steady state!\n\n\n\nSteady state errors for systems of type 0, 1, 2\n\\[\ne_{\\inf} = \\lim_{s\\rightarrow0} sE(s) = \\lim sG_e(s)Y_{ref}(s) = \\lim_{s\\rightarrow0} s\\frac{1}{1+L(s)}Y_{ref}(s)\n\\]\nwhere \\(G_e(s)\\) is the transfer function between the input \\(y_{ref}\\) and the error \\(E(s)\\).\n\nIf \\(y_{ref}(t)=1(t)\\) (step input) \\(\\rightarrow\\) \\(Y_{ref}(s) = \\frac{1}{s}\\), and the system \\(L(s)\\) is of type 0, then \\(\\lim_{s\\rightarrow0} L(s) = K\\), (static gain) and the error (in this case called position error) is finite. If the system is of type 1 or 2, \\(\\lim_{s\\rightarrow0} L(s) = \\inf\\) and the error is 0.\nIf \\(y_{ref}(t)=ramp(t)\\) (ramp input) \\(\\rightarrow\\) \\(Y_{ref}(s) = \\frac{1}{s^2}\\), and the system \\(L(s)\\) is of type 0, then \\(\\lim_{s\\rightarrow0} sL(s) = 0\\), and the error (in this case called velocity error) is infinite. If the system is of type 1, \\(\\lim_{s\\rightarrow0} sL(s) = \\frac{1}{k}\\) and the error is finite. If the system is of type 2, \\(\\lim_{s\\rightarrow0} sL(s) = \\inf\\) and the error is 0.\n\n\n\n\n\nType 0\nType 1\nType 2\n\n\n\n\nstep\n\\(\\frac{1}{1+k}\\)\n\\(0\\)\n\\(0\\)\n\n\nramp\n\\(\\inf\\)\n\\(\\frac{1}{k}\\)\n\\(0\\)\n\n\nparabolic\n\\(\\inf\\)\n\\(\\inf\\)\n\\(\\frac{1}{k}\\)\n\n\n\n\n\nComments\n\nThe Loop function \\(L(s)\\) must have high \\(|L|\\) at low frequency (for type 1 and 2 \\(|L| = \\inf\\) at low freq.)\nIf the plant does not respect desired conditions to obtain desired tracking performance we need to act on \\(R\\) to obtain those conditions\nThanks to the feedback we can have zero error even when we do not fully know the plant.\nPre-requisite: to do steady state analysis we need to have closed loop stability. If the system is unstable, there is no steady state."
  },
  {
    "objectID": "loop-shaping.html#closed-loop-transient-the-relationship-between-loop-function-harmonic-response-and-closed-loop",
    "href": "loop-shaping.html#closed-loop-transient-the-relationship-between-loop-function-harmonic-response-and-closed-loop",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Closed Loop Transient: the relationship between loop function harmonic response and closed loop",
    "text": "Closed Loop Transient: the relationship between loop function harmonic response and closed loop\n\nWhat is the relationship between the harmonic response of the loop transfer function \\(L(j\\omega)\\) and the closed loop \\(H(j\\omega)\\)\n\n\\[\n|H(j\\omega)| = \\frac{|L(j\\omega)|}{|1+L(j\\omega)|} = \\frac{|L(j\\omega)|}{\\sqrt{1+|L(j\\omega)|^2}}\n\\]\n\nwhen \\(\\omega \\rightarrow 0\\), to guarantee low tracking errors: \\(|L(j\\omega)| >> 1 \\Rightarrow |H(j\\omega)| \\approx \\frac{|L(j\\omega)|}{|L(j\\omega)|} = 1\\)\nwhen \\(\\omega \\rightarrow \\infty\\), \\(|L(j\\omega)| << 1 \\Rightarrow |H(j\\omega)| \\approx \\frac{|L(j\\omega)|}{1} = |L(j\\omega)|\\), because \\(L\\) is a causal system and must have more poles than zeros.\n\nLet’s consider again (see 06_Intro_to_freq_response_and_bode_plots):\n\\[\nL(s) = \\frac{1000}{(s+1)^3(s+10)}\n\\]\n\\[\nH(s) = \\frac{L(s)}{1+L(s)}\n\\]\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Open Loop T.F. L(s)\nmag_ol, phase_ol, omega_ol = control.bode_plot(control.tf([1000],[1, 13, 33, 31, 10]), dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# Close Loop T.F. H(s)\nmag_cc, phase_cc, omega_cc = control.bode(control.tf([1000],[1, 13, 33, 31, 10+1000]), dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n\n## Plot additional information on the Bode Plots\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[40-3, 40-3],'b--')\nplt.plot([0.5, 0.5],plt.ylim(),'b--');\nplt.text(1, 40, '$L(s)$', color='blue', fontsize=20)\nplt.text(0.5, -100, '$\\omega_b$', color='blue', fontsize=20)\n\nplt.plot(plt.xlim(),[-3, -3],'r--')\nplt.plot([5, 5],plt.ylim(),'r--')\nplt.text(0.01, 5, '$H(s)$', color='orange', fontsize=20)\nplt.text(5, -100, '$\\omega_{bc}$', color='blue', fontsize=20);\n\nfor item in ([ax1.title, ax1.xaxis.label, ax1.yaxis.label] +\n             ax1.get_xticklabels() + ax1.get_yticklabels()):\n    item.set_fontsize(20);\n\n\n\n\n\nMid-Frequency approximation of H and phase margin\n\nFrom the discussions above, we have that: \\(|L(j\\omega)| \\approx 1\\) in the mid-frequency range, and we call gain crossover frequency \\(\\omega_{gc}\\) where \\(L(j\\omega)\\) crosses 0dB.\nAssuming a dominant pole approximation, we expect \\(H\\) to behave similarly to a first-order or a second-order system.\n\nThis can be empirically determined from the phase margin\n\n\\(\\phi_m > 75^o \\Rightarrow\\) first order (single real pole)\n\\(\\phi_m \\le 75^o \\Rightarrow\\) second order (two complex conjugate poles)\n\nEmpirically means that must be verified through simulation/experiments but it is useful to kick off the analysis\n\n\n\n\nLoop analysis: discussions and conclusions\n\nThe closed loop system \\(H\\) has normally a low-pass filter-like behaviour\n\\(H\\) can typically be approximated by a single real pole when \\(\\varphi_m > 75^o\\) (first-order system), or with two complex conjugate poles otherwise\nThe break frequency (for both first and second order systems) approximate the 0dB cut-off frequency of \\(L\\) (also called critical frequency)\nAnd this also approximate the system bandwidth\n\n\nRelationship between dumping ratio and phase margin\n\nThe usage of the break frequency (or critical frequency) \\(\\omega_c\\) as an approximation of the location of the dominant pole does not allow to conclude about whether they are real or complex.\nThis is important because the dynamic behaviour depends on this (e.g., oscillatory vs non-osciallatory behaviour)\nWe can find the following approximation that relates the system damping ratio to the phase margin (in degrees): \\[\\xi \\approx \\frac{\\varphi_m}{100}\\]\nThis holds for \\(0 < \\varphi_m \\le 75^o\\)\n\n\n\n\n\n\n\n##### Sidenote - how to almost get the expression above\n\n\nWe can assume that \\(w_c\\) is a good approximation of the natural frequency of the dominant poles (which are also assumed complex conjugate for now).\n\n\nWe can then calculate: \\[\n|H(j\\omega_c)| = \\frac{|L(j\\omega_c|)}{|1+L(j\\omega_c)|}\n\\]\n\n\nSince \\(|L(j\\omega_c)|=1\\) (\\(\\omega_c\\) is the frequency at which this happens, then: \\(L(j\\omega_c)|= e^{j\\phi_c}\\)\n\n\n\\[\n|H(j\\omega_c)| = \\frac{|L(j\\omega_c|)}{|1+L(j\\omega_c)|} = \\frac{1}{|1+e^{j\\phi_c}|} = \\frac{1}{\\sqrt{(1+cos\\phi_c)^2+sin^2\\phi_c}} = \\frac{1}{\\sqrt{2(1+cos\\phi_c)}} =  \\frac{1}{\\sqrt{2(1-cos\\varphi_m)}} = \\frac{1}{2\\sin(\\varphi_m/2)}\n\\]\n\n\nwhere we used that \\(\\varphi_m = 180 - |\\phi_c|\\)\n\n\n- Since in 06_Intro_to_freq_response_and_bode_plots we calculated that, for the resonant peak of a pair of complex conjugate poles is: \\(|H(j\\omega)|=|\\frac{1}{2\\xi}|\\), we can conclude:\n\n\n\\[\n\\frac{1}{2\\sin(\\varphi_m/2)} = \\frac{1}{|2\\xi|} \\Rightarrow \\xi = \\sin(\\varphi_m/2)\n\\]\n\n\nComments: - We have done multiple assumptions and approximations (\\(\\omega_c = \\omega_n\\), second order system, no zeros) which might not be always valid - The above expression is hence only an approximation.\n\n\nFor this reason it is in general it is preferred to use the following approximation (in degrees):\n\n\n\\[\n\\xi = \\frac{\\varphi_m}{2} = \\frac{\\varphi_m}{2} \\frac{\\pi}{180} \\approx \\frac{\\varphi_m}{100}\n\\]\n\n\n\n\nApproximating \\(H\\) as discussed makes it possible to calculate the transient behaviour of \\(H\\) based on the phase margin \\(\\varphi_m\\) and its gain crossover frequency\n\n\n\nExample: from phase margins to closed loop transient behaviour\n\nRequirement: maximum overshoot \\(S \\le 15\\%\\)\nWe know (see 05_System_response) that the maximum overshoot is: \\[\n\\large\nS = 100e^{-\\frac{\\xi\\pi}{\\sqrt{1-\\xi^2}}}\n\\]\nwe can then solve for \\(\\xi\\): \\[\n\\frac{S}{100} = e^{-\\frac{\\xi\\pi}{\\sqrt{1-\\xi^2}}}\n\\]\n\\[\\Downarrow\\]\n\\[\n\\ln{\\frac{S}{100}} = {-\\frac{\\xi\\pi}{\\sqrt{1-\\xi^2}}}\n\\]\n\\[\\Downarrow\\]\n\\[\n\\ln{\\frac{S}{100}}\\cdot\\sqrt{1-\\xi^2} = {-\\xi\\pi}\n\\]\n\\[\\Downarrow\\]\n\\[\n  \\Big(\\ln{\\frac{S}{100}}\\Big)^2 \\Big({1-\\xi^2}\\Big) = {\\xi^2\\pi^2}\n\\]\n\\[\\Downarrow\\]\n\\[\n  \\Big(\\ln{\\frac{S}{100}}\\Big)^2 = {\\xi^2\\pi^2} + \\Big(\\ln{\\frac{S}{100}}\\Big)^2 \\xi^2\n\\]\n\\[\\Downarrow\\]\n\\[\n  \\xi \\ge \\sqrt{ \\frac{\\Big(\\ln{\\frac{S}{100}}\\Big)^2}{\\Big(\\ln{\\frac{S}{100}}\\Big)^2+\\pi^2}}  \\approx 0.5\n\\]\nand finally:\n\\[\\varphi_m \\approx 100\\xi \\ge 50^o\\]"
  },
  {
    "objectID": "loop-shaping.html#example-car-driving-uphill",
    "href": "loop-shaping.html#example-car-driving-uphill",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Example: Car Driving Uphill",
    "text": "Example: Car Driving Uphill\n\n\n\n\n\n\n\n\n\nSee also 02_Intro_to_control_theory and 04_Block_Diagrams (Section “Example: Car driving uphill”)\n\\[\nG(s) = \\frac{\\gamma}{m}\\frac{1}{s+\\beta/m} = \\frac{10}{s+0.1}\n\\]\n\nAssuming disturbance \\(\\theta=0\\)\nDesign Requirements:\n\nZero tracking error to step inputs\n\\(5\\%\\) tracking error to ramp inputs\nNo overshoot\nSettling time (95%) to step inputs \\(< 15s\\)\n\n\nLet’s see how to translate these requirements into requirements for the loop transfer function and how to design a controller \\(R\\) such that \\(L=RG\\) meets all requirements\n\nZero tracking error to step inputs \\(\\Rightarrow\\) system type 1, we need \\(R\\) with a pole at the origin\n\\(5\\%\\) tracking error to ramp inputs: \\[\n  E(s) = \\frac{1}{1+\\frac{10}{s+0.1}\\frac{K}{s}} \\frac{1}{s^2}\n  \\]\n\nAnd using the final value theorem:\n\\[E_{ss} = \\lim \\limits_{s\\rightarrow0} s\\frac{U(s)}{1+G(s)} = s \\frac{1}{1+\\frac{10}{s+0.1}\\frac{K}{s}} \\frac{1}{s^2}\\]\nWe obtain: \\(K>0.2\\) (or \\(> -14 dB\\))\n\n!cat answers/solution_loop_shaping-1\n\nlim s->0 s*(1/s * (s(s+0.1))/(s^2+0.1s+10k)) <= 0.05\n\n0.1/0.5 <= K\n\n\n\nNo overshoot \\(\\Rightarrow\\) real pole (\\(\\varphi_m \\ge 75^o\\))\nSettling time (95%) to step inputs \\(< 15s\\):\n\nSince \\(t_s = -\\tau \\ln0.05\\) \\(\\Rightarrow\\) Pole Break Frequency \\(\\omega > 0.2\\) rad/s\n\n\nLet’s use the Python Control Library to try out some controllers.\n\n# Process\nG = control.tf([10],[1, 0.1])\nprint('G: {}'.format(G))\n\n# Controller\nK = 1 # K>0.2 is our constraint\nR = control.tf([K],[1, 0])\nprint('R: {}'.format(R))\n\n# Closed loop system\nH = R*G/(1+R*G)\n\nprint('H: {}'.format(H))\n\nNameError: name 'control' is not defined\n\n\nWe can now plot the Bode plots.\n\nWe plot both \\(L\\) and \\(H\\), but we are only interested in understanding how to shape \\(L\\) so that \\(H\\) behaves as we want\nIn plot \\(H\\) is used as a reference.\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# Close Loop H\nmag_cc, phase_cc, omega_cc = control.bode(H, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n\n## Add more information on the plot\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\nplt.text(0.01, 5, '$H(s)$', color='orange', fontsize=20)\n\n\n\n#plt.plot(plt.xlim(),[40-3, 40-3],'b--')\n# Break frequency: w>=0.2 rad/s\nplt.plot([plt.xlim()[0], 0.2], [0, 0],'b--');\nplt.plot([0.2, 0.2], [-60, 0],'b--');\nplt.text(0.01, -40, 'Constraint on break \\nfrequency', color='blue', fontsize=20);\n\nplt.plot(plt.xlim(),[-14, -14],'r--');\nplt.text(11, -25, 'Constraint on LF gain', color='red', fontsize=20);\n\nplt.sca(ax2)                 # phase plot\nplt.plot([3.16, 3.16], [-180, -180+75], 'm--')\nplt.plot([3.16, plt.xlim()[1]], [-180+75, -180+75], 'm--')\nplt.text(4, -135, 'Constraint on Phase Margin', color='m', fontsize=20);\n\n\n\n\n\n[gm, pm, sm, wpc, wgc, wms] = control.stability_margins(R*G)\nprint('Gain Margin: ', gm)\nprint('Phase Margin (deg): {} (at f={})'.format(pm, wgc))\n\n# gm (float or array_like) – Gain margi\n# pm (float or array_like) – Phase margin\n# sm (float or array_like) – Stability margin, the minimum distance from the Nyquist plot to -1\n# wpc (float or array_like) – Phase crossover frequency (where phase crosses -180 degrees), which is associated with the gain margin.\n# wgc (float or array_like) – Gain crossover frequency (where gain crosses 1), which is associated with the phase margin.\n# wms (float or array_like) – Stability margin frequency (where Nyquist plot is closest to -1)\n\nGain Margin:  inf\nPhase Margin (deg): 1.8117006141631293 (at f=3.161487189599213)\n\n\nPhase margin is not enough\nLet’s see what the step response would be:\n\n[t, yout] = control.step_response(H, T=100);\n\nfig, axs = plt.subplots(1, 1, figsize=(10,5))\nplt.plot(t, yout, linewidth=3)\nplt.xlabel('time (s)')\nplt.ylabel('step response')\n\nText(0, 0.5, 'step response')\n\n\n\n\n\n\nThe system overshoots, and oscillates.\nWe need to increase the phase at the frequence where \\(|L|=0 dB\\)\n\nFor example:\n\\[\nR(s) = \\frac{s+0.5}{s}\n\\]\n\n# Process\nG = control.tf([10],[1, 0.1])\nprint('G: {}'.format(G))\n\n# Controller\nR = control.tf([1, 0.5],[1, 0])\nprint('R: {}'.format(R))\n\n# Closed loop system\nH = R*G/(1+R*G)\n\nprint('H: {}'.format(H))\n\nG: \n  10\n-------\ns + 0.1\n\nR: \ns + 0.5\n-------\n   s\n\nH: \n     10 s^3 + 6 s^2 + 0.5 s\n---------------------------------\ns^4 + 10.2 s^3 + 6.01 s^2 + 0.5 s\n\n\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# Close Loop H\nmag_cc, phase_cc, omega_cc = control.bode(H, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\nplt.text(0.01, 5, '$H(s)$', color='orange', fontsize=20)\n\n# Break frequency: w>=0.2 rad/s\nplt.plot([plt.xlim()[0], 0.2], [0, 0],'b--');\nplt.plot([0.2, 0.2], [-60, 0],'b--');\nplt.text(0.01, -40, 'Constraint on break \\nfrequency', color='blue', fontsize=20);\n\nplt.plot(plt.xlim(),[-14, -14],'r--');\nplt.text(11, -25, 'Constraint on LF gain', color='red', fontsize=20);\n\nplt.sca(ax2)                 # phase plot\nplt.plot([3.16, 3.16], [-180, -180+75], 'm--')\nplt.plot([3.16, plt.xlim()[1]], [-180+75, -180+75], 'm--')\nplt.text(4, -135, 'Constraint on Phase Margin', color='m', fontsize=20);\n\n\n\n\nAnd now we have improved substantially the phase margin:\n\n[gm, pm, sm, wpc, wgc, wms] = control.stability_margins(R*G)\nprint('Gain Margin: ', gm)\nprint('Phase Margin (deg): {} (at f={})'.format(pm, wgc))\n\n# gm (float or array_like) – Gain margin\n# pm (float or array_like) – Phase margin\n# sm (float or array_like) – Stability margin, the minimum distance from the Nyquist plot to -1\n# wpc (float or array_like) – Phase crossover frequency (where phase crosses -180 degrees), which is associated with the gain margin.\n# wgc (float or array_like) – Gain crossover frequency (where gain crosses 1), which is associated with the phase margin.\n# wms (float or array_like) – Stability margin frequency (where Nyquist plot is closest to -1)\n\nGain Margin:  inf\nPhase Margin (deg): 87.71326345329157 (at f=10.011962990448804)\n\n\nLet’s verify what the step response is now:\n\n[t, yout] = control.step_response(H, T=30);\n\nfig, axs = plt.subplots(1, 1, figsize=(10,5))\nplt.plot(t, yout, linewidth=3)\n\nplt.grid()\nplt.plot(plt.xlim(), [1+0.05, 1+0.05], 'r--')\nplt.plot(plt.xlim(), [1-0.05, 1-0.05], 'r--')\nplt.xlabel('time (s)')\nplt.ylabel('step response')\n\nText(0, 0.5, 'step response')"
  },
  {
    "objectID": "loop-shaping.html#loop-shaping-some-more-discussions",
    "href": "loop-shaping.html#loop-shaping-some-more-discussions",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Loop shaping: some more discussions",
    "text": "Loop shaping: some more discussions\n\nThe “barriers” on the Bode plots are there to help us shape the desired harmonic response of the loop function\nWe need to translate design requirements into requirements on the Bode plot of the loop funtion\nRepresent these requirements as admissible/non-admissible regions\nThe controller is designed so that the loop function always stays within the admissible regions"
  },
  {
    "objectID": "loop-shaping.html#disturbance-rejections-load-variations",
    "href": "loop-shaping.html#disturbance-rejections-load-variations",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Disturbance rejections: load variations",
    "text": "Disturbance rejections: load variations\n\n\n\n\n\n\n\n\n\n\n\\[\nY(s) = \\frac{1}{1+RG}\n\\]\n\\[\nD(s) = \\frac{1}{1+L}D(s)\n\\]\nSensitivity function: \\[\nS = \\frac{1}{1+L(s)}\n\\]\n\nDescribes how feedback influences the disturbances\nIf \\(|L|=|RG|\\) is large at frequencies where the power of the disturbance is concentrated, then \\(|S|\\) is small and the effect of the disturbance on the output is attenuated\nLower values of \\(|S|\\) means higher attenuation of the external disturbance.\nTypically, plant disturbances are low frequency, and one would like \\(|L|=|RG|\\) to be large at low frequency\n\nComplementary Sensitivity function: \\[\nT = \\frac{L(s)}{1+L(s)}\n\\]\n\nMaps the noise input \\(n\\) to the output \\(y\\)\nNoise rejection defines high frequency specifications\n\\(S+T=1\\)\nNote that \\(T\\) is also the transfer function from \\(y_{ref}\\) to \\(y\\).\nIf \\(|L|=|RG|\\) is small at frequencies where the noise \\(n\\) is concentrated then \\(|T|\\) will be small and the effect of the noise on the output is minimised.\nMeasurement noise tend to occur at high frequency and this means that typically we would like \\(|L|=|RG|\\) to be small at high frequency\nThis constraint does not conflict with the low-frequency constraints for the disturbance \\(d\\) and the reference \\(y_{ref}\\).\n\nLet’s see what happens a bit more:\n\n# Process\nG = control.tf([10],[1, 0.1])\nprint('G: {}'.format(G))\n\n# Controller\nR = control.tf([1, 0.5],[1, 0])\nprint('R: {}'.format(R))\n\n# Closed loop system\nH = R*G/(1+R*G)\nprint('H: {}'.format(H))\n\n# Sensitivity\nS = 1/(1+R*G)\nprint('S: {}'.format(S))\n\n# Complementary Sensitivity\nT = R*G/(1+R*G)\nprint('T: {}'.format(T))\n\nG: \n  10\n-------\ns + 0.1\n\nR: \ns + 0.5\n-------\n   s\n\nH: \n     10 s^3 + 6 s^2 + 0.5 s\n---------------------------------\ns^4 + 10.2 s^3 + 6.01 s^2 + 0.5 s\n\nS: \n  s^2 + 0.1 s\n----------------\ns^2 + 10.1 s + 5\n\nT: \n     10 s^3 + 6 s^2 + 0.5 s\n---------------------------------\ns^4 + 10.2 s^3 + 6.01 s^2 + 0.5 s\n\n\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# Sensitivy function S\nmag_s, phase_s, omega_s = control.bode(S, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n# Complementary Sensitivity T (which also corresponds to the closed loop H)\nmag_t, phase_t, omega_t = control.bode(T, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n### Add more information on the plots\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\nplt.text(0.01, 5, '$T(s) = H(s)$', color='green', fontsize=20)\nplt.text(0.01, -60, '$S(s)$', color='orange', fontsize=20);\n\nplt.plot([10], [0],'r.', markersize=20);\nplt.plot([10, 10], [-60, 0],'r--');\nplt.text(10, 10, 'Break \\nfrequency', color='blue', fontsize=10);\n\n\n\n\n\nIt is not possible to reject load disturbances for frequencies where \\(H\\) is small, i.e., high frequency. But this is OK, since usually load disturbances are at low frequencies.\nOr if we look at the sensitivity function \\(S\\), it is not possible to reject load disturbances for frequencies higher than the break frequency\nThese observations translate into low frequency constraints in the Bode plot of the loop transfer function: we need \\(L\\) large at low frequency to obtain a small \\(S\\).\n\n\\[\n|S(j\\omega)| \\le \\epsilon, \\hspace{0.5cm} \\text{when} \\hspace{0.5cm} \\omega \\in [0, \\omega_n]\n\\]\n\\[\n\\Downarrow\n\\]\n\\[\n|1+L(j\\omega)| \\ge \\frac{1}{\\epsilon}, \\hspace{0.5cm} \\approx |L(j\\omega)| \\ge \\frac{1}{\\epsilon}\n\\]\n\nThis means that rejecting load distubances translates into a new constraint at low frequency in the Bode plot of the loop transfer function\n\nFor instance, using our previous example:\n\nRequirement: attenuation of load disturbances by at least 20dB up until 0.06 rad/s\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n\n### Add more information on the plot\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\n\n# Break frequency: w>=0.2 rad/s\nplt.plot([plt.xlim()[0], 0.2], [0, 0],'b--');\nplt.plot([0.2, 0.2], [-60, 0],'b--');\nplt.text(0.01, -40, 'Constraint on break \\nfrequency', color='blue', fontsize=20);\n\nplt.plot([plt.xlim()[0], 10],[-14, -14],'r--');\nplt.text(1, -25, 'Constraint on LF gain', color='red', fontsize=20);\n\nplt.plot([plt.xlim()[0], 0.06], [20, 20],'g--');\nplt.plot([0.06, 0.06], [-60, 20],'g--');\nplt.text(0.005, -3, 'Constraint on LF\\ndisturbances', color='green', fontsize=20);\n\n\n\nplt.sca(ax2)                 # phase plot\nplt.plot([3.16, 3.16], [-180, -180+75], 'm--')\nplt.plot([3.16, plt.xlim()[1]], [-180+75, -180+75], 'm--')\nplt.text(4, -135, 'Constraint on Phase Margin', color='m', fontsize=20);"
  },
  {
    "objectID": "loop-shaping.html#noise-rejections-measurement-noise",
    "href": "loop-shaping.html#noise-rejections-measurement-noise",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Noise rejections: measurement noise",
    "text": "Noise rejections: measurement noise\n\n\n\n\n\n\n\n\n\n\n\\[\nY(s) = \\frac{RG}{1+RG}N(s) = \\frac{L(s)}{1+L(s)}N(s)\n\\]\nComplementary Sensitivity function: \\[\nT = \\frac{L(s)}{1+L(s)}\n\\]\n\nMaps the noise input \\(n\\) to the output \\(y\\)\nNoise rejection defines high frequency specifications\n\\(S+T=1\\)\nNote that \\(T\\) is also the transfer function from \\(y_{ref}\\) to \\(y\\).\nIf \\(|L|=|RG|\\) is small at frequencies where the noise \\(n\\) is concentrated then \\(|T|\\) will be small and the effect of the noise on the output is minimised.\nMeasurement noise tend to occur at high frequency and this means that typically we would like \\(|L|=|RG|\\) to be small at high frequency\nThis constraint does not conflict with the low-frequency constraints for the disturbance \\(d\\) and the reference \\(y_{ref}\\).\nWe need the complementary sensitivity function \\(\\approx 1\\) at low frequency (steady state) to meet control requirements\nNoise rejection must then be: \\(H(j\\omega) \\ll 1\\) for \\(\\omega \\gg ...\\)\nNote: since noise rejection happens at high frequency, it is not possible to have low tracking errors if sensors are noisy in the LF regime\n\n\\[\n|H(j\\omega)| \\le \\epsilon, \\hspace{0.5cm} \\text{when} \\hspace{0.5cm} \\omega \\in [\\omega_n, \\infty]\n\\]\n\\[\n\\Downarrow\n\\]\n\\[\n|H(j\\omega)|=\\frac{L(j\\omega)}{1+L(j\\omega)} \\le \\epsilon, \\hspace{0.5cm} \\approx |L(j\\omega)| \\le {\\epsilon}\n\\]\n\nThis means that rejecting measurement noise translates into a new constraint at high frequency in the Bode plot of the loop transfer function \\(L(j\\omega)\\).\n\nLet’s see what happens in our previous example:\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n\n#### Add more information on the plots\n# bandwidth\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\n\n# Break frequency: w>=0.2 rad/s\nplt.plot([plt.xlim()[0], 0.2], [0, 0],'b--');\nplt.plot([0.2, 0.2], [-60, 0],'b--');\nplt.text(0.01, -40, 'Constraint on break \\nfrequency', color='blue', fontsize=20);\n\nplt.plot([plt.xlim()[0], 10],[-14, -14],'r--');\nplt.text(1, -25, 'Constraint on LF gain', color='red', fontsize=20);\n\nplt.plot([plt.xlim()[0], 0.06], [20, 20],'g--');\nplt.plot([0.06, 0.06], [-60, 20],'g--');\nplt.text(0.005, -3, 'Constraint on LF\\ndisturbances', color='green', fontsize=20);\n\nplt.plot([2, plt.xlim()[1]], [0, 0],'m--');\nplt.plot([2, 2], [0, 80],'m--');\nplt.text(4, 40, 'Constraint on HF\\nnoise', color='m', fontsize=20);\n\n\n\nplt.sca(ax2)                 # phase plot\nplt.plot([3.16, 3.16], [-180, -180+75], 'm--')\nplt.plot([3.16, plt.xlim()[1]], [-180+75, -180+75], 'm--')\nplt.text(4, -135, 'Constraint on Phase Margin', color='m', fontsize=20);\n\n\n\n\n\nOur controller does not meet noise rejection requirements for \\(\\omega \\; in \\; [2, 10]\\) rad/s\n\n\nControl Sensitivity\n\n\n\n\n\n\n\n\n\n\n\\[\nU(s) = \\frac{R}{1+GR}Y_{ref}(s) = \\frac{R}{1+L}Y_{ref} = Q(s)Y_{ref}\n\\]\nwhere\n\\[\nQ(s)=\\frac{R(s)}{1+L(s)}\n\\]\n\nMakes it possible to analyse the control output \\(u\\) as a function of the reference signal \\(y_{ref}\\) (or of the measurement noise \\(n\\))\nCan be used to include in the control design constraints due to limits on actuators\n\nAs usual, let’s plot the Bode plots of the control sensitivity function \\(Q(s)\\):\n\\[\n|Q(j\\omega)|=\\frac{|R(j\\omega)|}{|1+L(j\\omega)|}\n\\]\n\nSince we would like \\(L\\) large at low frequency:\n\nfor \\(\\omega \\rightarrow 0 \\Rightarrow |Q(j\\omega)| \\approx \\frac{1}{|G(j\\omega)|}\\)\n\nSince we would like \\(L\\) small at high frequency\n\nfor \\(\\omega \\rightarrow \\infty \\Rightarrow |Q(j\\omega)| \\approx {R(j\\omega)}\\)\n\nfor \\(\\omega = \\omega_{c} \\Rightarrow |L(j\\omega)| = 1 \\Rightarrow |R(j\\omega)| = \\frac{1}{|G(j\\omega)|}\\)"
  },
  {
    "objectID": "loop-shaping.html#control-sensitity-consequences",
    "href": "loop-shaping.html#control-sensitity-consequences",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Control Sensitity: Consequences",
    "text": "Control Sensitity: Consequences\n\n\\(|G(j\\omega)| \\ll 1\\), when \\(\\omega \\ge \\omega_{bp}\\)\n\nLow-pass behaviour of the plant\n\n\nConsidering the \\(0\\) dB bandwidth (\\(\\omega_{bp}\\)), then:\n\nif \\(\\omega_c > \\omega_{bp}\\), \\(\\Rightarrow |L(j\\omega)| > 1\\), for \\(\\omega \\in (\\omega_{bp}, \\omega_{c})\\)\n\n\\(|L(j\\omega)|\\) has a larger bandwidth than the plant.\n\n\\(\\Rightarrow\\) \\(|Q(j\\omega)| \\approx \\frac{1}{|G(j\\omega)|} \\gg 1\\), for \\(\\omega \\in (\\omega_{bp}, \\omega_{c})\\)\n\nTake away points:\n- Enlarging the bandwidth of the plant might lead to high control values, which might not be acceptable! - Never extend the bandwidth too much\nTo avoid extending the bandiwidth too much we have more high frequency constraints (for both the Loop transfer Function and the Controller):\n\\[\n|L(j\\omega)| < \\epsilon_u, \\hspace{0.5cm} \\text{for} \\hspace{0.5cm} \\omega \\ge \\omega_u\n\\]\n\\[\n|R(j\\omega)| < \\epsilon_{ru}, \\hspace{0.5cm} \\text{for} \\hspace{0.5cm} \\omega \\ge \\omega_{ru}\n\\]\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\n# Loop Transfer Function L=GR\nmag_ol, phase_ol, omega_ol = control.bode_plot(R*G, dB=True, omega_limits=(0.01, 100), linewidth=5);\n\n\n\n### Add more information on the plot\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(0.4, 40, '$L(s)$', color='blue', fontsize=20)\n\n# Break frequency: w>=0.2 rad/s\nplt.plot([plt.xlim()[0], 0.2], [0, 0],'b--');\nplt.plot([0.2, 0.2], [-60, 0],'b--');\nplt.text(0.01, -40, 'Constraint on break \\nfrequency', color='blue', fontsize=20);\n\nplt.plot(plt.xlim(),[-14, -14],'r--');\nplt.text(11, -25, 'Constraint on LF gain', color='red', fontsize=20);\n\nplt.plot([plt.xlim()[0], 0.06], [20, 20],'g--');\nplt.plot([0.06, 0.06], [-60, 20],'g--');\nplt.text(0.005, -3, 'Constraint on LF\\ndisturbances', color='green', fontsize=20);\n\nplt.plot([2, plt.xlim()[1]], [0, 0],'m--');\nplt.plot([2, 2], [0, 80],'m--');\nplt.text(4, 40, 'Constraint on HF\\nnoise', color='m', fontsize=20);\n\nplt.plot([2, plt.xlim()[1]], [10, 10],'k--');\nplt.plot([2, 2], [10, 80],'k--');\nplt.text(4, 20, 'HF Constraint for R(s)', color='k', fontsize=20);\n\nplt.plot([plt.xlim()[0], 0.5], [10, 10],'k--');\nplt.plot([0.5, 0.5], [-60, 10],'k--');\nplt.text(1, -60, 'LF Constraint for R(s)', color='k', fontsize=20);\n\n\n\nplt.sca(ax2)                 # phase plot\nplt.plot([3.16, 3.16], [-180, -180+75], 'm--')\nplt.plot([3.16, plt.xlim()[1]], [-180+75, -180+75], 'm--')\nplt.text(4, -135, 'Constraint on Phase Margin', color='m', fontsize=20);\n\n\n\n\n\nDesigning a controller, it is not trivial, even in the simple example that we are considering."
  },
  {
    "objectID": "loop-shaping.html#summary",
    "href": "loop-shaping.html#summary",
    "title": "Controller design: Loop analysis and shaping",
    "section": "Summary",
    "text": "Summary\n\nNominal stability: \\(L(0)>0\\) and \\(\\varphi_m>0\\). No critical cancellations in \\(R(s)G(s)\\)\nRobust stability: phase and gain margin. Also, don’t exceed with \\(\\omega_c\\) or you will be very sensitive to delays - remember for a delay \\(\\tau\\), the contribution to the phase is \\(\\omega_c\\tau\\)\nTracking: Use \\(\\omega_c\\) based on the signals you would like to track (faster signals require higher \\(\\omega_c\\)). Use the phase margin to select the damping ratio that you need. More in general, requirements on settling time and overshoot can be translated into limits for \\(\\omega_c\\) and \\(\\varphi_m\\).\nReject disturbance: \\(L(j\\omega)| >> 1\\) at frequency important for the disturbance \\(d\\). Usually this implies a requirment on \\(\\omega_c\\).\nAttenuate noise on measurements: Typically lives at high frequency, and hance this implies a requirment on \\(\\omega_c\\).\nLimits on the control variable: we need to keep \\(R(j\\omega)\\) limited for \\(\\omega > \\omega_c\\), or \\(L(j\\omega) >> |G(j\\omega)\\), for \\(\\omega > \\omega_c\\)\nCausal regulators: need more poles than zeros. This is a requirement on the asymptotic behaviour of \\(L(j\\omega)\\) and \\(G(j\\omega)\\). \\(-k_L \\le -k_G\\) (where \\(k_L\\) is the slope of \\(|L(j\\omega)|\\) for \\(\\omega \\rightarrow \\inf\\), and \\(k_G\\) is the slope of \\(|G(j\\omega)|\\) for \\(\\omega \\rightarrow \\inf\\)\n\n\nDiscussions\n\nToo strigent design requirements can make the controller design very difficult or even impossible\nThe first thing to do is always to verify that requirements are feasible for our problem: sometime it is better to buy better sensors, rather than desiging a controller with requirements too strict\n\n\n\nComputer Aided Control System Design\nNumarically implement analytic rules that we have been discussing, and that can only be manually applied in simple cases.\n\nMATLAB + Simulink + SISOTool: Quite powerful. It is one of the most used.\nPython Control Library: More limited, but can still be effective. This is what we are using!"
  },
  {
    "objectID": "a_assignment.html#laplace-transforms",
    "href": "a_assignment.html#laplace-transforms",
    "title": "Assignment Part 1",
    "section": "Laplace Transforms",
    "text": "Laplace Transforms\n\nCalculate the Laplace Transform \\(X(s)\\) of the signal \\(x(t)\\):\n\n\\[\nx(t) = (4t-3cos(5t))e^{-2t}\n\\]\n\nCalculate the Inverse Laplace Transform \\(g(t)\\) of the transfer function \\(G(s)\\):\n\n\\[\nG(s)=2 + \\frac{3}{2s^3+3s^2+s}\n\\]"
  },
  {
    "objectID": "a_assignment.html#block-diagrams",
    "href": "a_assignment.html#block-diagrams",
    "title": "Assignment Part 1",
    "section": "Block Diagrams",
    "text": "Block Diagrams\n\nCalculate the equivalent transfer function \\(G_t(s)=\\frac{Y(s)}{X(s)}\\) of the following block diagram:"
  },
  {
    "objectID": "a_assignment.html#system-response",
    "href": "a_assignment.html#system-response",
    "title": "Assignment Part 1",
    "section": "System Response",
    "text": "System Response\n\nWrite the transfer function to a step input of a second order system characterised by:\n\n\nStatic gain \\(G(0)=5\\)\nDamping ratio \\(\\xi=0.5\\)\nSettling time \\(t_s=3 s\\)\nNo zeros\n\n\nPlot the qualitative behaviour of the step response of the system:\n\n\\[\nG(s)=\\frac{20(3+0.1s)(s^2+10s+160)}{(2s+10)(0.1s+5)(s^2+2s+400)}\n\\]\n\nFor the system \\(G(s)\\) defined above, calculate:\n\n\nIts steady state value \\(y_\\infty\\) to a step input (when \\(t \\rightarrow \\infty\\))\nIts settling time \\(t_s\\)\nIf the system oscillates, calculate the period \\(T_\\omega\\) of the oscillation"
  },
  {
    "objectID": "a_assignment.html#routh-criterion",
    "href": "a_assignment.html#routh-criterion",
    "title": "Assignment Part 1",
    "section": "Routh Criterion",
    "text": "Routh Criterion\n\nDetermine the values of \\(K\\) for which the following feedback system is asymptotically stable:"
  },
  {
    "objectID": "a_assignment.html#bode-plots",
    "href": "a_assignment.html#bode-plots",
    "title": "Assignment Part 1",
    "section": "Bode Plots",
    "text": "Bode Plots\n\nPlot the Bode amplitude and phase plots for the system \\(G_d(s)\\) discussed in question 7."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feedback Control",
    "section": "",
    "text": "Teaching semester: SPRING 2023\nLanguage of instruction: Italian and English"
  },
  {
    "objectID": "index.html#course-coordinator",
    "href": "index.html#course-coordinator",
    "title": "Feedback Control",
    "section": "Course coordinator",
    "text": "Course coordinator\nAndrea Munafo"
  },
  {
    "objectID": "index.html#lecturers",
    "href": "index.html#lecturers",
    "title": "Feedback Control",
    "section": "Lecturer(s)",
    "text": "Lecturer(s)\nAndrea Munafo"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Feedback Control",
    "section": "Install",
    "text": "Install\nThe notebooks run with python 3.9 and use the following python libraries: - sympy - python control - numpy - pandas - matplotlib - opencv-python\nNotebook 01_Getting_started_with_Python_and_Jupyter_Notebook.ipynb provides a short introduction on how to set up an anaconda environment to get you started.\nTo use all notebooks you might need to install the feedback control package. You can do this entering this into your terminal:\npip install -e '.[dev]'\nThis is the recommended way to make a Python package importable from anywhere in your current environment:\n\n-e – short for “editable”, lets you immediately use changes made to your package during development.\n. – refers to the current directory.\n[dev] – includes “development” requirements: other packages that your notebooks use solely for documentation or testing."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Feedback Control",
    "section": "How to use",
    "text": "How to use\nEach notebook is thought to be independent from every other, so it is possible to run them in any order you prefer."
  },
  {
    "objectID": "index.html#acknowledgements-and-references",
    "href": "index.html#acknowledgements-and-references",
    "title": "Feedback Control",
    "section": "Acknowledgements and references",
    "text": "Acknowledgements and references\n\nImages above are from the paper Conlan Cesar, Benjamin Whetton, Michael DeFilippo, Michael Benjamin, Michael Sacarny, Scott Reed, Andrea Munafo, Coordinating Multiple Autonomies to Improve Mission Performance, OCEANS 2021 MTS/IEEE, October, 2021\nSome of the images and content used in the notebooks have been based on resources available from engineeringmedia.com. The website and the youtube videos are a fantastic resource on control systems.\nThe pendulum example is inspired by Control tutorials for Matlab and Simulink\nRelevant textbooks used to prepare these notebooks are reported in 00_Syllabus.ipynb."
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Feedback Control",
    "section": "Additional resources",
    "text": "Additional resources\n\nControl systems academy\nProcess Dynamics and Control in Python\nKarl J. Åström and Richard M. Murray, Feedback Systems: An Introduction for Scientists and Engineers\nLecture series on Control Engineering by Prof. Madan Gopal\nDesigning Lead and Lag Compensators in Matlab and Simulink\nControl Systems Basics"
  },
  {
    "objectID": "nyquist_stability_criterion.html",
    "href": "nyquist_stability_criterion.html",
    "title": "Nyquist Stability Criterion",
    "section": "",
    "text": "Note: to use FuncAnimation please uncomment line: %matplotlib notebook in each cell. It is currently commented to avoid errors in github CI."
  },
  {
    "objectID": "nyquist_stability_criterion.html#open-and-close-loop-analysis",
    "href": "nyquist_stability_criterion.html#open-and-close-loop-analysis",
    "title": "Nyquist Stability Criterion",
    "section": "Open and Close Loop Analysis",
    "text": "Open and Close Loop Analysis\nLet’s consider a typical open loop control system, with plant \\(G(s)\\) and controller \\(H(s)\\):\n\n\n\n\n\n\nWe typically know both \\(G(s)\\) and \\(H(s)\\)\nThe open loop transfer function is \\(G(s)H(s)\\)\n\nWhen we close the loop:\n\n\n\n\n\n\nThis changes the transfer function for the entire system, which becomes:\n\n\\[\n\\frac{G(s)}{1+G(s)H(s)}\n\\]\nWe can then differentiate two cases:\nOpen Loop case\nTo find out if the open loop system is stable: - analyse the poles of \\(G(s)H(s)\\) (when this transfer function goes to infinity) - if there is a pole with \\(Re > 0\\), then the open loop transfer function is unstable\nClosed Loop case\nTo find out if the closed loop system is stable:\n\\[\n\\frac{G(s)}{1+G(s)H(s)}\n\\] - analyse the denominator: \\(1+G(s)H(s) = 0\\) (so that the closed loop transfer function goes to infinity) - zeros (roots of the numerator) of \\(1+G(s)H(s) = 0\\) causes the closed loop transfer function to go to infinity - if there is a root with \\(Re > 0\\), then the closed loop transfer function is unstable\nNote we are assuming that \\(G(s)\\) and \\(H(s)\\) are known, so poles and zeros of \\(G(s)H(s)\\) are known. We can analyse this function and make a determination on whether the system is open loop stable or not.\nAt the same time, note that we would like to understand how the closed loop system behaves.\nGoal - When we want to study the stability of the closed loop system: - We want to find the roots of \\(1+𝐺(𝑠)𝐻(𝑠)=0\\), - or take the open loop transfer function \\(G(s)H(s)\\), add 1 and find its zeros.\nWhy is this difficult - The system can be high order (e.g. order 50), - Finding zeros would only give us stability information, - Other information could be useful (e.g. stability margins).\nLet’s understand this a little better plotting poles and zeros of a transfer function.\nWe use the Python Control Library, and define our transfer function that we call sys:\n\nsys = control.tf([1, 3, 5, 7], [2, 4, 6, 8, 2])\n\n\nprint(sys)\n\n\n     s^3 + 3 s^2 + 5 s + 7\n-------------------------------\n2 s^4 + 4 s^3 + 6 s^2 + 8 s + 2\n\n\n\nNow we plot the poles and zeros for the open loop system \\(G(s)H(s)\\). And for the stability we are interested in the location of the poles, plotted in red in the picture.\nWe can use the pzmap function of the Python Control Library to plot poles and zeros.\n\nfig = plt.figure(figsize=(10, 5))\n\n# Calls function pzmap which calculates poles and zeros\ncontrol.pzmap(sys, title='Pole Zero Map for the Open Loop System GH');\n       \n# What is really happening is that pzmap is plotting the roots of the denominator (poles) \n# and the roots of the numerator (zeros). We can show this plotting the roots explicitely:\n\n# zeros - roots of the numerator\nplt.plot(np.real(np.roots([1, 3, 5, 7])), np.imag(np.roots([1, 3, 5, 7])),'b.');\n\n# poles - roots of the denominator\nplt.plot(np.real(np.roots([2, 4, 6, 8, 2])), np.imag(np.roots([2, 4, 6, 8, 2])),'rx');\n\n\n\n\nAnd now let’s plot poles and zeros of the closed loop system.\nNote: - we are now interested in the location of the zeros, plotted in red in the picture; we are interested in \\(1+G(s)H(s)\\).\n\nfig = plt.figure(figsize=(10, 5))\n\n\ncontrol.pzmap(sys+1, title='Pole Zero Map for the Closed Loop System 1+GH');\n\n# zeros - roots of the numerator -  these are the poles of the Closed loop transfer function\nplt.plot(np.real(np.roots(np.array([2, 4, 6, 8, 2])+np.array([0, 1, 3, 5, 7]))), \n         np.imag(np.roots(np.array([2, 4, 6, 8, 2])+np.array([0, 1, 3, 5, 7]))),'r.') \n\n# poles - roots of the denominator\nplt.plot(np.real(np.roots([2, 4, 6, 8, 2])), np.imag(np.roots([2, 4, 6, 8, 2])),'bx');\n\n\n\n\n\nThere is not a clear relationship between the poles of \\(G(s)H(s)\\) and the zeros of \\(1+G(s)H(s)\\),\nWhen we add 1 this can really move the location of the roots substantially\n\nLet’s instead plot the Nyquist diagram. For now we just call the nyquist_plot function from the Python Control Library:\n\nfig = plt.figure(figsize=(10, 5))\n# nyquist_plot to plot the Nyquist plot\ncontrol.nyquist_plot([sys, 1+sys]);\n\n\nplt.legend(['sys', 'sys', '-1', 'sys+1', 'sys+1'], loc='upper right');\n\n\n\n\nAnd on the Nyquist plot it is simple to see that we are really only adding 1 and shifting it accordingly. - Adding 1 and calculate the how this affects zeros/poles is difficult - Adding 1 to a Nyquist plot is easy to do\nIf we know how to understand the stability from the Nyquist plot then everthing becomes easier\n\nHow to generate the Nyquist Plot\n\nOnce we know how to generate a Nyquist plot, it is easy to understand how it relates to stability\nKey is the Cauchy’s Argument Principle\n\n\nCauchy’s Argument Principle\nLet’s take an arbitraty transfer function:\n\\[\nG(s) = \\frac{s+3}{s+2}\n\\]\n\nPole \\(s=-2\\)\nZero \\(s=-3\\)\n\n\nWe can plot poles and zeros in the s-plane:\n\n\n\n\n\nIf we take one point in the s-plane, for example: \\(s=-2+1j\\), we get another complex number when we plug it in \\(G(s)\\):\n\\[\n\\frac{s+3}{s+2} = \\frac{(-2+1j)+3}{(-2+1j)+2} = \\frac{1+1j}{j} = \\frac{1+1j}{j}\\frac{-j}{-j} = 1 -j\n\\]\nAnd we can plot this new complex number on another plane (that we can call \\(\\Omega\\)-plane):\n\n\n\n\n\nOur transfer function is mapping a point from the s-plane into a new point into the new plane \\(\\Omega\\) - As we pick more and more points in the s-plane, forming a continuous line in the s-plane (contour), they will form a continuous line in the \\(\\Omega\\)-plane - And if we pick points on a contour in the s-plane (a line that closes up on itself), it becomes a closed line in the \\(\\Omega\\)-plane (we call it a plot) - The plot contains the magnite and phase information for each of the system’s poles and zeros.\n\nLet’s verify this in Python:\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 10))\n     \n########################################\n# This is only to add some more context to the plots (axis, grids and scales the axis)\nfor i in range(len(axs)):\n    axs[i].plot([-10, 10], [0, 0], color='black', linewidth=3, linestyle='--') # axis\n    axs[i].plot([0, 0], [-10, 10], color='black', linewidth=3, linestyle='--') # axis\n    axs[i].axis([-4, 3, -3, 3]) # scales the axis\n    axs[i].grid() # add grid\n    axs[i].set_aspect('equal', 'box')\n    axs[i].set_xticks([-3, -2, -1, 1, 2, 3])\n    axs[i].set_xlabel('Real')\n    axs[i].set_ylabel('Imag');    \naxs[0].set_title('s-plane')\naxs[1].set_title('$\\Omega$-plane')\n########################################        \n    \n# plot our poles and zeros in the s-plane    \naxs[0].plot(-3, 0, color='blue', marker='o', markersize=12)\naxs[0].plot(-2, 0, color='red', marker='x', markersize=12)\n\n# This is the interesting part:  \n#    Map between s-plane and Omega plane.\n#    Thefor loop is simply doing the same operation for all points selected in the\n#    s-plane (s_point). This is what we have done manually above.\nfor xi in np.arange(0, 6.28, 0.1):\n    s_point = np.sin(xi), 3*np.cos(xi) # pick one s-point\n    axs[0].plot(s_point[0],s_point[1], # plot the s-point in the s-plane\n                marker='.', color='m', markersize=10)\n    \n    # Map one s_point to a W_point\n    W_point = (complex(s_point[0],s_point[1]) + 3)/(complex(s_point[0],s_point[1]) + 2)    # This is our transfer function\n    axs[1].plot(np.real(W_point), np.imag(W_point), marker='.', color='r', markersize=10)  # plot the point\n\n\n\n\n\nThe contour in the \\(\\Omega\\)-plane is the codomain of the Transfer function and for this reason it contains information on the magnitude and phase of the zeros and poles of the transfer function.\nThis is useful to calculate the stability of the transfer function\n\n\nFor simplicity let’s now consider a simpler transfer function, with only one zero:\n\\[\nG(s)=s+2\n\\]\n\nIf we take the a point on the s-plane: \\(s=-1+j\\),\nit becomes the point: \\((-1+j)+2=1+j\\) in the \\(\\Omega\\)-plane.\n\n\n\n\n\n\n\n\n\n\nIt is important to note that the new point in the \\(\\Omega\\)-plane and the phasor between the zero and the point we chose in the s-plane is the same phasor\nThis extends to multiple poles and zeros\nThis means that it is possible to plot the mapping between the two planes graphically\n\nTo do this:\n\n\nPick the point in the s-plane that you want to map over to the \\(\\Omega\\)-plane\n\n\nDraw all the phasors from zeros and poles to the point\n\n\nCalculate the magnitude multiplying the magnitudes of the zeros and dividing by the magnitude of the poles\n\n\nThis resulting magnitude is the length of the phasor in \\(\\Omega\\)-plane\n\n\nCalculate the phase adding all the zero phases and subtracting all the pole phases\n\n\n\\[\nG(s)H(s) = \\frac{(s-\\beta_1)(s-\\beta_2)...}{(s-\\alpha_1)(s-\\alpha_2)...}\n\\]\n\n\n\n\n\n\n\n\nThis is called a Polar plot, they tell us about the magnitude and phase contribution of a system at various frequency.\n\n\nLet’s understand now what happens when we move along an arbitrary contour:\n\n\n\n\n\n\n\nFor each point along the contour we apply the same reasoning:\n\nCalculate the magnitude multiplying the magnitudes of the zeros and dividing by the magnitude of the poles\nCalculate the phase adding all the zero phases and subtracting all the pole phases\n\nNote that, as we move along the contour:\n\nthe zero \\(\\beta_1\\) will contribute a net angle of \\(-2\\pi\\)\nthe zero \\(\\beta_2\\) will contribute a net angle of \\(0\\) (as we move, for each positive angle there is a negative angle). This is true for every other pole or zero out of the contour.\n\nUsing this insight we can say from a qualitative standpoint:\n\nThe resulting contour is the \\(\\Omega\\)-plane will encircle the origin once and only one, in the clockwise direction because the angle is \\(-2\\pi\\).\nWe are not interested in the specific shape, but we know that will encircle the origin\n\nWhat happens if there is a pole inside the contour?\n\n\n\n\n\n\n\nAs we move along the contour:\n\nthe pole inside the contour will contribute a total angle of \\(-2\\pi\\)\nbut a pole is at the denominator of \\(G(s)H(s)\\) and hence this contributes a further negative phase: hence the total angle is \\(2\\pi\\)\nwe still encircle the origin of the \\(\\Omega\\)-plane once but now in the counter-clockwise direction\n\n\n\nThe following Python code, demonstrates how the mapping works for the following cases: - one pole encircled by the contour in the s-plane - one zero encircled by the contour in the s-plane - one pole not-encircled - more complex transfer functions: - no point goes around a pole or a zero - points go around a pole or a zero\nComment and uncomment the various parts to see how points in the s-plane are mapped into points in the \\(\\Omega\\)-plane.\nAnd of course, you can change the code below to experiment yourself!\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 10));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\n\n################################\n# The first three contours have been demonstrated in class for cases 1, 2, 3 below\n# 1. First contour: no point goes around a pole or a zero\n# contour = 1\n# xoff = -2 #-0.75  # X-offset of the contour\n# yoff = 0; #%0.75;  # Y-offset of the contour\n# angles = np.linspace(2*np.pi, 0, 160);     # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n# 2. Second contour: points go around a zero\n# contour = 2\n# xoff = 0.5  # X-offset of the contour\n# yoff = 0;   # Y-offset of the contour\n# angles = np.linspace(np.pi, -np.pi, 160);     # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n# 3. Third contour: no point goes around a pole or a zero\n# contour = 3\n# xoff = -3  # X-offset of the contour\n# yoff = 2.5;   # Y-offset of the contour\n# angles = np.linspace(np.pi, -np.pi, 160);     # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n################################\n# Contours 4 and 5 have been demonstrated in class for simpler transfer functions, for cases 4, 5 and 6\n# 4. Fourth contour: goes around a zero (ex.)\ncontour = 4\nxoff = -3   # X-offset of the contour\nyoff = 0;   # Y-offset of the contour\nangles = np.linspace(np.pi, -np.pi, 160);     # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n# 5. Fifth contour: does not go around a zero (ex.)\n# contour = 4\n# xoff = 0    # X-offset of the contour\n# yoff = 0;   # Y-offset of the contour\n# angles = np.linspace(np.pi, -np.pi, 160);     # Generate angles around the circle; 160 is tje Number of points around the circular contour\n################################\n\n\n# define the points in the countour based on the parameters of the countours defined above    \npt = [np.cos(angles)+xoff, np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n\n    # Define the system transfer function, uncomment the one you're running\n\n    ##################################################\n    # CASE 1: 2 poles and 1 zero (!)\n#     tnf = s/(s**2 + 6*s + 18)\n#     axs[0].plot([-3], [3], color='blue', marker='x', markersize=10);\n#     axs[0].plot([-3], [-3], color='blue', marker='x', markersize=10);\n#     axs[0].plot([0], [0], color='blue', marker='o', markersize=10);\n#     axs[0].set_xlim([-5, 2])\n#     axs[0].set_ylim([-5, 5])    \n    \n    # CASE 2: 2 poles and 2 zeros\n    # tnf = (s**2 + 1.5*s + 0.8125)/((s + 1)*(s + 0.8))\n    # axs[0].plot([-1, -.8] , [0, 0], color='blue', marker='x', markersize=10);\n    # axs[0].plot([-0.75, -0.75], [.5, -.5], color='blue', marker='o', markersize=10);\n    # axs[0].set_xlim([-3, 2])\n    # axs[0].set_ylim([-2, 2]);\n\n    # CASE 3: 2 poles only\n    # tnf = 1/((s + 1)*(s + 0.8)); \n    # axs[0].plot([-1, -.8] , [0 0], color='blue', marker='x', markersize=10);\n    # axs[0].set_xlim([-3, 2])\n    # axs[0].set_ylim([-2, 2]);\n    \n    ##################################################\n    # CASE 4: 1 zero \n    # Demonstrated with the fourth contour to show how the mapped contour moves around the origin\n    # when the s-plane contour goes around the zero\n    tnf = (s + 3); \n    axs[0].plot([-3] , [0], color='blue', marker='.', markersize=10);\n    axs[0].set_xlim([-4, 2])\n    axs[0].set_ylim([-2, 2]);   \n\n    # CASE 5: 1 pole \n    # Demonstrated with the fourth contour to show how the mapped contour moves around the origin\n    # when the s-plane contour goes around the pole\n#     tnf = 1/((s + 3)); \n#     axs[0].plot([-3] , [0], color='blue', marker='x', markersize=10);\n#     axs[0].set_xlim([-4, 2])\n#     axs[0].set_ylim([-2, 2]);\n    \n    # CASE 6: 1 pole\n    # Demonstrated with the fifth contour to show how the mapped contour moves around the origin\n    # when the s-plane contour DOES NOT go around the pole\n#     tnf = 1/((s + 1)); \n#     axs[0].plot([-1] , [0], color='blue', marker='x', markersize=10);\n#     axs[0].set_xlim([-3, 2])\n#     axs[0].set_ylim([-2, 2]);\n    \n    \n    #####\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');\n    # Sets the axis limits    \n    if contour == 1 or contour == 2:\n        axs[1].set_xlim([-.5, .5])\n        axs[1].set_ylim([-.5, .5]);    \n    elif contour == 4 or contour == 5:\n        axs[1].set_xlim([-2, 4]);\n        axs[1].set_ylim([-3, 2]);\n    else:\n        axs[1].set_xlim([-3, 2])\n        axs[1].set_ylim([-3, 2]);       \n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\n\n\nNotes\n\nWhen we select a contour choosing points in the s-plane that never go around a pole or a zero, then the phase never goes around 360 degree. In the example above, the phase stays within approximately 150 and 210 degrees (countour 1 in the code cell above, case 1)\nWhen we go around a zero in a clockwise direction we would add 360 degree of phase as we move around the zero, and the point in the \\(\\Omega\\)-plane rotates 360 degree in the clockwise direction (see contour 2 in the code cell above, case 1)\nWhen we go around a pole in a clockwise direction we would subtract 360 degree of phase as we move around the pole, and the point in the \\(\\Omega\\)-plane rotates 360 degree in the counter-clockwise direction (see contour 3 in the code cell above, case 1)\n\nWhat is happening becomes clearer when we plot what happens for a single pole: - moving around the pole in a clockwise direction, and the resulting plot circles the origin once in the counter-clockwise direction since we are subtracting 360 degree of phase\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 5));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\nxoff = -0.75  # X-offset of the contour\nyoff = 0;     # Y-offset of the contour\nangles = np.linspace(2*np.pi, 0, 160);  # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n\npt = [np.cos(angles)+xoff, np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n\n    # Define the system transfer function\n   \n    #  1 pole\n    tnf = 1/((s + 1)); \n    axs[0].plot([-1] , [0], color='blue', marker='x', markersize=10);\n    axs[0].set_xlim([-3, 2])\n    axs[0].set_ylim([-2, 2]);\n\n\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');        \n    axs[1].set_xlim([-3, 2])\n    axs[1].set_ylim([-3, 2]);\n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\n\nIf we now add a second pole, there will be two rotations around the origin, one for each pole\n\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 5));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\nxoff = -0.75  # X-offset of the contour\nyoff = 0;     # Y-offset of the contour\nangles = np.linspace(2*np.pi, 0, 160);  # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n\npt = [np.cos(angles)+xoff, np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n\n    # Define the system transfer function\n   \n    #  2 poles only\n    tnf = 1/((s + 1)*(s + 0.8)); \n    axs[0].plot([-1] , [0], color='blue', marker='x', markersize=10);\n    axs[0].plot([-.8], [0], color='blue', marker='x', markersize=10);\n    axs[0].set_xlim([-3, 2])\n    axs[0].set_ylim([-2, 2]);\n\n\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');        \n    axs[1].set_xlim([-3, 2])\n    axs[1].set_ylim([-3, 2]);\n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\nFinal example - we add two more zeros - we do not circle the origin at all: we are adding 360 degree twice for the zeros but at the same time we are subtracting the phase of the two poles.\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 5));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\nxoff = -0.75  # X-offset of the contour\nyoff = 0;     # Y-offset of the contour\nangles = np.linspace(2*np.pi, 0, 160);  # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n\npt = [np.cos(angles)+xoff, np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n    \n    # Define the system transfer function\n    # 2 poles and 2 zeros\n    tnf = (s**2 + 1.5*s + 0.8125)/((s + 1)*(s + 0.8))\n    axs[0].plot([-1] , [0], color='blue', marker='x', markersize=10);\n    axs[0].plot([-.8], [0], color='blue', marker='x', markersize=10);\n    axs[0].plot([-0.75], [.5], color='blue', marker='o', markersize=10);\n    axs[0].plot([-0.75], [-.5], color='blue', marker='o', markersize=10);\n    axs[0].set_xlim([-3, 2])\n    axs[0].set_ylim([-2, 2]);\n\n\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');        \n    axs[1].set_xlim([-3, 2])\n    axs[1].set_ylim([-3, 2]);\n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\n\n\n\nCauchy’s argument principle\n\n\nWe can tell the relative difference between the number of poles and zeros inside of a contour by counting how many time the plot circles the origin and in which direction\n\n\n\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 5));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\nxoff = 1.5  # X-offset of the contour\nyoff = 0;     # Y-offset of the contour\nangles = np.linspace(2*np.pi, 0, 160);  # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n\npt = [np.cos(angles)+xoff, np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n    \n    # Define the system transfer function    \n    tnf = (s-1)*(s-1.5)/(s-1.7)\n#     axs[0].plot([1.7] , [0], color='blue', marker='x', markersize=10);\n#     axs[0].plot([1], [0], color='blue', marker='o', markersize=10);\n#     axs[0].plot([1.5], [0], color='blue', marker='o', markersize=10);\n    axs[0].set_xlim([-1, 4])\n    axs[0].set_ylim([-2, 2]);\n\n\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');        \n    axs[1].set_xlim([-2.5, 2.5])\n    axs[1].set_ylim([-2.5, 2.5]);\n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\nQuestion: - Given the contour in red above, and the corresponding clockwise mapping in green, - what can you tell about the number of poles and zeros inside the red contour?\n\n# Uncomment the following line to see the solution\n!cat answers/solution_nyquist_1\n\n\n* Circles the origin once in the clockwise direction, so there must be one more zero than poles inside the contour\n\n* It is important to note that we can only tell about the relative difference between poles and zeros\n\n\nLet’s see another example:\n\n# %matplotlib notebook\nfig, axs = plt.subplots(1, 2, figsize=(10, 5));\nline_0, = axs[0].plot([])     # A tuple unpacking to unpack the only plot\nline_1, = axs[1].plot([])     # A tuple unpacking to unpack the only plot\naxs[0].grid()\naxs[1].grid()\n\nxoff = -1  # X-offset of the contour\nyoff = 0;     # Y-offset of the contour\nangles = np.linspace(2*np.pi, 0, 160);  # Generate angles around the circle; 160 is tje Number of points around the circular contour\n\n\npt = [2*np.cos(angles)+xoff, 2*np.sin(angles)+yoff]; \n\ndef animate_plot(bin):    \n    # Set 's' to the next point on the contour\n    s = complex(pt[0][bin], pt[1][bin]);\n\n    # Plot the next point on the contour in the s-plane    \n    line_0 = axs[0].plot(np.real(s), np.imag(s), color='red', marker='.', markersize=15);\n    axs[0].set_title('S-plane Contour');\n    \n    # Define the system transfer function\n    tnf = (s+1)/((s+1.7)*(s+2.8)*(s+2))\n    axs[0].set_xlim([-4, 4])\n    axs[0].set_ylim([-2, 2]);\n\n    # Plot the mapping from the TF in the w-plane    \n    line_1 = axs[1].plot(np.real(tnf), np.imag(tnf), color='g', marker='.', markersize=15);\n    axs[1].plot(0, 0, marker='.', color='k', markersize=15);\n    axs[1].set_title('$\\Omega$-plane Plot');        \n    axs[1].set_xlim([-5, 10])\n    axs[1].set_ylim([-10, 10]);\n\n    return line_0, line_1\n\nanim = FuncAnimation(fig, animate_plot, frames=len(angles), interval=20)\nanim;\n\n\n\n\n\n\n\nQuestion: - Given the contour in red above, and the corresponding counterclockwise mapping in green, - what can you tell about the number of poles and zeros inside the red contour?\n\n# Uncomment the following line to see the solution\n!cat answers/solution_nyquist_2\n\n\n* Circles the origin twice in the counter-clockwise direction, so there must be two more poles than zeros inside the contour.\n\n\n\n\n\nThe Nyquist plot\n\nWe can now apply the Cauchy’s principle argument\nWe would like to know if there are zeros of \\(1+GH\\) in the right half plane (in which case the system is unstable)\nWe need a contour that includes the entire right half plane:\n\nincludes the entire imaginary axis\ngoes up until \\(+j\\omega\\)\nsweeps around at \\(\\infty\\) to enclose the entire RHP\ngoes back to the imaginary axis at \\(-j\\omega\\) and up to 0\n\n\n\n\n\n\n\n\nThis is called the Nyquist contour\nNote that it avoids the poles on the imaginary axis\nMapping the Nyquist contour on the \\(\\Omega\\)-plane we obtain the Nyquist plot\n\nTo draw it, we need to plug in each single point along the \\(j\\omega\\) axis and all the points at \\(\\infty\\) in the right half plane\nWe need to repeat the steps we saw before. Will see how to do this in a moment.\n\nIf we take \\(1+GH\\) to map the Nyquist contour (in the s-plane) to the Nyquist plot (in the \\(\\Omega\\)-plane)\nWe can see how many times the origin is circled and in which direction\nDetermine how many more poles or zeros are inside the contour.\nWe could directly analyse \\(1+GH\\) (i.e., plot the Nyquist plot of \\(GH\\) and then shift the plot by 1).\nOr which is simpler:\n\nplot \\(GH\\) and shift the origin to the left by 1: we can look at how many circling of the point \\(-1+0j\\) the plot of \\(GH\\) does.\n\n\nSteps:\n\nTake the open loop transfer function \\(GH\\)\nPlot the Nyquist plot of \\(GH\\) (plug in all the points along the Nyquist contour)\nCount the number of times the point \\(-1\\) is encircled and in which direction\nDetermine the relative number of poles and zeros inside the nyquist contour.\n\nNote - The Nyquist method tells us about the relative number of poles and zeros - To know exactly how many zeros of \\(1+GH\\) we have in the RHP, we need to know how many poles we have in the RHP - We typically know how many poles \\(1+GH\\) has in the RHP: they are the same as those of the open loop \\(GH\\) system (and we usually know those)\nLet’s see why:\n\nThe open loop system is: \\[\nGH = \\frac{N_G}{D_G}\\frac{N_H}{D_H}\n\\]\nAnd: \\[\n1+GH = 1+\\frac{N_G}{D_G}\\frac{N_H}{D_H} = \\frac{D_G D_H + N_G N_H}{D_G D_H}\n\\]\n\nWe can know state the famous Nyquist equation:\nTherefore: \\[\nZ = N + P\n\\]\nwhere - \\(Z\\) is the number of zeros in the right half plane (or poles in closed loop) - \\(N\\) is the number of clockwise encirclements of -1 - \\(P\\) is the number of open loop right half plane poles\nNote: - \\(N\\) is the number of encirclement of -1 clock wise. - It is an algebric quantity - If we have counter clockwise encirclements we need to invert its sign\n\n\nOpen loop stable\n\nWhen \\(P=0\\) (open loop system is stable)\n\\(Z=N \\Rightarrow N=0\\) or no encirclements of the point \\(-1\\) to have a stable closed loop system (for stability \\(Z=0\\)).\nThis is the condition most often encountered\n\n\n\nOpen loop unstable\n\nIn order to guarantee that there are no zeros (roots) in the right half plane, you need to have exactly 1 counter-clockwise (CCW) encirclement of the point -1 for each open loop pole in the right half plane:\n\n\\[\nP = -N\n\\]\nNote - To know how many zeros of \\(1+GH\\) (roots ot the numerator) are in the right half plane - We must know the unstable open loop poles of \\(GH\\) - Some information on the open loop system is needed\nComments - With the Nyquist plot we can anaylse the stability of a system with an unstable open loop plant. - We cannot use Bode plots to analyse the stability of closed loop systems when the open loop is unstable. We might obtain a wrong answer.\n\nBode plots are easy to sketch, but not as powerful to analyse the system stability."
  },
  {
    "objectID": "nyquist_stability_criterion.html#estimating-nyquist-plots",
    "href": "nyquist_stability_criterion.html#estimating-nyquist-plots",
    "title": "Nyquist Stability Criterion",
    "section": "Estimating Nyquist Plots",
    "text": "Estimating Nyquist Plots\nGiven a transfer function:\n\\[\nG(s) = \\frac{1}{s^2+3s+2}\n\\]\nwith poles: \\(s=-1, s=-2\\)\n\n\n\n\n\n\nWe can select a point on the contour, plug it into the transfer function and calculate the result:\n\n\\[\nG(s) = \\frac{1}{s^2+3s+2}\n\\Rightarrow\n\\frac{1}{(2j)^2+3\\cdot(2j)+2} = \\frac{1}{-4+2 + 6j} =  \\frac{1}{-2 + 6j} \\frac{-2 - 6j}{-2 - 6j} = \\frac{-2 - 6j}{40} = -0.05 - 0.15j\n\\]\n\nThis would give us one point in the \\(\\Omega\\)-plane\nWe can do it for every single point in the \\(s\\)-plane…\n\nWe can identify two main pieces of the Nyquist contour:\n\n\nThe imaginary axis (the \\(j\\omega\\) line), with \\(\\omega \\in [-\\infty, +\\infty]\\).\n\nWe can then replace \\(s=j\\omega\\) in the transfer function to calculate it.\nGiven the symmetry of poles and zeros about the real axis, the negative portion is just the reflection of the positive portion of the axis.\nWe only need to calculate positive frequencies.\n\nThe rest of the countour: starts at \\(+\\infty\\) on the imaginary axis, goes all around to get to \\(+\\infty\\) on the real axis and then goes back to \\(-\\infty\\) on the imaginary axis.\n\nFor strictly proper (order of denominator > order of the numerator) or proper systems (order of denominator = order of numerator), this entire segment (which is infinitely long) maps to a single point in the \\(\\Omega\\)-plane.\nAll physically realisable systems are of these two types…\n\nRemember that: - the phase in the \\(\\Omega\\)-plane is: \\[\n\\angle P_\\Omega = \\sum \\angle \\text{zeros} - \\sum \\angle poles\n\\]\n\nthe gain in the \\(\\Omega\\)-plane is: \\[\n| P_\\Omega | = \\frac{ K \\prod | \\text{zeros} |}{\\prod |\\text{poles}|}\n\\]\nCase 1 - strictly proper:\nFor a point at infinity the gain goes to zero (since we have more poles than zeros, we are dividing by infinity).\nSo any point in the contour maps to the origin in the \\(\\Omega\\)-plane.\nPhase is not very important, because the point is always the same (as the s-point moves the phase change but the point is still the same).\nCase 2 - proper:\nFor a point at infinity the gain is a finite real number (refer to the expressions above on phase and gain)\n\n\n\n\n\n\nThe Phase is very important to determine where on the line the point is:\n\nangle of a pole or of a zero when the point is at infinity is the same: the phase is 0 degree.\n\nThe entire segment maps to a non-zero positive real number.\n\n\nTo conclude\n\nSince the entire case 2) is mapped to a single point, that means that we can simply use the point at \\(+\\infty\\) on the positive \\(j\\omega\\) axis,\nDue to symmetry of poles and zeros about the real axis, we also do not need to worry of the negative \\(\\omega\\),\nWe need to only study and plot the positive \\(\\omega\\)\n\n\nSteps to plot a Nyquist plot\n\n\nSet \\(s=j\\omega\\) in the transfer function\n\n\nSweep \\(\\omega \\in [0, \\infty]\\) and plot the resulting complex numbers\n\n\nDraw the reflection about the real axis to account for negative \\(\\omega\\)\n\nNote that the Nyquist contour is traced in the clock-wise direction (by convention)\n\nKey is step #2:\n\nFor simple transfer functions there are only four points that we need to solve for\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=0\\) (start of the plot)\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=\\infty\\) (mid point of the plot)\n\n\nIntersections with the imaginary axis\n\n\nIntersections with the real axis\n\n\n\nHaving these points is typically enough if we are interested in the stability only. From these points we can deduce the rest of the diagram.\nFor more complex transfer functions, steps 3 and 4 might become complicated. In those cases, it is probably simpler to use the Python Control Library or MATLAB.\nExample:\n\\[\nG(s) = \\frac{1}{s^2+3s+2}\n\\]\n\n\n\\(\\frac{1}{(j\\omega)^2+3(j\\omega)+2} |_{\\omega=0}=\\frac{1}{2}\\), \\(\\angle =0\\)\n\n\n\\(\\frac{1}{(j\\omega)^2+3(j\\omega)+2} |_{\\omega=\\infty}=\\frac{1}{\\infty}=0\\) (strictly proper transfer function), \\(\\angle =-180\\)\n\n\nnote that for this transfer function, when we analyse the point \\(\\omega = \\infty\\), the phase is \\(e^{-j2\\theta}\\), \\(\\theta=\\pi/2\\). This is easy to see if we look at the original transfer function and substitute the generic point: \\(s=Re^{j\\theta}\\) and we note that \\(s^2\\) dominates the espression.\n\n\nTo do steps 3 and 4 and find the intersections, it is easier to convert our \\(G(s)|_{s=j\\omega}\\) to a \\(x+jy\\) vector to explicit the real and imaginary components:\n\\[\n\\frac{1}{(j\\omega)^2+3(j\\omega)+2} = \\frac{1}{(2-\\omega^2) + 3\\omega j} \\frac{(2-\\omega^2) - 3\\omega j}{(2-\\omega^2) - 3\\omega j} = \\frac{(2-\\omega^2) - 3\\omega j}{(2-\\omega^2)^2 + 9\\omega^2}\n\\]\n\n\nTo calculate the imaginary intercept: set the real part to 0 and find the \\(\\omega\\) to use in the imaginary part:\n\n\n\\[\n\\frac{(2-\\omega^2) }{(2-\\omega^2)^2 + 9\\omega^2} = 0 \\Rightarrow \\omega=\\sqrt{2} \\;\\;\\Rightarrow \\;\\;\\frac{-3\\sqrt{2}}{2\\cdot9} = -\\frac{\\sqrt{2}}{6} \\approx -0.236\n\\]\n\n\nTo calculate the real intercept: set the imaginary part to 0 and find the \\(\\omega\\) to use in the real part:\n\n\n\\[\n\\frac{- 3\\omega j}{(2-\\omega^2)^2 + 9\\omega^2} = 0 \\Rightarrow \\omega=0\\; \\text{,}\\; \\omega=\\infty\n\\]\nWe found points \\(\\omega=0\\; \\text{,}\\; \\omega=\\infty\\) already in points 1 and 2.\nIt means that the plot only crosses the real axis at the start and at the mid point of the plot.\n\n\n\n\n\n\nThe bottom part is the polar plot of the frequency response\n\nWe can plot it in Python and verify:\n\nsys = control.tf([1], [1, 3, 2])\n\nfig = plt.figure(figsize=(10, 5))\ncontrol.nyquist_plot(sys);\nplt.legend(['sys'], loc='upper right');\n\n\n\n\nQuestion - Is the system stable?\n\n# Uncomment the following line to see the solution\n!cat answers/solution_nyquist_3\n\n\n* Yes. The closed loop system is stable\n* No open loop unstable poles (open loop poles are s=-1, s=-2)\n* There are no encirclements of the point -1\n\n\nComments - This method does not tell us the exact plot - It is enough to verify the stability of the system\n\n\n\n\n\n\n\nPoles on the imaginary axis\nLet’s consider a pole at the origin:\n\\[\nG(s) = \\frac{1}{s}\n\\]\n\nAs we move closer to the pole:\n\nThe phase initially is the same (90 deg, see Figure below as the cyan marker goes to 0)\nThe magnitude gets smaller \\(\\Rightarrow\\) the magnitude in the \\(\\Omega-plane\\) tends to \\(\\infty\\)\nWhen we are right at the pole: the gain is 0, the phase is undefined\n\n\n\n\n\n\n\n\nWe cannot use the original Nyquist contour with poles on the imaginary axis.\nWe modify the contour a bit to avoid the poles on the imaginary axis\nWe need to move it as little as possible so that we only remove the imaginary poles but nothing else\nThis makes it possible to calculate the phase\n\n\n\n\n\n\nGoing back to our original transfer function \\(\\Large \\frac{1}{s}\\), when we modify the contour is clear that: - Gain is still infinity (we are still dividing by a very small number) - Phase clearly sweeps between \\([-90, 90]\\) - We can now plot the Nyquist plot\n\nWe can evaluate gain and phase at 3 key s-points:\n\nat \\(w=0\\): gain is \\(\\infty\\), phase is 0\nas we loop around the pole: gain is \\(\\infty\\), phase goes from \\(0\\) to \\(-90\\) deg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd then reflect the Nyquist plot around the real-axis\n\nLet’s see what the Python Control Library tells us:\n\n# We define our transfer function 1/s\nsys = control.tf([1], [1, 0])\n\n# Create the figure\nfig = plt.figure(figsize=(10, 5))\n\n# Call nyquist_plot\ncontrol.nyquist_plot(sys);\n\n# Add a legend\nplt.legend(['sys'], loc='upper right');\n\n\n\n\nFor this system, with can conclude: - No OL poles in the RHP and no encirclement of the point -1, so the closed loop system is stable. - Python control library does not really do a good job to represent it so we need to be careful when interpreting the plot.\n\nQuestion:\n\nwhat happens if instead we modified the contour in the other direction?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe system is stable:\n\nOne open loop pole inside the contour\nThe Nyquist plot encircles the -1 point once in the CCW direction\nHence no closed loop poles in the RHP\n\n\nOpen Loop zeros on the imaginary axis?\n\nThe gain is zero\nPhase not defined\nHowever since the gain is zero this is not a problem: the point is at the origin of the \\(\\Omega\\)-plane and phase does not change that.\nWe could modify the contour but we would get the same result just as if we did not modify the countor: we can simply proceed with the normal Nyquist contour.\n\n\n\nNyquist plot: final comments\n\nNyquist plot is really useful when the Open Loop plant has poles or zeros in the RHP\nSome systems have poles that are barely unstable\n\nBode plots are difficult to read in these cases\nNyquist instead provides a much clearer answer\n\nTry to compare Bode plots and Nyquist plots when one pole of your system is slightly unstable\n\nLet’s use, for example the following transfer function\n\nsys_u = control.tf([3.553e-15, -0.1642, -0.1243, -0.00161, 9.121e-17],\n                 [1, 1.825, 2.941, 0.03508, 0.01522, -1.245e-15])\n\nprint(sys_u)\n\n\n 3.553e-15 s^4 - 0.1642 s^3 - 0.1243 s^2 - 0.00161 s + 9.121e-17\n-----------------------------------------------------------------\ns^5 + 1.825 s^4 + 2.941 s^3 + 0.03508 s^2 + 0.01522 s - 1.245e-15\n\n\n\nWe would like to improve its performance using unitary feedback.\nWe can note that the open loop system is unstable, because there is a change in sign in the characteristic equation.\nAnd then we can also verify it with Python:\n\nnp.roots([1, 1.825, 2.941, 0.03508, 0.01522, -0.1245e-15])\n\narray([-9.08114038e-01+1.44747521j, -9.08114038e-01-1.44747521j,\n       -4.38596244e-03+0.0720649j , -4.38596244e-03-0.0720649j ,\n        8.18002628e-15+0.j        ])\n\n\nLet’s plot the Bode diagrams.\nThe Python Control Library does not handle well the very small numbers at the numerator of this example (Badly conditioned filter coefficients), while MATLAB does.\nThe figure below reports results obtained in MATLAB:\n\n\n\n\n\n\n\n\n\nAnd we can also show the Nyquist plot:\n\n\n\n\n\n\n\n\n\n\nThere are two CW encirclements of -1\nOne OL unstable pole\nA closed loop system with unitary feedback would have 3 unstable poles."
  },
  {
    "objectID": "lead_lag_compensators.html",
    "href": "lead_lag_compensators.html",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport control"
  },
  {
    "objectID": "lead_lag_compensators.html#standard-feedback-loop",
    "href": "lead_lag_compensators.html#standard-feedback-loop",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Standard Feedback loop",
    "text": "Standard Feedback loop\n\n\n\n\n\n\n\n\n\nController \\(R(s)\\): - Converts the error term into an actuator command - We are free to choose any control scheme we like. - As long as the closed loop performance of the system meets our requirements\nNote: controller = compensator\n\nLead and lag compensators are used quite extensively in control.\nA lead compensator can increase the stability or speed of reponse of a system;\nA lag compensator can reduce (but not eliminate) the steady-state error.\nDepending on the effect desired, one or more lead and lag compensators may be used in various combinations.\nLead, lag, and lead/lag compensators are usually designed for a system in transfer function form."
  },
  {
    "objectID": "lead_lag_compensators.html#preliminary-design-consideration",
    "href": "lead_lag_compensators.html#preliminary-design-consideration",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Preliminary Design Consideration",
    "text": "Preliminary Design Consideration\n\nThe root-locus plot of a system may indicate that the desired performance cannot be achieved just by the adjustment of gain (or some other adjustable parameter).\nFor example, the system may not be stable for all values of gain (or other adjustable parameter).\nIt is necessary to reshape the root loci to meet the performance specifications.\nThe design problems are of improving system performance by insertion of a compensator.\nWhich means designing a filter whose characteristics tend to compensate for the undesirable and unalterable characteristics of the plant."
  },
  {
    "objectID": "lead_lag_compensators.html#design-by-root-locus-method.",
    "href": "lead_lag_compensators.html#design-by-root-locus-method.",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Design by Root-Locus Method.",
    "text": "Design by Root-Locus Method.\n\nRe-shaping the root locus of the system by adding poles and zeros to the system’s open-loop transfer function and forcing the root loci to pass through desired closed-loop poles in the s plane\nThe characteristic of the root-locus design is its being based on the assumption that the closed-loop system has a pair of dominant closed-loop poles.\nEffects of zeros and additional poles do not affect the response characteristics very much.\n\nin the design by the root-locus method, the root loci of the system are reshaped through the use of a compensator so that a pair of dominant closed-loop poles can be placed at the desired location."
  },
  {
    "objectID": "lead_lag_compensators.html#series-compensation-and-parallel-or-feedback-compensation",
    "href": "lead_lag_compensators.html#series-compensation-and-parallel-or-feedback-compensation",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Series Compensation and Parallel (or Feedback) Compensation",
    "text": "Series Compensation and Parallel (or Feedback) Compensation\n\nSeries Compensation\n\n\n\n\n\n\n\n\n\n\nthe compensator \\(G_c(s)\\) is placed in series with the plant. This scheme is called series compensation.\n\n\n\nParallel or Feedback Compensation\n\n\n\n\n\n\n\n\n\n\nAn alternative to series compensation is to feed back the signal(s) from some ele- ment(s) and place a compensator in the resulting inner feedback path. Such compensation is called parallel compensation or feedback compensation.\n\n\n\nComments\n\nThe problem usually boils down to a suitable design of a series or parallel compensator.\nThe choice between series compensation and parallel compensation depends on the nature of the signals in the system, the power levels at various points, available components, the designer’s experience, economic considerations, and so on.\nIf a compensator is needed to meet the performance specifications, the designer must realize a physical device that has the prescribed transfer function of the compensator.\nPhase lead/lag compensators and networks\n\nIf a sinusoidal input is applied to the input of a network, and the steady-state output (which is also sinusoidal) has a phase lead, then the network is called a lead network. (The amount of phase lead angle is a function of the input frequency.)\nIf the steady-state output has a phase lag, then the network is called a lag network.\nIn a lag–lead network, both phase lag and phase lead occur in the output but in different frequency regions; phase lag occurs in the low-frequency region and phase lead occurs in the high-frequency region.\nA compensator having a characteristic of a lead network, lag network, or lag–lead network is called a lead compensator, lag compensator, or lag–lead compensator.\n\nPID controllers are also frequently used (see 16_PID_Control.ipynb)\nWe will use the root-locus or frequency-response methods to design the compensators\n\nThe root-locus approach to design is very powerful when the specifications are given in terms of time-domain quantities, such as the damping ratio and undamped natural frequency of the desired dominant closed-loop poles, maximum overshoot, rise time, and settling time.\n\nNote that the final result might not be unique! The best or optimal solution might not be precisely defined by the time-domain or frequency-domain specifications.\n\n\n\nEffects of the Addition of Poles\n\nAdding poles to the open-loop TF tend to pull the root locus to the right\nThis lowers the system’s relative stability and slows down the settling of the response\n\nRemember that the addition of integral control adds a pole at the origin, thus making the system less stable\n\n\n\n\n\n\n\n\n\n\n\nEffects of the Addition of Zeros\n\nAdding zeros to the open-loop transfer function has the effect of pulling the root locus to the left\nThis makes the system more stable and speeds up the settling of the response.\n\nthe addition of a zero in the feedforward transfer function means the addition of derivative control to the system. The effect of such control is to introduce a degree of anticipation into the system and speed up the transient response."
  },
  {
    "objectID": "lead_lag_compensators.html#phase-lead",
    "href": "lead_lag_compensators.html#phase-lead",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Phase Lead",
    "text": "Phase Lead\n\nWhat is phase lead\n\n\n\n\n\n\n\n\n\n\n\nt = np.arange(0, 10, 0.1)\nplt.plot(t, np.sin(t), color='r', label='sin');\nplt.plot(t, np.cos(t), color='b', label='cos');\nplt.legend();\nplt.grid()\n\n\n\n\n\nThe \\(cos\\) signal is ahead of the \\(sin\\) signal by \\(90\\) deg\nThe output leads the input by 90 deg\n\nWe can plot the Bode plots:\n\n\n\n\n\n\n\n\n\n\nDifferentiation gives positive phase\nIntegration gives negative phase (Mirrors the derivative plot)\nA zero in a transfer function adds phase\nA pole in a transfer function subtracts phase\nLead compensator: adds phase (at least in some frequency range of interest)\nLag compensator: subtracts phase (at least in some frequency range of interest)\n\n\nEquations\n\nLead compensator\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1} = \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\n\\]\n\none real pole and one real zero\n\\(w_z < w_p\\)\n\\(K=\\frac{w_p}{w_z}\\) (gain)\nWe can take care of the gain easily (e.g., using the root locus method)\n\n\n\nLag compensator\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1} = \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\n\\]\n\none real pole and one real zero\n\\(w_z > w_p\\)\n\\(K=\\frac{w_p}{w_z}\\) (gain)"
  },
  {
    "objectID": "lead_lag_compensators.html#bode-plot-lead-compensator",
    "href": "lead_lag_compensators.html#bode-plot-lead-compensator",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Bode plot: Lead Compensator",
    "text": "Bode plot: Lead Compensator\n\\[\nR(s) = \\frac{w_p}{s + w_p}\\frac{s + w_z}{w_z}\n\\]\nLet’s look at the zero-pole contribution separately:\n\n\n\n\n\n\n\n\n\n\nThe zero adds 90 deg and amplifies high frequencies\nThe pole subtracts 90 deg and attenuates high frequencies\nMultiplying the two T.F. together means adding everything together on the Bode plot\nLead compensator:\n\nBehaves like a real zero early on, at low frequency\nUntil the real pole pulls it back at high frequency\nSee blue line for its approximate representation\n\n\n\n\n\n\n\n\n\n\n\nNote: - A lead compensator increases the gain at high frequency (but less than a real zero would do) - This means that it is less noisy than a derivative controller on its own - A lead compensator adds phase between the two corner frequencies and no phase outside - Moving the two frequency means we can change where we add our phase\n\nWhat do we expect a Lag compensator to do?"
  },
  {
    "objectID": "lead_lag_compensators.html#example",
    "href": "lead_lag_compensators.html#example",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Example",
    "text": "Example\nLet’s see an example:\n\n# w_z < w_p (lead compensator)\nw_z = 1\nw_p = 10\n\n\n# define the usual s variable\ns = control.tf([1, 0], [1])\n\n\n# Compensator transfer function\nR_s = w_p/(s+w_p)*(s+w_z/w_z)\n\nWe can plot the zero (blue) and the pole (orange) parts together with the combined Bode plot (green):\n\nfig, axs = plt.subplots(1, figsize=(10,5))\n\n# zero (blue)\ncontrol.bode_plot((s+w_z)/w_z, dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n# pole (orange)\ncontrol.bode_plot(w_p/(s+w_p), dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n\n# compensator (green)\ncontrol.bode_plot(R_s, dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n\n# Note: If wrap_phase is True the phase will be restricted to the range [-180, 180) (or [-\\pi, \\pi) radians)\n\n\n\n\n\nWhat happens when we move the zero closer to the pole?\n\n\n# w_z < w_p (lead compensator)\nw_z = 5\nw_p = 10\n\n\nfig, axs = plt.subplots(1, figsize=(10,5))\n\n# zero (blue)\ncontrol.bode_plot((s+w_z)/w_z, dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n# pole (orange)\ncontrol.bode_plot(w_p/(s+w_p), dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n\n# compensator transfer function (green)\nR_s = w_p/(s+w_p)*(s+w_z)/w_z\ncontrol.bode_plot(R_s, dB=True, omega_limits = [0.1, 100], wrap_phase =True);\n\n\n\n\n\nStill a phase lead is present, but much smaller\nWhat happens if the zero is right on top of the pole?\n\n\n!cat answers/solution_lead_lag_compensator-1\n\nThey cancel each other out.\n\n\n\nif \\(w_p\\) < \\(w_z\\) we obtain a lag compensator\n\n\n# w_z > w_p (lag compensator)\nw_z = 50\nw_p = 10\n\n\nfig, axs = plt.subplots(1, figsize=(10,5))\n# zero (blue)\ncontrol.bode_plot((s+w_z)/w_z, dB=True, omega_limits = [1, 1000], wrap_phase =True);\n# pole (orange)\ncontrol.bode_plot(w_p/(s+w_p), dB=True, omega_limits = [1, 1000], wrap_phase =True);\n\n# compensator transfer function (green)\nR_s = w_p/(s+w_p)*(s+w_z)/w_z\ncontrol.bode_plot(R_s, dB=True, omega_limits = [1, 1000], wrap_phase =True);\n\n\n\n\n\nThe zero affects the system at higher frequency\nThe system behaves like a real pole at lower frequency\nUntil the zero comes into effect and “cancels” the pole at higher frequency\nThis add phase lag to the system\n\nNote: The same transfer function structure can produce phase lead or lag, adjusting the relative position of the pole and the zero"
  },
  {
    "objectID": "lead_lag_compensators.html#leadlag-compensators",
    "href": "lead_lag_compensators.html#leadlag-compensators",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Lead/Lag compensators",
    "text": "Lead/Lag compensators\n\nDesign a compensator that uses both a lead and a lag compensator:\n\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1}\n       \\frac{\\frac{s}{w_{z_1}}+1}{\\frac{s}{w_{p_1}}+1}\n= \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\\frac{w_{p_1}}{w_{z_1}}\\frac{s + w_{z_1}}{s + w_{p_1}}\n\\]\n\n# # Lead compensator\nw_z = .5\nw_p = 1\n\nR_Lead = w_p/(s+w_p)*(s+w_z)/w_z\n\n# Lag compensator\nw_z1 = 15\nw_p1 = 5\n\n\nR_Lag = w_p1/(s+w_p1)*(s+w_z1)/w_z1\n\nPlot the pole zero map for the Lead compensator:\n\ncontrol.pzmap(R_Lead);\n\n\n\n\nPlot the pole zero map for the Lag compensator:\n\ncontrol.pzmap(R_Lag);\n\n\n\n\nWe construct the Lead-Lag compensator:\n\n# Lead-Lag compensator transfer function\nR_LL = R_Lead*R_Lag\n\nAnd we can plot the Bode Plot to see what the phase does:\n\nfig, axs = plt.subplots(1, figsize=(10,5))\ncontrol.bode_plot(R_LL, dB=True, omega_limits = [.01, 1000], wrap_phase =True);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis compensator is leading at low frequency, and lagging at higher frequency"
  },
  {
    "objectID": "lead_lag_compensators.html#designing-a-lead-compensator-with-the-root-locus",
    "href": "lead_lag_compensators.html#designing-a-lead-compensator-with-the-root-locus",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Designing a Lead Compensator with the Root Locus",
    "text": "Designing a Lead Compensator with the Root Locus\n\nLet’s consider our control loop again:\n\n\n\n\n\n\n\n\n\n\n\nWe have a model of our plant \\(G(s)\\)\n\nwe are given a transfer function\nwe have identified the model\n\nDesign requirements - performance goal:\n\nStability\nRise time\nSettling time\nMax Overshoot\nDamping ratio\nGain/Phase margin\n\n\\(G(s)\\) alone does not meet our requirements\nWe need to design the controller \\(R(s)\\)\nHow do we choose \\(R(s)\\)?\n\n\nProcedures for designing a lead compensator with the Root Locus\n\nFrom the performance specifications, determine the desired location for the dominant closed-loop poles.\nDraw the Root Locus of the original system (uncompensated) and verify whether gain adjustments alone is enough to obtain the desired closed-loop poles.\nAssume the lead compensator to be:\n\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1} = \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\n\\]\nwhere \\(w_z < w_p\\) and can be determine from angle conditions and requirements of open-loop gain.\n\nDetermine the value of the gain from the magnitude condition\nCheck that the compensated system meets all performance specification. If not repeat changing the position of the pole and zero.\n\nLet’s consider:\n\\[\nG(s) = \\frac{1}{(s+2)(s+4)}\n\\]\n\nWhat is the Root Locus of \\(G(s)\\)?\n\n\n!cat answers/solution_lead_lag_compensator-2\n\nAngles of asymptotes:\n\nPhi_A = (2q+1)/(n-m) * 180 = 90, 270\n\nwhere q=0, 1, 2, ..., (n-m-1)\n\nAnd the centroid of the asymptotes is\n\nC = (sum(Finite poles)-sum(Finite zeros))/(n-m) = -3\n\n\n\ns = control.tf([1, 0],[1])\n\n\nG_s = 1/((s+2)*(s+4))\n\n\nfig, axs = plt.subplots(1, figsize=(10,5))\ncontrol.rlocus(G_s);\n\n\n\n\n\n\n\n\nWhat if we had a Lead compensator?\nHow does the root locus change?\n\nLet’s choose, arbitrarily, a Lead compensator (\\(w_z < w_p\\)):\n\\[\nR(s) = \\frac{(s+5)}{(s+6)}\n\\]\n\n!cat answers/solution_lead_lag_compensator-3\n\nPoles = -2, -4 (from the system), -6 (from the compensator)\nZeros = -5 (from the compensator)\n\nn - m = 3 - 1 = 2\nq = 0, 1\n\nPhi_A = (2q+1)/(n-m) * 180 = 90, 270\n\nC = (sum(Finite poles)-sum(Finite zeros))/(n-m) = ((-2-4-6) - (-5) )/(2) = (-12+5)/2 = -7/2 = -3.5\n\nNote: the centroid C is not the breakaway point from the real axis.\n\n\nLet’s see how it looks like with Python:\nWe define the controller:\n\nR_s = (s+5)/(s+6)\n\nPlot the Root Locus:\n\nfig, axs = plt.subplots(1, figsize=(10,5))\ncontrol.rlocus(G_s*R_s);\n\n\n\n\n\n\n\nWith a lead compensator: - We have moved the asynmptotes further into the left half plane - Increasing the gain \\(K\\) the close loop poles would be more to the left: we have added stability to the system\nHow does this help us? - When we use the root locus method we typically know where we would like our dominant closed loop poles to be so that we meet our requirements - With the root locus method, we first convert our requirements into pole locations\n\nWe need a lead compensator if we need to move our poles to the left of where our current (uncompensated) poles are\nWe need a lag compensator if we need to move our poles to the right of where our current (uncompensated) poles are\nIf the root locus already goes through the desired locations, we only need to choose the correct gain\n\nNote: we could still have a steady state error problem, but we know how to fix this already\n\nGiven desired poles, solving for the compensator becomes a trigonometry problem:\nTo be part of the Root Locus: \\(\\sum{\\angle Poles} - \\sum{\\angle Zeros} = 180^o\\)\n\nFor example, for our system we saw that the root locus is:\n\n\n\n\n\n\n\n\n\n\nThere are no zeros (so we do not need to subtract)\nThe sum of \\(\\theta_1+\\theta_2\\)=180 because that point is part of the root locus\n\nGiven our system: \\[\nG(s) = \\frac{1}{(s+2)(s+4)}\n\\]\nIf we want poles:\n\\[s_d = -4 \\pm 2j\\]\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we sum up all the angles: \\(90+(90+\\theta)\\) = \\(90+(90+45)\\) = 225\n\nsince \\(\\theta = \\tan^{-1}(2/2) = \\tan^{-1}(1) = 45\\)\n\nAnd of course, this is not on the Root Locus.\n\nTo have it on the Root Locus, we need to remove 45 deg of phase\nUsing a phase lead compensator we can do it adding a single pole and a single zero:\n\n\\[\n225 + (\\theta_p - \\theta_z) = 180\n\\]\n\n\\((\\theta_p - \\theta_z)\\) is our lead compensator\nIf we pick a zeros at -5\n\n\\(\\theta_z= 63.43 ^o\\)\n\nThe pole must go into one specific location:\n\n\\(\\theta_p=180-225+63.43=18.43\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!cat answers/solution_lead_lag_compensator-4\n\nIf beta is the angle of the pole of the compensator:\n\ntan(beta) = 2/p\n\np = 2/tan(beta)\n\nbeta = 18.43\nbeta_rad = 18.43*np.pi/180 = .321\n\np = 2/tan(beta) = 2/tan(.321) = -6.015\n\nthe pole location is:\n\ns = p+4 = -10.015\n\n\nThe compensator for this particular problem is:\n\\[\nR(s) = \\frac{s+5}{s+10.015}\n\\]\n\nR_s = (s+5)/(s+10.015)\nfig, axs = plt.subplots(1, figsize=(7,7))\ncontrol.rlocus(G_s*R_s);\n\n\n\n\n\n\n\nOne more step: - We need to calculate the gain that moves the poles in closed loop where we want them\n\\[\n\\frac{KG(s)R(s)}{1+KG(s)R(s)}\n\\]\nWe can then find:\n\\[\nK = \\frac{-1}{1+G(s)R(s)}\\Big|_{s=-4 + 2 j} \\approx 16\n\\]\nNote: Why is K chosen in this way?\n\n\nChoosing the first zero\n\nNo hard and fast rule\nWe need to have enough negative angle to bring the sum to 180 deg\nPay attention not to interfere with your dominat poles\n\nFor example, if you had:\n\n\n\n\n\n\n\n\n\n\n\nWe can then sketch bounds:\n\n\n\n\n\n\n\n\n\nRule fo thumb: place your zero at or closely to the left of the second real axis open loop pole\n\nDoes not guarantee that overshoot requirements will be met\nTrial and error might be needed\nWith higher order systems, might be difficult to predict where other non-dominant poles go: we must avoid making them unstable\nFaster responding system means responding to noise as well.\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\\[\nG(s) = \\frac{10}{s(s+1)}\n\\]\n\ns = control.tf([1,0],[1])\n\n\nG_s = 10/(s*(s+1))\nprint(G_s)\n\n\n  10\n-------\ns^2 + s\n\n\n\n\ncontrol.rlocus(G_s);\n\n\n\n\nThe closed-loop transfer function is:\n\nG_cc = control.feedback(G_s, 1, -1)\nprint(G_cc)\n\n\n     10\n------------\ns^2 + s + 10\n\n\n\nwith closed loop poles:\n\nfig = plt.figure()\nclosed_loop_poles = np.roots([1, 1, 10])\nprint(closed_loop_poles)\n\ncontrol.rlocus(G_s);\nplt.plot(np.real(closed_loop_poles[0]), np.imag(closed_loop_poles[0]),\n         np.real(closed_loop_poles[1]), np.imag(closed_loop_poles[1]), marker='o',  markersize=12)\nplt.ylim((-4, 4))\n\n[-0.5+3.122499j -0.5-3.122499j]\n\n\n(-4.0, 4.0)\n\n\n\n\n\n\nNatual frequency is \\(\\omega_n=\\sqrt{10}=3.16 rad/s\\)\nDamping ratio is \\(\\xi=0.5/\\sqrt{10}=0.1581\\)\n\nBecause the damping ratio is small, this system will have a large overshoot in the step response and is not desirable.\n\nfig = plt.figure()\n[tout, yout] = control.step_response(G_cc)\n\nplt.plot(tout, yout)\nplt.xlabel('time (sec)')\nplt.ylabel('outputs')\n\nText(0, 0.5, 'outputs')\n\n\n\n\n\n\nWe add a Lead compansator to improve its performance\n\nDamping ratio \\(\\xi=0.5\\)\nNatural frequency \\(\\omega_n=3\\) rad/s\n\n\nThis means having closed loop poles:\n\\[\nS^2 + 2\\xi + \\omega_n^2 = s^2 + 3s + 9\n\\]\nor \\(s=-1.5\\pm 2.5981j\\)\n\nnp.arccos(.5)*180/3.14\n\n60.03043287114254\n\n\n\nWe cannot move the closed-loop poles to the new desired location simply changing the gain\n\n\n\nProcedure\n\nFind the sum of the angles at the desired location of one of the dominant closed-loop poles with the open-loop poles and zeros of the original system\nDetermine the necessary angle \\(\\phi\\) to be added so that the total sum of the angles is equal to \\(180°(2k + 1)\\) (phase condition).\nThe lead compensator must contribute this angle \\(\\phi\\) (more than one network might be needed if \\(\\phi\\) is quite large).\n\nThe angle from the pole at the origin to the desired dominant closed-loop pole:\n\nnp.rad2deg(np.arctan2(2.5981, -1.5))\n\n119.99977283671743\n\n\nThe angle from the pole at \\(s=–1\\) to the desired closed-loop pole is:\n\nnp.rad2deg(np.arctan2(2.5981, -1.5+1))\n\n100.89329729362939\n\n\nAngle deficiency to satisfy the phase condition of the root locus:\n\\[\nAngle\\; Deficiency=180-120-100.894=-40.894\n\\]\nThis must be contributed by a lead compensator.\nNote that the solution to such a problem is not unique. There are infinitely many solutions.\n\n\nMethod\n\nDraw a horizontal line passing through the desired location for one of the dominant closed-loop poles (we call this P)\nDraw a line connecting point P and the origin.\nBisect the resuting PAO angle\nDraw two lines \\(PC\\) and \\(PD\\) that make angles \\(\\pm \\phi/2\\) with the bisector PB.\nThe intersections of \\(PC\\) and \\(PD\\) with the negative real axis give the necessary locations for the pole and zero of the lead network\nThe open-loop gain is determined by use of the magnitude condition\nThis procedure will push the pole to the left as mush as possible, which can be beneficial\n\n\n\n\n\n\n\n\n\n\nIn our case:\n\nzero at \\(s=–1.9432\\)\npole at \\(s=–4.6458\\)\n\nand the lead compensator is:\n\\[\nR(s) = K\\frac{s+1.9432}{s+4.6458}\n\\]\nWe can then use the magnitude condition to obtain the value of \\(K\\):\n\\[\n\\Big |K\\frac{s+1.9432}{s+4.6458}\\frac{10}{s(s+1)} \\Big|_{s=-1.5 + 2.5981j} = 1\n\\]\n\\[\nK = 1.2287\n\\]\n\nG_s = 1.2287*(s+1.9432)/(s+4.6458)*10/(s*(s+1))\nprint(G_s)\n\n\n     12.29 s + 23.88\n-------------------------\ns^3 + 5.646 s^2 + 4.646 s\n\n\n\n\nG_cc_compensated = control.feedback(G_s, 1, -1)\nprint(G_cc_compensated)\n\n\n         12.29 s + 23.88\n---------------------------------\ns^3 + 5.646 s^2 + 16.93 s + 23.88\n\n\n\n\nfig = plt.figure()\ndesired_closed_loop_poles = [-1.5+2.5881j, -1.5-2.5881j]\nprint(closed_loop_poles)\n\ncontrol.rlocus(G_s);\nplt.plot(np.real(desired_closed_loop_poles[0]), np.imag(desired_closed_loop_poles[0]),\n         np.real(desired_closed_loop_poles[1]), np.imag(desired_closed_loop_poles[1]), marker='o',  markersize=12)\nplt.ylim((-4, 4))\n\n[-0.5+3.122499j -0.5-3.122499j]\n\n\n(-4.0, 4.0)\n\n\n\n\n\n\n\nWait we have three closed-loop poles\nThird closed-loop pole of the designed system is found by dividing the characteristic equation by the known factors:\n\\[\ns^3 + 5.646s^2 + 16.933s + 23.875 = (s + 1.5 + j2.5981)(s + 1.5 - j2.5981)(s + 2.65)\n\\]\n\nfig = plt.figure()\n\n[tout, yout] = control.step_response(G_cc)\n[tout_c, yout_c] = control.step_response(G_cc_compensated)\n\nplt.plot(tout, yout, label='original')\nplt.plot(tout_c, yout_c, label='compensated')\nplt.legend()\nplt.xlabel('time (sec)')\nplt.ylabel('outputs')\n\nText(0, 0.5, 'outputs')"
  },
  {
    "objectID": "lead_lag_compensators.html#designing-a-lag-compensator-with-the-root-locus",
    "href": "lead_lag_compensators.html#designing-a-lag-compensator-with-the-root-locus",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Designing a Lag Compensator with the Root Locus",
    "text": "Designing a Lag Compensator with the Root Locus\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1} = \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\n\\]\n\none real pole and one real zero\n\\(w_z > w_p\\)\n\\(K=\\frac{w_p}{w_z}\\) (gain)\n\nWe can re-write the above equation as:\n\\[\nR(s) = \\frac{\\tau_z s + 1}{\\tau_p s + 1}\n\\]\n\nThe transfer function has the same structure\nWe can use the same design technique that we saw for the Lead Compensators\nWith a Phase Lag compensator we can move our dominant poles closer to the imaginary axis\nThis is however not the main reason Lag Compensators\nLag Compensators are useful to address steady state errors\n\ne.g., error to the step input\nimprove steady state errors, without changing the position of the dominant poles (they are already where we need them)\nthis means we do not want shape the root locus very much\n\n\nThe system exhibits satisfactory transient-response characteristics but unsatisfactory steady-state characteristics. This means that the root locus in the neighborhood of the dominant closed-loop poles should not be changed appreciably but we want to increase the open-loop gain to achieve desired state-state performance.\n\nTo avoid an appreciable change in the root loci, the angle contribution of the lag network should be limited to a small amount, say less than 5°.\nTo assure this, we place the pole and zero of the lag network relatively close together and near the origin of the s plane.\n\n\nHow does a lag compensator reduce \\(E_{ss}\\)\n\n\n\n\n\n\n\n\n\nWhere: \\[\nG(S) = \\frac{N(s)}{D(s)}\n\\]\nand our Lag compensator is:\n\\[\nR(s) = \\frac{s-z}{s-p}\n\\]\nand the input is \\(U(s)\\).\nThe steady state error for the uncompensated system is:\n\\[\nE_{ss}  = \\lim_{s\\rightarrow0} s \\cdot \\frac{U(s)}{1+\\frac{N(s)}{D(s)}}\n\\]\nFor a step input: \\(U(s)=\\frac{1}{s}\\)\n\\[\n\\Rightarrow E_{ss}  = \\lim_{s\\rightarrow0} s \\cdot \\frac{U(s)}{1+\\frac{N(s)}{D(s)}} = \\frac{D(0)}{D(0)+N(0)}\n\\]\nThe steady state error for the compensated system is:\n\\[\nE_{ss} = \\lim_{s\\rightarrow0} s \\cdot \\frac{U(s)}{1+\\frac{N(s)(s-z)}{D(s)(s-p)}}\n\\]\nFor a step input: \\(U(s)=\\frac{1}{s}\\)\n\\[\n\\Rightarrow E_{ss,c} = \\lim_{s\\rightarrow0} s \\cdot \\frac{U(s)}{1+\\frac{N(s)(s-z)}{D(s)(s-p)}} = \\frac{D(s)(s-p)}{D(s)(s-p)+N(s)(s-z)}\\Big|_{s\\rightarrow0} = \\frac{D(0)p}{D(0)p+N(0)z}\n\\]\n\\[\n\\frac{z}{p} = \\frac{D(0)-E_{ss,c}D(0)}{E_{ss,c}N(0)}\n\\]\n\nWe can choose \\(E_{ss,c}\\) and have the corresponding \\(\\frac{z}{p}\\)\nTo have \\(E_{ss,c}=0\\), \\(\\frac{z}{p} \\rightarrow \\infty\\)\nWe can only reduce the steady state error and not eliminate it. We need to change the system type to eliminate it\n\n\n\nExample\n\\[\nG(s) = \\frac{5s^2+6s+2}{4s^2+s+3}\n\\]\n\nGoal: reduce steady state error \\(E_{ss, c}=0.1\\)\n\n\\[\n\\frac{z}{p} = \\frac{D(0)-E_{ss,c}D(0)}{E_{ss,c}N(s)} = \\frac{3-E_{ss, c}\\cdot 3}{E_{ss, c}\\cdot2} = \\frac{3-0.1\\cdot3}{0.1\\cdot2} = 13.5\n\\]\n\nWe know the zero/pole ratio\nWhere do we place them?\nE.g., \\(z=-0.1\\), \\(p=-1.35\\) or \\(z=1\\), \\(p=13.5\\) would work\n\nRemember that the Root Locus is where the angles of poles (+) and zeros (-) add to 180 degrees\n\n\n\n\n\n\n\n\n\n\nWe do not want to move the location of our roots too much when using a Lag compensator\nThe the angle of the poles and zeros of the compensator should be very small (no Root Locus shaping):\n\n\\[\n\\theta_p - \\theta_z \\approx 0\n\\]\n\nMeeting the condition above while keeping the desired \\(\\frac{z}{p}\\) is much easier if we place the zero and pole very close to the imaginary axis\nPractical constraints means you cannot move them too close (resistors and capacitors limits)\n\nRule of thumb: the location of the zero is approximately 50 times closer to the imaginary axis as the closer dominant pole\nE.g., - dominant poles at \\(-1\\pm \\text{Imag} j\\) - zero at \\(-1/50=0.02\\) - pole at \\(\\Big(\\frac{z}{p}\\Big)_{des} = \\frac{0.02}{p}\\)\n\n\nExample\n\n\n\n\n\n\n\n\n\n\\[\nG(s) = \\frac{1}{(s+1)(s+3)}\n\\]\nWith a Lead compensator to meet stability and rise time requirements \\[\nR(s) = \\frac{16(s+4)}{(s+9)}\n\\]\nIn this case: \\[\nE_{ss} = \\frac{D(0)}{D(0)+N(0)} = \\frac{9\\cdot3\\cdot1}{9\\cdot3\\cdot1+16\\cdot4}\\approx 0.3\n\\]\nRequirement: \\(E_{ss} =0.1\\)\n\\[\n\\frac{z}{p} = \\frac{D(0) -E_{ss} D(0)}{E_{ss} N(0)} = \\frac{9\\cdot3\\cdot1 - 0.1\\cdot9\\cdot3\\cdot1}{0.1\\cdot16\\cdot4} \\approx 3.8\n\\]\nDominant poles: \\(s_{1,2} = -3\\pm-2j\\)\n\nThe zero goes to \\(\\frac{-3}{50} = -0.06\\)\nThe Pole goes to \\(3.8 = \\frac{-0.06}{p} \\Rightarrow p = -0.6/3.8 = -0.016\\)\n\nLag compensator: \\[\nR(s) = \\frac{s+0.06}{s+0.016}\n\\]\n\nWe can have the Lag compensator in series with a Lead compensator\n\nExercise: - Plot the step response and the impulse respose of the uncompensated and compensated system\n\ns = control.tf([1, 0],[1])\n\nsys_u = 1/((s+1)*(s+3))*(16*(s+4)/(s+9))\nprint('sys_u', sys_u)\n\nsys_c = 1/((s+1)*(s+3))*(16*(s+4)/(s+9))*(s+0.06)/(s+0.016)\nprint('sys_c', sys_c)\n\nsys_u \n       16 s + 64\n------------------------\ns^3 + 13 s^2 + 39 s + 27\n\nsys_c \n           16 s^2 + 64.96 s + 3.84\n---------------------------------------------\ns^4 + 13.02 s^3 + 39.21 s^2 + 27.62 s + 0.432\n\n\n\n\nt_u, yout_u = control.step_response(control.feedback(sys_u, 1), T=100)\nt_c, yout_c = control.step_response(control.feedback(sys_c, 1), T=100)\n\n\nfig, ax = plt.subplots(1, figsize=(10,7))\n\nplt.plot(t_u, yout_u, color='b', label='uncompensated');\nplt.plot(t_c, yout_c, color='r', label='compensated');\nplt.grid();\nplt.legend();\nplt.yticks(np.arange(0, 1, 0.1));\n\n\n\n\n\nAs expected: the uncompensated system has \\(E_{ss} = 0.3\\)\nThe compensated system has \\(E_{ss} = 0.1\\)\nSettling time does not change very much, as desired\n\nWe can see this better looking at the impulse response\n\nt_u, yout_u = control.impulse_response(control.feedback(sys_u, 1), T=3)\nt_c, yout_c = control.impulse_response(control.feedback(sys_c, 1), T=3)\n\n\nfig, ax = plt.subplots(1, figsize=(10,7))\n\nplt.plot(t_u, yout_u, color='b', label='uncompensated');\nplt.plot(t_c, yout_c, color='r', label='compensated');\nplt.grid();\nplt.legend();\nplt.yticks(np.arange(0, 1.5, 0.1));\n\n\n\n\n\nThis confirmes that we did not change the position of the dominant poles very much"
  },
  {
    "objectID": "lead_lag_compensators.html#designing-a-lead-compensator-with-the-bode-plot",
    "href": "lead_lag_compensators.html#designing-a-lead-compensator-with-the-bode-plot",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Designing a Lead Compensator with the Bode Plot",
    "text": "Designing a Lead Compensator with the Bode Plot\n\nLet’s consider our control loop again:\n\n\n\n\n\n\n\n\n\n\n\nCompensator: Lead compensator, so we need to select one pole and one zero\nFirst: Convert requirements to frequency domain requirements if needed\n\nGain/Phase margin\nBandwidth\nGain crossover frequency\nZero-frequency magnitude or DC Gain\nSteady-state error\n\n\n\n\n\n\n\n\n\n\n\nA phase-lead compensator can also be designed using a frequency response approach. A lead compensator in frequency response form is given by the following transfer function:\n\\[\nLead(s) = \\frac{a\\tau s + 1}{\\tau s + 1}\n\\]\nwith \\(a>1\\) (when \\(a<1\\) we would have a lag compensator).\nNote that the previous expression is equivalent to the form (which we used for the Root Locus):\n\\[\nLead(s) = K\\frac{s + w_z}{s + w_p}\n\\]\nwhen \\(w_p =\\frac{1}{\\tau}\\), \\(w_z = \\frac{1}{a\\tau}\\), and \\(K = a\\).\nIn frequency response design, the phase-lead compensator adds positive phase to the system over the frequency range \\(\\frac{1}{a\\tau}\\) to \\(\\frac{1}{\\tau}\\).\nAnd a Bode plot of a phase-lead compensator \\(Lead(s)\\) has the following form:\n\n\n\n\n\n\n\n\n\n\nThe two corner frequencies are at \\(\\frac{1}{a\\tau}\\) and \\(\\frac{1}{\\tau}\\)\nNote the positive phase that is added to the system between these two frequencies.\nDepending on the value of \\(a\\), the maximum added phase can be up to 90 degrees\nIf you need more than 90 degrees of phase, two lead compensators in series can be employed.\n\n\nExample\n\\[\nG(s) = \\frac{1}{0.2s+1}\n\\]\n\nSystem requirements\n\nSteady state error < 0.02 to a unit ramp input\nPhase margin \\(> 48\\) deg\n\n\nFor a ramp unit (assuming a stable system):\n\n\nwe need at least a type 1 system to have a finite error (single pole at the origin)\nwe need at least a type 2 system to have a zero error\n\nNote we cannot meet our requirements with a lead compensator alone. It does not have a pole at the origin\nOur controller needs to have this structure to start from \\[\nR(s) = K\\frac{1}{s}\n\\]\nand now we can add the lead compensator to deal with the phase margin.\n\nIt is always better to start from the type of the system: adding a pole at the origin will affect your phase.\nGiven that a single pole at the origin is enough, we will not add a second one\n\nAdds complexity\nMight reduce stability\nDo not overdesign!\n\n\n\nChoose the gain to meet \\(e_{ss} < 0.02\\)\n\nFor an input \\(U(s)\\):\n\\[\ne_{ss} = \\lim_{s\\rightarrow0}sE(s) = s \\frac{1}{1+G_F(s)}U(s)\n\\]\nwhere, in this case:\n\\[\nG_F(s) = G(s)K\\frac{1}{s}\n\\]\n\\[\nRamp = \\frac{1}{s^2}\n\\]\nIf we plug in the numbers:\n\\[\ne_{ss} = \\lim_{s\\rightarrow0}sE(s) = s \\frac{1}{s^2}\\frac{0.2s^2+s}{0.2s^2+s+K+1} < 0.02 \\Rightarrow K > 49\n\\]\nWe can then choose:\n\\[\nR(s) = 50\\frac{1}{s}\n\\]\nLet’s now draw the Bode plot:\n\nBring the system in the Bode form\nSketch the Bode Diagram for each individual part\nAdd them all up\n\n\n\n\n\n\n\n\n\n\nLet’s check it with Python\n\ns = control.tf([1, 0], [1])\nG_s = 1/(0.2*s+1)\nR_s = 50/s\n\n\nfig, ax = plt.subplots(1, figsize=(10,7))\ncontrol.bode_plot(G_s*R_s, dB=True);\n\n\n\n\n\n[gm, pm, _, pm_f] = control.margin(G_s*R_s)\n\nprint('Phase margin: deg', pm, 'at rad/s', pm_f)\n\nPhase margin: deg 17.964235916371393 at rad/s 15.421164188583809\n\n\nWe do not respect our phase margin requirement \\(18<48\\)\n\nA lead compensator adds phase for a specific frequency range\nWe also add gain, which means we move the crossover frequency to higher frequency\n\n\n\n\n\n\n\n\n\n\n\nTo determin how much phase let’s look at the lead compensator equations, which we can re-write in this form to highlight the relative relationship between the pole and the zero:\n\n\\[\nLead(s) = \\frac{a\\tau s + 1}{\\tau s + 1}\n\\]\n\nIn this form, the steady state gain is 1 (0 dB). Does not affect the steady state we already determined.\n\\(a > 1\\) to be a lead compensator\n\\(a < 1\\) is a lag compensator\n\nFrom the equation above it is easy to verfy that the following equations can be used to determine significant points of the response:\nUpper cutoff frequency \\[\nw_u = \\frac{1}{a\\tau}\n\\]\n\ngain starts to increase\n\nLower cutoff frequency\n\\[\nw_l = \\frac{1}{\\tau}\n\\]\n\ngain starts to flatten out\n\nMax phase (obtained at the center frequency \\(w_m\\))\n\\[\n\\Phi_{max} = \\sin^{-1} \\Big( \\frac{a-1}{a+1} \\Big)\n\\]\nFreq at Max phase \\[\nw_{m} = \\frac{1}{\\tau\\sqrt{a}}\n\\]\nGain at Max phase \\[\nGain_{m} = \\sqrt{a}\n\\]\n\n\n\n\n\n\n\n\n\nChoosing \\(a\\) and \\(\\tau\\):\n\nChoose the maximum phase you would like to add \\(\\Phi_{max}\\) and solve for \\(a\\)\nChoose the frequency \\(w_{m}\\) where you would like to add \\(\\Phi_{max}\\) and solve for \\(\\tau\\)\n\n\nWe would get a lower phase increase\nTrial and error is an option\n\nAdd a safety factor (e.g., 15 deg)\n\n\nLet’s go back to our design - We need 30 deg more at 15 rad/s (from 18 to 38) - We add some safety factor:\n\\[\n\\Phi_{max} = \\sin^{-1} \\Big( \\frac{a-1}{a+1} \\Big) = 37 \\Rightarrow a=4\n\\]\nFreq at Max phase \\[\nw_{m} = \\frac{1}{\\tau\\sqrt{a}} = 22.2 rad/s \\Rightarrow \\tau=0.022\n\\]\n\n# to find a:\nimport numpy as np\nprint(np.sin(37*3.14/180))\n1.6/0.4 # a\n\n0.6015535345767008\n\n\n4.0\n\n\nFinal controller:\n\\[\nR(s) = 50\\frac{1}{s}\\frac{0.088s+1}{0.022s+1}\n\\]\nAnd we should verify that we obtain the desired phase margin plotting the Bode plot.\nLet’s plot the Bode plots for - the initial controller (in blue) \\[\nR(s) = G(s)\\frac{50}{s}\n\\]\n\nthe final lead compensator (in orange) \\[\nR(s) = 50\\frac{1}{s}\\frac{0.088s+1}{0.022s+1}\n\\]\n\n\ns = control.tf([1, 0], [1])\nG_s = 1/(0.2*s+1)\nR_s = 50/s*(0.088*s+1)/(0.022*s+1)\n\nfig, ax = plt.subplots(1, figsize=(10,7))\ncontrol.bode_plot(G_s*50/s, dB=True, wrap_phase=True, omega_limits=[0.1, 1000]);\n\ncontrol.bode_plot(G_s*R_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.1, 1000]);\n\n\n\n\n\nThe gain plot did not change too much\nWe can see the lead compensator as a delta that we sum to the uncompensated Bode plot\nWe can only add up to \\(\\Phi_{max}= 90\\) deg (there is only one zero)\nIn practice: \\(\\Phi_{max} < 55\\) deg\nMore phase lead needed means two lead compensators in series"
  },
  {
    "objectID": "lead_lag_compensators.html#designing-a-lag-compensator-with-the-bode-plot",
    "href": "lead_lag_compensators.html#designing-a-lag-compensator-with-the-bode-plot",
    "title": "Phase Lead/Phase Lag Compensators",
    "section": "Designing a Lag Compensator with the Bode Plot",
    "text": "Designing a Lag Compensator with the Bode Plot\n\nLet’s consider our control loop again:\n\n\n\n\n\n\n\n\n\n\n\\[\nG(s) = \\frac{1}{0.2s+1}\n\\]\n\nSystem requirements\n\nSteady state error < 0.02 to a unit ramp input\nPhase margin \\(> 48\\) deg\n\nWhen we designed the Lead compensator using the Bode plots:\n\nWe saw we need a type 1 system at least, and then we chose the gain \\(K\\) to meet our steady state error requirement\n\nWe designed an initial controller:\n\\[\nR(s) = \\frac{50}{s}\n\\]\n\nAchieves requirement 1, but not 2\n\n\ns = control.tf([1, 0], [1])\nG_s = 1/(0.2*s+1)\nR_s = 50/s\n\nfig, ax = plt.subplots(1, figsize=(10,7))\n\ncontrol.bode_plot(G_s*R_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.1, 1000]);\n\n\n\n\n\nBefore, we used a Lead compensator to meet our phase requirement, with the final controller that was:\n\n\\[\nR_s = \\frac{50}{s}\\frac{(0.088s+1)}{(0.022*s+1)}\n\\]\n\nWe could use a Lag compensator to meet our phase margin requirement\nBut with the Lag compensator we meet the requirement changing the gain crossover frequency\nNote that we want to still retain the performance we obtained before with the partially compensated system (i.e., when we were using the controller \\(\\frac{50}{s}\\). This means that we do not want to change the DC gain because that has been set to achieve the steady state requirements\n\nLet’s consider a typical Lag Compensator Bode Plot.\nAnd to do so, we consider:\n\\[\nR_{lag}(s) = \\frac{2s+1}{4s+1}\n\\]\n\nfig, ax = plt.subplots(1, figsize=(10,7))\ncontrol.bode_plot((2*s+1)/(4*s+1), dB=True, wrap_phase=True);\n\n\n\n\n\nA low frequency, the gain is 1 (0 dB)\nUseful because we do not want to change our DC gain\nAt high frequency, the magnitude is 1/2 or -6dB (for this specific choice of zero/pole)\nWe want to leverage the high frequency attentuation with a relative flat frequency shift to move the crossover frequency and the 0dB DC gain at low frequency not to impact the steady state regime\nThis means:\n\nWe need to have the high frequency attenuation in the frequency range of the Bode plot that we want to shape\nWe need to push the phase lag to lower frequencies as much as possible\n\n\nLet’s go back to our Bode Plot\n\n# Uncomment the following lines if you want to plot the Bode Plot below\n# s = control.tf([1, 0], [1])\n# G_s = 1/(0.2*s+1)\n# R_s = 50/s\n\n# fig, ax = plt.subplots(1, figsize=(10,7))\n\n# control.bode_plot(G_s*R_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.1, 1000]);\n\n\n\n\n\n\n\n\n\n\n\nWe would need to decrease the gain by about 18 dB to have a crossover frequency with a 48 degree phase margin\nAdd a safety margin (e.g., drop the gain by 20 dB to add some safety)\nLet’s calculate how to have a drop in gain of 20dB or 10 at high frequency:\n\n\\[\nR(s) = \\frac{\\tau_z\\cdot s +1}{\\tau_p\\cdot s +1} \\Rightarrow \\frac{\\tau_z}{\\tau_p} = 10\n\\]\nor, the relative ratio between the zero and the pole is 10:\n\\[\n\\frac{\\tau_z s+1}{10\\tau_p s+1}\n\\]\n\nWe want the phase lag as low frequency as possible.\nWe need the zero and poles as close to the imaginary axis as possible (or \\(\\tau\\) as large as possible)\nThe larger \\(\\tau\\), the closer to the imaginary axis is the pole-zero pair, the less the lag compensator interfere with the original system, while still acting to improve our phase margin\nThis is the same approach that we used for the Root Locus to design the Lag compensator\n\nRule of thumb: place the zero 50 times closer to the origin as the dominant poles\nFor our case, we have a pole at \\(s=5\\), which means we would like a zero at \\(s=(1/50)*5\\), or if we use the time-constant representation \\(0.2*50\\):\n\\[\nLag(s) = \\frac{10s+1}{100s+1}\n\\]\nAnd here is the final controller:\n\\[\nR(s) = \\frac{50}{s}\\frac{10s+1}{100s+1}\n\\]\n\ns = control.tf([1, 0], [1])\n\n# System G(s)\nG_s = 1/(0.2*s+1)\n\n# Partial Compensator P(s)\nP_s = 50/s\n\n# Lead Compensator Lead(s)\nLead_s = (0.088*s+1)/(0.022*s+1)\n\n# Lag Compensator Lag(s)\nLag_s = (10*s+1)/(100*s+1)\n\n# Lead Lag compensator\nR_s = 50/s*Lead_s*Lag_s\n\n# create the axis and figure\nfig, ax = plt.subplots(1, figsize=(10,7))\n\n# Blue, uncompensated\ncontrol.bode_plot(P_s*G_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.0001, 1000], color='b');\n\n# Green, Lead compensated\ncontrol.bode_plot(P_s*G_s*Lead_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.0001, 1000], color='g');\n\n# Red, Lag compensated\ncontrol.bode_plot(P_s*G_s*Lag_s, dB=True, margins=True, wrap_phase=True, omega_limits=[0.0001, 1000], color='r');\n\n\n\n\n\nCompare the phase maring of both the Lead Compensated, and the Lag Compensated\nAt low frequency the DC gain has not changed for any system\nWith the Lag compensator we moved the cross-over frequency to lower frequency: we slowed down the system\n\nWe can see this plotting the step response\n\nt_o, yout_o = control.step_response(control.feedback(P_s*G_s, 1), T=5)\nt_lead, yout_lead = control.step_response(control.feedback(P_s*G_s*Lead_s, 1), T=5)\nt_lag, yout_lag = control.step_response(control.feedback(P_s*G_s*Lag_s, 1), T=5)\n\n\nfig, ax = plt.subplots(1, figsize=(10,7))\n\nplt.plot(t_o, yout_o, color='b', label='Uncompensated');\nplt.plot(t_lead, yout_lead, color='g', label='Lead compensated');\nplt.plot(t_lag, yout_lag, color='r', label='Lag compensated');\n\nplt.grid();\nplt.legend();\nplt.yticks(np.arange(0, 2, 0.1));\n\n\n\n\n\nThe original system is less stable\nThe Lead compensated is very fast\nThe Lag compensated is slower\n\nNote: - A slower system does not react to high frequency noise as much, and this tends to be better - If we do not need to track fast signals, a slower system can be better\n\n\nFinal comments\n\nLead compensation basically speeds up the response and increases the stability of the system.\nLag compensation improves the steady-state accuracy of the system, but reduces the speed of the response.\nIf improvements in both transient response and steady-state response are desired, then both a lead compensator and a lag compensator may be used simultaneously.\nThis type of lead/lag compensators is designed in the frequency domain by determining \\(a\\) from the amount of phase needed to satisfy the phase margin requirements, and determing \\(\\tau\\) to place the added phase at the new gain-crossover frequency.\nThe lead compensator increases the gain of the system at high frequencies (the amount of this gain is equal to \\(a\\)). This can increase the crossover frequency, which will help to decrease the rise time and settling time of the system (but may amplify high frequency noise).\nA lead-lag compensator combines the effects of a lead compensator with those of a lag compensator. The result is a system with improved transient response, stability, and steady-state error.\nTo implement a lead-lag compensator, first design the lead compensator to achieve the desired transient response and stability, and then design a lag compensator to improve the steady-state response of the lead-compensated system."
  },
  {
    "objectID": "intro_to_control_theory.html#notebook-objectives",
    "href": "intro_to_control_theory.html#notebook-objectives",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Notebook Objectives",
    "text": "Notebook Objectives\nThis notebook provides an overview of the big picture problem that we’re trying to solve as control system engineers.\n\nControl theory makes it possible to solve a number of engineering problems, not only as control engineers but as any engineer\n\nSwitching power regulators\nAutomatic gain control circuits that automatically increase or decrease the gain of a signal\nIsolation system in a motor mount that is sensitive to vibrations\nIndustrial robotics\netc.\n\n\nControl system is: - Building models of your system - Simulate them to make predictions - Understanding the dynamics and how they interact with the rest of the system - Filtering and rejecting noise - Selecting or designing hardware (sensors, actuators) - Testing the system in expected and unexpected environments - It is understanding your system!"
  },
  {
    "objectID": "intro_to_control_theory.html#some-cool-examples",
    "href": "intro_to_control_theory.html#some-cool-examples",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Some (cool) examples",
    "text": "Some (cool) examples\n\n\n\n\n\n\n\n\nSpaceX Nails Landing of Reusable Rocket on Land, From Bloomberg Technology\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpaceX Nails Landing of Reusable Rocket on Land (close up)\n\n\n\n\n\n\nWhere does Control System Engineering come into place?\n\nAttitude control\nLanding control\nTrajectory tracking\nLand control (e.g., antennna tracking)\n\n\n\n\n\n\n\n\n\nAutonomous Ferries (Rolls-Royce)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtlas (Boston Dynamics)"
  },
  {
    "objectID": "intro_to_control_theory.html#how-do-you-get-a-system-to-do-what-you-want",
    "href": "intro_to_control_theory.html#how-do-you-get-a-system-to-do-what-you-want",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "How do you get a system to do what you want?",
    "text": "How do you get a system to do what you want?\nControl Theory is a mathematical framework that gives us the tools to develop autonomous systems"
  },
  {
    "objectID": "intro_to_control_theory.html#what-is-a-system",
    "href": "intro_to_control_theory.html#what-is-a-system",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "What is a system?",
    "text": "What is a system?\nThe concept is straigthforward, but the term is sometime applied very generically\nFor us:\n\nA system is a collection of interconnected parts that form a larger more complex whole\n\nEngineering problems are usually complex. - Divide complex projects into smaller parts, or systems makes it possible to simplify the problem and to specialise in specific areas - Usually more than one layer of complexity - Each one of the interconnected parts that form a larger system can be a complex system in its own right!\n\nControl systems\nAs a control engineer: - Goal is to create something that meets the functional or performance requirements you set for the project. - The collection of the interconnected parts that are created specifically to meet these requirements are the control system (at least in general) - More specifically: A control system is a mechanism (hardware or software) that alters the future state of a system\n\nFor any project however the control system again might be a collection of interconnected parts that require specialists:\n\nsensor experts,\nactuators experts,\ndigital signal processing experts,\nstate estimation experts,\netc.\n\n\nFor example, car braking system:\n\n\n\n\n\n\n\n\n\n\n\nAnd of course, the braking system itself is just one of the main interconnected parts that create the car.\n\n\nSystem as a box\n\nWe represent systems graphically as a box\nArrows going into the box are inputs\nArrows going out of the box are output\n\n\n\n\n\n\n\nThe system inside the box is described through a math model (e.g. equation of motion)\nThe “box” can be used to describe systems that are simple or complex\n\nSo if we want to start representing it more formally:\n\n\n\n\n\n\n\\(u\\): control inputs\n\\(\\xi\\): disturbances\n\\(f\\): system (physics is involved here!)\n\\(\\theta\\): system parameters (e.g., mass, inertia, spring constants, etc. )\n\\(y\\): output\n\nA system can be anything..\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes we also talk about parts, components, subsystems and processes (or plants)"
  },
  {
    "objectID": "intro_to_control_theory.html#three-different-problems",
    "href": "intro_to_control_theory.html#three-different-problems",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Three different problems",
    "text": "Three different problems\n\nThree parts to our simple block diagram\n\nThe system\nThe inputs (that drive the system)\nThe output (that the system generates)\n\n\nAt any given time one of the three parts are unknown, and the part that is unknown defines the problem that you are solving.\n\nThe system identification problem\n\nAs a practicing engineer you won’t always be given a model of your system\nYou will need to determine it yourself\nThis is done through a process called System Identification\n\n\n\n\n\n\n\nYou might be doing System Identification if you are asking yourself these questions:\n\nWhat are the mathematical equations that will convert my known inputs into my measured outputs?\nWhat should I model?\nWhat are the relevant dynamics of my system?\nWhat is my system?\n\n\n\n\nBlack box vs White box\nIn system identification, black box and white box identification are two different approaches for modeling a system.\nBlack box: - You are given a box that you cannot open, but you are asked to model what is inside - Subject what is inside the box to various known inputs, measure the output and then infer what is inside the box based on the relationship between the input and the output.\nWhite box - Now you can see exactly what is inside the box (hardware, software). - You can write the mathematical equations of the dynamics directly (e.g. Netwon’s equations of motion).\nFor example, for a sping-mass system:\n\n\n\n\n\n\\[F = m\\ddot{x} + kx\\]\n\n\n\nEven the while box method might require to run tests and applying inputs to measure outputs to calculate the parameters of your system.\nE.g., modeling a linear spring: you know the equation but what is the exact spring constant?\n\nUsually, you need to do a bit of both."
  },
  {
    "objectID": "intro_to_control_theory.html#example-modeling-a-car",
    "href": "intro_to_control_theory.html#example-modeling-a-car",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Example: modeling a car",
    "text": "Example: modeling a car\nLet’s consider an example: car cruise control, using a simplified mathematical model.\n\n\n\n\n\n\\[m\\frac{dv}{dt} + \\alpha|v|v+\\beta v = \\gamma u-mgsin(\\theta)\\]\n\nInput: gas pedal\nOutput: speed (speedometer)\nDisturbance: the slope of the road\n\nAssumptions to simplify the model: - Thrust is proportional to how much pressure we put on the pedal and this directly translates into how much is open the gas valve - Frictions and drags are linear with speed - Small angles: \\(\\theta < 30^o\\) so that \\(sin(\\theta) \\approx \\theta\\)\nUsing our assumptions, the previous model can be simplified as:\n\\[m\\frac{dv}{dt} + \\alpha|v|v+\\beta v = \\gamma u-mgsin(\\theta) \\approx m\\frac{dv}{dt} + \\beta v = \\gamma u-mg\\theta\\]\nEven in those cases where we want to simplify the model, it can often be useful to have it in a standard representation, in this case state space representation:\n\\[\\mathbf{\\dot{x}} = [\\dot{x_1}, \\dot{x_2}]^T\\]\n\\[\\begin{cases}\n\\dot{x_1} &= x_2 \\\\\n\\dot{x_2} &= -\\frac{\\alpha}{m}|x_2|x_2 - \\frac{\\beta}{m}x_2 + \\frac{\\gamma}{m}u -gsin(\\theta) \\\\\ny &=x_2\n\\end{cases}\\]\n\\[\\mathbf{x}(t_0) = x_0\\]\n\nThis is a system of first-order ordinary differential equations (ODEs) that describes the motion of our car along a road.\nVelocity is the derivative of the position\nTwo state variables: position \\(x_1\\) and velocity \\(x_2\\)\nTwo inputs: \\(u\\) (Force applied to the car) and \\(\\theta\\) (angle of the road with respect to the horizontal)\nOne output: velocity of the car\nThe initial condition \\(\\mathbf{x}(t_0) = x_0\\) specifies the initial position and velocity of the car at time \\(t_0\\).\nThe state \\(\\mathbf{x}\\) includes the minimal amount of information at the current time to predict its behaviour in the future, only based on the knowledge of its future inputs.\nIn the case where we have differential equations from actual physical systems we also need the initial conditions.\nNote that there is no time in the equations above: time-invariant system\n\nState equations - standard form: \\[\n\\dot{x} = f(x, u, t;\\theta)\n\\]\n\\[\ny = g(x, u, t;\\theta)\n\\]\n\\[\nx(t_0) = x_0\n\\]"
  },
  {
    "objectID": "intro_to_control_theory.html#standard-representations",
    "href": "intro_to_control_theory.html#standard-representations",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Standard representations",
    "text": "Standard representations\n\n“Standard representations” are the bulding blocks of the engineering language\nAnalysis and design are based on having the system in a standard representation\nSpecific problems typically need to be translated into a standard representation\nControl System Software typically assumes that the system is already in a standard representation (and expects inputs based on that)\n\nThis looks like something we can code up!\n\nsource\n\nCar\n\n Car (x0, params)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "intro_to_control_theory.html#model-linearisation-intro",
    "href": "intro_to_control_theory.html#model-linearisation-intro",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Model Linearisation (intro)",
    "text": "Model Linearisation (intro)\n\nThe previous model was relatively complicated (e.g., included sin and cos, module of…)\nWe can simplify it to make it more mathematically manageble\nThis is called Linearisation\nLinearisation involves creating a linear approximation of a nonlinear system that is valid in a small region around the operating or trim point, a steady-state condition in which all model states are constant.\nLinearisation is needed to design a control system using classical design techniques (e.g., Bode plot and root locus design) and analyse system behavior, such as system stability, disturbance rejection, and reference tracking.\n\n\nEquilibrium\nGiven a system:\n\\[\n\\dot{x} = f(x,u,t;\\theta)\n\\]\nand given an input \\(u(t)\\),\na state \\(\\bar{x}\\) is said to be an equilibrium if:\n\\[\n0 = f(\\bar{x},u,t;\\theta)\n\\]\nthe system does not move from \\(\\bar{x}\\).\n\n\nLinearising around an equilibrium\n\nWe need to first assume that the system operates near an operating point or equilibrium point.\nAt this point, nonlinear effects can be neglected.\nFor simplicity:\n\nwe neglect time dependency\nwe omit parameters \\(\\theta\\)\n\nGiven input \\(\\bar{u}\\)\nEquilibrium \\(\\bar{x}\\)\n\nwe can write:\n\\[\n\\dot{x} = f(x,u)\n\\]\n\\[\n0 = f(\\bar{x},\\bar{u})\n\\]\nWe can also define new variables \\(\\tilde{x} = x - \\bar{x}\\), which represents the deviation of the state from the equilibrium point and \\(\\tilde{u} = u - \\bar{u}\\), which represents the deviation of the input from the equilibrium input:\n\\[\\tilde{x} = x - \\bar{x}\\] \\[\\tilde{u} = u - \\bar{u}\\]\n\n\nTaylor expansion around the equilibrium\n\nApproximate a function as an infinite sum of its derivatives evaluated at a single point\nGiven a function f(x), the Taylor series expansion around a point a is:\n\n\\[ f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x-a)^n \\]\nwhere \\(f^{(n)}(a)\\) denotes the \\(n\\)th derivative of \\(f\\) evaluated at \\(a\\), and \\(n!\\) is the factorial of \\(n\\).\n\nWe apply Taylor expansion to the state equations around the equilibrium and we keep only the linear terms\n\n\\[\n\\dot{x} = f(x, u) = f(\\bar{x},\\bar{u}) + \\frac{\\partial f}{\\partial x}\\Big|_{x=\\bar{x}, u=\\bar{u}} (x-\\bar{x}) + \\frac{\\partial f}{\\partial u}\\Big|_{x=\\bar{x}, u=\\bar{u}} (u-\\bar{u}) + \\mathcal{O}(\\Delta^2x) + \\mathcal{O}(\\Delta^2u)  \n\\]\n\nTo linearize the system described by the following equation:\n\\[m\\frac{dv}{dt} + \\alpha|v|v+\\beta v = \\gamma u-mgsin(\\theta)\\]\n\nWe need to first assume that the system operates near an operating point or equilibrium point. At this point, the velocity and input are assumed to be small enough such that their nonlinear effects can be neglected.\nWe can then use Taylor series expansion to approximate the nonlinear terms around this equilibrium point, keeping only the linear terms.\nAssuming an equilibrium point of \\(v=v_0\\) and \\(u=u_0\\), the Taylor series expansion of the nonlinear terms around this point is:\n\n\\[\\alpha|v|v \\approx \\alpha|v_0|v + \\frac{\\alpha v_0^2}{2}\\left(\\frac{v-v_0}{|v_0|}\\right)\\]\n\nSubstituting this approximation into the original equation, we get:\n\n\\[m\\frac{dv}{dt} + \\alpha|v_0|v + \\frac{\\alpha v_0^2}{2}\\left(\\frac{v-v_0}{|v_0|}\\right) + \\beta v = \\gamma u - mgsin(\\theta)\\]\n\nWe can then define a new variable \\(\\tilde{v} = v - v_0\\), which represents the deviation of the velocity from the equilibrium point. Using this variable, we can rewrite the equation as:\n\n\\[m\\frac{d\\tilde{v}}{dt} + (\\alpha|v_0|+\\frac{\\alpha v_0^2}{2|v_0|})\\tilde{v} + \\beta v_0 = \\gamma \\tilde{u}\\]\nwhere \\(\\tilde{u} = u - u_0\\) is the deviation of the input from its equilibrium point.\n\nThis linearised equation is in the form of a standard linear system, with \\(\\tilde{v}\\) as the state variable, \\(\\tilde{u}\\) as the input, and \\(m\\), \\((\\alpha|v_0|+\\frac{\\alpha v_0^2}{2|v_0|})\\), and \\(\\beta v_0\\) as the system coefficients.\n\n\nIn control system we tend to use standard representations and in the case of our state space model, and apply the Taylor expansion to that:\n\\[\n\\dot{\\tilde{x}} = \\dot{x} - \\dot{\\bar{x}} = f(x, u) - f(\\bar{x}, \\bar{u})\n\\]\n\\[\n\\dot{x} = f(x, u) = f(\\bar{x},\\bar{u}) + \\frac{\\partial f}{\\partial x}\\Big|_{x=\\bar{x}\\\\u=\\bar{u}} (x-\\bar{x}) + \\frac{\\partial f}{\\partial u}\\Big|_{x=\\bar{x}\\\\u=\\bar{u}} (u-\\bar{u}) + \\mathcal{O}(||\\Delta x||^2) + \\mathcal{O}(||\\Delta u||^2)  \n\\]\nIn this case, the matrices \\(\\frac{\\partial f}{\\partial x}\\) and \\(\\frac{\\partial f}{\\partial u}\\) are called Jacobians. These are matrix of numbers.\n\\[\\begin{equation}\nA = \\frac{\\partial f}{dx}\\Big|_{\\;\\bar{x}, \\bar{u}}= \\begin{bmatrix}\n\\frac{\\partial f_1}{dx_1} & \\frac{\\partial f_1}{dx_2} & ...\\\\\n\\frac{\\partial f_2}{dx_1} & \\frac{\\partial f_2}{dx_2} & ... \\\\\n... & ... & ...\n\\end{bmatrix}_{\\;\\bar{x}, \\bar{u}}\n\\end{equation}\\]\n\\[\\begin{equation}\nB = \\frac{\\partial f}{du}|_{\\;\\bar{x}, \\bar{u}}== \\begin{bmatrix}\n\\frac{\\partial f_1}{du_1} & \\frac{\\partial f_1}{du_2} & ...\\\\\n\\frac{\\partial f_2}{du_1} & \\frac{\\partial f_2}{du_2} & ... \\\\\n... & ... & ...\n\\end{bmatrix} _{\\;\\bar{x}, \\bar{u}}\n\\end{equation}\\]\n\\(A \\in \\mathcal{R}^{n \\cdot n}\\), \\(B \\in \\mathcal{R}^{n \\cdot m}\\)\nThis means that we can write:\n\\[\n\\dot{x} = Ax+Bu\n\\]\n\nNote that when we linearise, \\(x\\) and \\(u\\) are the deviations w.r.t. the equilibrium.\nWe can also calculate the Jacobian for the output equation which leads the output equation is linear form:\n\n\\[\ny = Cx + Du\n\\]\nLet’s go back to our car example.\nWe need to compute the Jacobian matrix of the system evaluated at the operating point \\((\\bar{x},\\bar{u})\\), where \\(\\bar{x}\\) and \\(\\bar{u}\\) are the constant equilibrium values of \\(x\\) and \\(u\\), respectively.\n\\[\\begin{equation}\n\\dot{x}=f(x,u)=\n\\begin{cases}\n\\dot{x_1} &= x_2 \\\\\n\\dot{x_2} &= -\\frac{\\alpha}{m}|x_2|x_2 - \\frac{\\beta}{m}x_2 + \\frac{\\gamma}{m}u -gsin(\\theta)\n\\end{cases}\n\\end{equation}\\]\n\\[\\;\\;\\]\n\\[\\begin{equation}\nA = \\frac{\\partial f}{\\partial x}= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2}\\\\\n\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 1\\\\\n0 & -\\frac{\\alpha}{m}|\\bar{x}_2|-\\frac{\\beta}{m}\n\\end{bmatrix}\n\\end{equation}\\]\n\\[\\begin{equation}\nB=\\frac{df}{\\partial u}= \\begin{bmatrix}\n\\frac{\\partial f_1}{\\partial u} & \\frac{\\partial f_1}{\\partial\\theta}\\\\\n\\frac{\\partial f_2}{\\partial u} & \\frac{\\partial f_2}{\\partial\\theta}\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0\\\\\n\\frac{\\gamma}{m} & -gcos(\\theta)\n\\end{bmatrix}\n\\end{equation}\\]\n\nand if we choose input \\(\\theta=0\\) and \\(u=0\\)\nthen an equilibrium is \\(\\forall x_1, x_2=0\\) (\\(x_1\\) does not affect the result):\n\n\\[\\begin{equation}\nA=\\frac{\\partial f}{\\partial x}\\Big|_{x_1=0\\\\x_2=0}=\n\\begin{bmatrix}\n0 & 1\\\\\n0 & -\\frac{\\beta}{m}\n\\end{bmatrix}\n\\end{equation}\\]\n\\[\\begin{equation}\nB=\\frac{\\partial f}{\\partial u}\\Big|_{u=0\\\\\\theta=0}=\n\\begin{bmatrix}\n0 & 0\\\\\n\\frac{\\gamma}{m} & -g\n\\end{bmatrix}\n\\end{equation}\\]\nand bringing everything together for our system:\n\\[\\dot{\\mathbf{x}}=A\\mathbf{x}+B\\mathbf{u}\\] \\[y=[0 \\; 1]\\mathbf{x}\\]\nThis is also something we can code up!\n\nsource\n\n\nLinearCar\n\n LinearCar (x0, params)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nComments\n\nMultiple models can be used to represent a physical system\nThere are a number of parameters in our models (\\(m\\), \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\))\nTheir value depend on the specific car you are modeling\nIt might not be easy to have good values for these parameters"
  },
  {
    "objectID": "intro_to_control_theory.html#the-simulation-problem",
    "href": "intro_to_control_theory.html#the-simulation-problem",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "The simulation problem",
    "text": "The simulation problem\n\nPredict how your outputs change given a known set of inputs and the mathematical model of the system\nUsually a lot of design time is spent in this stage\nIt can be difficult to figure out the set of inputs and their range to characterise the operational envelope of the system\n\n\n\n\n\n\nYou need to run a simulation to answer: - Does my system model match my test data? - Will my system work in all operating environments? - What happens when… - Simulations can also be used to limit the number of field tests (which are usually expensive or might let you avoid dangerous maneouvers)\nLet’s go back to our Python car. We can now easily simulate it.\n\n# We assume we have identified the model parameters\nm = 10\nalpha = 1\nbeta = 1\ngamma = 1\nparams = (m, alpha, beta, gamma)\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = Car(x_0, params)\n\n\n# We define out inputs:\ntheta = np.radians(20) # Disturbance\nu = 0                  # Input\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 10, 0.1 # time\n\nposition = []\nvelocity = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    print('.', end ='')\n    car.step(dt, u, theta)\n    x, y, v = car.sensor_i()\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('\\n[DONE] Simulation is now complete.')\n\n....................................................................................................\n[DONE] Simulation is now complete.\n\n\nDone! Now we can see what happened..\nFor example we can plot the values of a few interesting variables.\n\nfig, ax = plt.subplots();\n\nplt.plot(time, velocity)\nplt.xlabel('time (s)')\nplt.ylabel('speed (m/s)');\n\n\n\n\nWhat happens if we simulate our LinearCar?\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\n# HERE WE USE LINEARCAR!\ncar = LinearCar(x_0, params)\n\n# We define out inputs:\ntheta = np.radians(20) # disturbance\nu = 0                  # Input\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 10, 0.1 # time\n\nlin_position = []\nlin_velocity = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    print('.', end ='')\n    car.step(dt, u, theta)\n    x, y, v = car.sensor_i()\n    lin_position.append((x,y)), lin_velocity.append(v)\n    time.append(t)\n    \nprint('\\n[DONE] Simulation is now complete.')\n\n....................................................................................................\n[DONE] Simulation is now complete.\n\n\n\nfig, ax = plt.subplots();\n\nplt.plot(time, lin_velocity)\nplt.xlabel('time (s)')\nplt.ylabel('speed (m/s)');\n\n\n\n\n\nComments\n\nMultiple models can be used to represent a physical system\nThe complexity of the model depends on the objectives\nDifferent models might lead to different results\n\n\n\nMore complex representation\n\nOur initial simulation was relatively simple\nWe can have much more complex representations of our simulation\n\n\n# First set up the figure, the axis, and the plot elements we want to animate\nfig, ax = plt.subplots();\n\nax.set_xlim((min(position)[0], max(position)[0]))\nax.set_ylim((min(position)[1]*2, max(position)[1]))\nline, = ax.plot([], [], lw=4);\n\n\n\n\n\n# draw terrain\ndef terrain(theta_rad, x_0, x_range):\n    y_range = theta*(x_range-x_0[0])+x_0[1]-10\n    close_triangle = (x_range[0], y_range[-1])\n    x_range = np.append(x_range, close_triangle[0])\n    y_range = np.append(y_range, close_triangle[1])\n    X = np.matrix([x_range, y_range]).transpose()\n    patch = plt.Polygon(X, color='yellow')\n    return patch\n\nx_range = np.linspace(int(position[0][0]), int(position[-1][0]), num=20)\npatch = terrain(theta_rad=theta, x_0=x_0, x_range=x_range)\n\n# initialization function: plot the background of each frame\ndef init():\n    ax.add_patch(patch)\n    return patch,\n\n# animation function. This is called sequentially\ndef animate(i):    \n    #line.set_data(time[max(0,i-2):i], position[max(0,i-2):i])\n    x_min, x_max = position[max(0,i-2)][0], position[i][0]\n    y_min, y_max = position[max(0,i-2)][1], position[i][1]\n    line.set_data([x_min, x_max], [y_min, y_max])\n    return (line,)\n\n\n# call the animator. blit=True means only re-draw the parts that have changed.\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=len(time), interval=40, blit=True,\n                               repeat_delay=10000, repeat=True);\n\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "intro_to_control_theory.html#the-control-problem",
    "href": "intro_to_control_theory.html#the-control-problem",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "The Control Problem",
    "text": "The Control Problem\n\nControl theory gives you the tools needed to answer this question.\nWithout control theory, the designer is relegated to choosing a control system through trial and error.\nHow can I get my system to meet my performance requirements?\nHow can I automate a process that currently involves humans in the loop?\nHow can my system operate in a dynamic and noisy environment?"
  },
  {
    "objectID": "intro_to_control_theory.html#a-simple-example",
    "href": "intro_to_control_theory.html#a-simple-example",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "A Simple Example",
    "text": "A Simple Example\nsimulation of a basic temperature control system using a simple regulator\n\n\n\n\n\n\n\nimport time\nimport matplotlib.pyplot as plt\n\n# Define the regulator function\ndef temperature_controller(current_temp, setpoint):\n    # Set the allowable error range\n    error_range = 0.5\n\n    # Calculate the error\n    error = setpoint - current_temp\n\n    # If the error is within the allowable range, do nothing\n    if abs(error) <= error_range: return 0.0\n\n    # If the error is positive, turn on the heater\n    elif error > 0: return 1.0\n\n    # If the error is negative, turn on the cooler\n    else: return -1.0\n\n\n# Set the simulation parameters\ninitial_temp = 20.0\nsetpoints = [25.0, 22.0, 27.0]\nduration = 30.0  # seconds for each setpoint\n\n# Initialize the temperature and time lists\ntemperature = [initial_temp]\ntime_list = [0.0]\nsetpoint_list = [setpoints[0]]\n\n# Initialize the regulator output list\nregulator_output = [0.0]\n\n# Simulate the temperature control system\nfor i, setpoint in enumerate(setpoints):\n    t = 0.0\n    setpoint = setpoints[i]\n    while t < duration:\n        # Measure the current temperature\n        current_temp = temperature[-1]\n        # Use the regulator to calculate the control output\n        control_output = temperature_controller(current_temp, setpoint)\n        # Apply the control output to the temperature\n        temperature.append(current_temp + control_output)\n        # Add the control output to the regulator output list\n        regulator_output.append(control_output)        \n        # Add the current setpoint to the setpoint list\n        setpoint_list.append(setpoint)\n        # Update the time\n        t += 1.0\n        # Add the time to the time list\n        time_list.append(i * duration + t)\n\n\n# Plot the results\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.set_xlabel('Time (s)')\nax1.set_ylabel('Temperature (C)', color=color)\nax1.plot(time_list, setpoint_list, color='tab:green', linestyle='--', linewidth=5)\nax1.plot(time_list, temperature, color=color, linewidth=3)\nax1.tick_params(axis='y', labelcolor=color)\nax2 = ax1.twinx()\ncolor = 'tab:blue'\nax2.set_ylabel('Regulator Output', color=color)\nax2.plot(time_list, regulator_output, color=color, linewidth=3)\nax2.tick_params(axis='y', labelcolor=color)"
  },
  {
    "objectID": "intro_to_control_theory.html#linear-time-invariant-lti-systems",
    "href": "intro_to_control_theory.html#linear-time-invariant-lti-systems",
    "title": "Introduction to Control Theory: The Control Problem",
    "section": "Linear Time Invariant (LTI) Systems",
    "text": "Linear Time Invariant (LTI) Systems\nGeneral form of a system:\n\\[\n\\begin{cases}\n\\dot{x}(t) = f(x(t), u(t), t)\\\\\ny(t) = g(x(t), u(t), t)\n\\end{cases}\n\\]\nWhen \\(f\\) and \\(g\\) are linear in \\(x\\) and \\(u\\) then the system can be written in the form:\n\\[\n\\begin{cases}\n\\dot{x}(t) = A(t)x(t) + B(t)u(t)\\\\\ny(t) = C(t)x(t) + D(t)u(t)\n\\end{cases}\n\\]\n\nThe system is said linear in this case,\nMatrices \\(A(t), B(t), C(t), D(t)\\) can be in general function of time,\nIf they are not, we talk about Linear Time-Invariant Systems\n\n\\[\n\\begin{cases}\n\\dot{x}(t) = Ax(t) + Bu(t)\\\\\ny(t) = Cx(t) + Du(t)\n\\end{cases}\n\\]\nAll LTI systems have the following defining properties: - Homogeneity: - If you scale the input \\(u(t)\\) then the output will be scaled by the same factor: - \\(au(t) \\Rightarrow ay(t)\\) (e.g. if you double the input, the output would also double).\n\nex. step input of amplitude 1 and 2:\n\n\n\n\n\n\n\nSuperposition (Additivity)\n\nSuppose that:\n\ninput \\(x_1(t)\\) produces output \\(y_1(t)\\)\ninput \\(x_2(t)\\) produces output \\(y_2(t)\\)\n\nIf you add two inputs together, then the output is the superposition of the two separate outputs, i.e. the sum of the individual outputs:\n\n\n\n\n\n\n\nSometimes, these two properties are stated together: “if the input is scaled and summed, then the output will also be scaled and summed by the same amount”. - A system that respect these two property is a linear system\n\nTime Invariance\n\nThe system behaves the same regardless of when the action takes place\nFormally there is no explicit dependency of time in the equations\nThe same input translated in time produces the same output also translated in time:\nAn input of \\(x(t-\\tau)\\) produces an output of \\(y(t-\\tau)\\).\n\n\n\n\n\n\nThese conditions are very restritive and practically no real world system meets them.\nHowever, linear systems can be solved!\nWide range of systems can be approximated accurately by an LTI model\n\n\n“Linear systems are important because we can solve them.” — Richard Feynman.\n\n\nImpulse Response\n\nLTI systems can be characterised by their response to an impulse function\nThis is the output of the system when presented with a brief input signal\n\nMore formally, somewhat: The impulse response is the output of a system when an input that is an impulse function is applied to it. An impulse function is a theoretical input signal that is very short in duration and has a very large magnitude, but its area under the curve (integral) is equal to 1.\n\nAlso known as Dirac delta function\nThe impulse response of a system can be used to determine how the system will respond to other inputs.\n\nAn ideal impulse function is a function that is zero everywhere but at the origin, where it is infinitely high.\n\n\n\n\n\nConsider first the ramp function shown in the upper left. It is zero for \\(t<0\\) and one for \\(t>T\\), and goes linearly from 0 to 1 as time goes from 0 to T.\nIf we let \\(T\\rightarrow0\\), we get a unit step function, \\(\\gamma(t)\\) (upper right).\nIf we take the derivative of our ramp function (lower left), we get a rectangular pulse with height 1/T (the slope of the line) and width T.\nThis rectangular pulse has area (height·width) of one.\nIf we take the limit as \\(T\\rightarrow0\\), we get a pulse of infinite height, zero width, but still with an area of one; this is the unit impulse and we represent it by \\(\\delta(t)\\).\nSince we can’t show the height of the impulse on our graph, we use the vertical axis to show the area. The unit impulse has area=1, so that is the shown height.\nLet’s see what happens when we apply an impulse to our car.\n…Before we do that, we need to define an impulse function in Python.\n…and before we do that, let’s note that in the real world, an impulse function is a pulse that is much shorter than the time response of the system (sometime also called Dirac pulse).\n\nsource\n\n\ndelta\n\n delta (t, delta_t=0, eps=None)\n\nDirac Pulse\nThe function delta computes an approximation of the impulse function (also called Dirac delta function) over a given time range t. The impulse function is a theoretical function that is zero everywhere except at t=0, where it is infinite. In practice, we cannot generate an infinite impulse, so we use a finite approximation.\nThe function takes three arguments: t: a scalar or an array of time values at which to compute the function. delta_t: the time at which the impulse occurs. This is an optional argument with a default value of 0, which means the impulse occurs at t=0. eps: the width of the pulse used to approximate the impulse. This is an optional argument with a default value of None, which means the function will try to estimate it automatically based on the time values in t. If eps is not defined and t has more than one element, eps is estimated as the difference between the first two elements of t. Otherwise, eps is set to the value of the eps argument.\nComputation of the impulse approximation: It uses the step function to create a pulse that has a width of eps centered at delta_t, and then subtracts two of these pulses to create a pulse that approximates the impulse function.\nThe function returns the computed pulse as an array of the same shape as t.\n\nsource\n\n\nstep\n\n step (t, step_time=0)\n\nHeaviside step function\nAnd here is what it looks like:\n\nfig, ax = plt.subplots(1, 1, figsize=(3, 3))\n\nt = np.linspace(0, 5, 60)\nax.plot(t, delta(t, delta_t=1), linewidth=3, color='blue')\nax.set_xlabel('time (s)')\nfig.tight_layout()\n\n\n\n\nNow we can apply this impulse as input to our car.\n\n# We assume we have identified the model parameters\nm = 10\nalpha = 1 # it is not used in the linear system\nbeta = 10 # friction\ngamma = 1 # coeff. applied to the input.\nparams = (m, alpha, beta, gamma)\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = LinearCar(x_0, params)\n\ntheta = np.radians(0) # disturbance (deg) - we can also try np.radians(20)\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 10, 0.1 # time\n\nposition = []\nvelocity = []\ninput_values = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    print('.', end='')\n    # HERE, we apply the impulse\n    u = delta(t, delta_t=0, eps=0.01)\n    car.step(dt, u, theta)\n    x, y, v = car.sensor_i()\n    position.append((x,y)), velocity.append(v)\n    time.append(t)\n    input_values.append(u)\n    \nprint('\\n[DONE] Simulation is now complete')\n\n....................................................................................................\n[DONE] Simulation is now complete\n\n\n\nplt.plot(np.arange(t0, tf, dt), velocity, linewidth=4, color='blue', label='t_0')\nplt.xlabel('time (s)'), plt.ylabel('velocity (m/s)');\nplt.ylim(0,)\nplt.grid();\n\n\n\n\nThanks to time invariance: - if we have another impulse we can expect the same response at this second time, - and if we hit twice as hard, then the response is twice as large\n\ndef multiple_impulses(t):\n    u1 = 1*delta(t, delta_t=0, eps=0.01)\n    u2 = 2*delta(t, delta_t=1, eps=0.01)\n    u3 = 2*delta(t, delta_t=4, eps=0.01)\n    \n    return u1 + u2 + u3\n\n\ncar = LinearCar(x0=(0,0), params=params)\n\nposition = []\nvelocity = []\ninput_values = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    print('.', end='')\n    # HERE, we apply the impulse\n    u = multiple_impulses(t)\n    \n    car.step(dt, u, theta)\n    x, y, v = car.sensor_i()\n    position.append((x,y)), velocity.append(v)\n    time.append(t)\n    input_values.append(u)\n    \nprint('\\n[DONE] Simulation is now complete')\n\n....................................................................................................\n[DONE] Simulation is now complete\n\n\n\nplt.plot(np.arange(t0, tf, dt), velocity, linewidth=4, color='blue', label='t_0')\nplt.xlabel('time (s)'), plt.ylabel('velocity (m/s)');\nplt.ylim(0,)\nplt.grid();\n\n\n\n\nThanks to the superposition principle\n\nThe full response is the summation of the signals.\nIf we have a more complex signal (e.g. ramp) we can break it down to a sequence of impulses (the output is the response to each of the impulses)\n\n\nsource\n\n\nramp_as_impulses\n\n ramp_as_impulses (t, time_vector)\n\nAnd we can verify that this is a ramp:\n\ninput_u = []\n\nfor t in np.arange(t0, tf, .4):\n    input_u.append(ramp_as_impulses(t, np.arange(t0, tf, .4)))\n\n\nlen(input_u)\n\n25\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(3, 3))\n\nax.plot(np.arange(t0, tf, .4), input_u, linewidth=3, color='blue')\nax.set_xlabel('time (s)')\nfig.tight_layout()\n\n\n\n\n\ncar = LinearCar(x0=(0,0), params=params)\n\nposition = []\nvelocity = []\ninput_values = []\ntime = []\nfor index, t in enumerate(np.arange(t0, tf, dt)):\n    print('.', end='')\n    # HERE, we apply the ramp as a sequence of impulses\n    u = ramp_as_impulses(t, np.arange(t0, tf, dt))    \n    #print('time:', t, '-> index:', index, '->u[i]:', u[index])\n    car.step(dt, u[index], theta)\n    x, y, v = car.sensor_i()\n    position.append((x,y)), velocity.append(v)\n    time.append(t)\n    input_values.append(u)\n    \n    \nprint('\\n[DONE] Simulation is now complete')\n\n....................................................................................................\n[DONE] Simulation is now complete\n\n\n\nplt.plot(np.arange(t0, tf, dt), velocity, linewidth=4, color='blue', label='t_0')\nplt.xlabel('time (s)'), plt.ylabel('velocity (m/s)');\nplt.ylim(0,)\nplt.grid();\n\n\n\n\n\nIn reality, the impulses and responses will be infinitesimal\nIn this case, the summation in the time domain is a convolution \\(u(t) \\circledast H(t)\\), where \\(H(t)\\) is the impulse response of the system\n\n\\[\n{\\displaystyle (f\\circledast g)(t):=\\int _{-\\infty }^{\\infty }f(\\tau )g(t-\\tau )\\,d\\tau .}\n\\]\nConvolution is a mathematical operation of two functions that describes how the shape of one is modified by the shape of the other. The result of this operation is the integral of their product, after folding one and shifted across the other.\n\nThe impulse response of a system is important in control theory because it allows us to predict the behavior of the system under various inputs, without having to actually apply those inputs.\nWe can simply convolve the input with the impulse response to obtain the output.\nThis can be a difficult integration..\n\n\n\n\nImpulsive Response, formally\nConsider a physical system (electrical, mechanical, pneumatic, etc.) that has an impulsive signal \\(\\delta(t)\\) centered at \\(t=0\\) as input. Let us observe the temporal behavior of a quantity (mechanical, pneumatic, electrical, etc.) that can be considered an output. This new signal is called the impulse response and is indicated by \\(h(t)\\).\nThe behavior of \\(h(t)\\) represents the output quantity observed after a time \\(t\\) has passed since the impulse \\(\\delta(t)\\) was applied as input, and if the system is causal, \\(h(t)=0\\) for \\(t<0\\), as shown below.\nFurthermore, if the system is also linear and time-invariant, by applying an input consisting of multiple impulses, each with a different area \\(a_i\\) and centered at a different instant \\(\\tau_i\\), that is\n\\[x(t) = \\sum_{i}^{N} a_i \\delta(t-\\tau_i)\\]\nan output is obtained equal to\n\\[y(t) = \\sum_{i}^{N} a_i h(t-\\tau_i)\\]\nLet us consider the same physical system as before, to whose input a generic signal \\(x(t)\\) is applied, and that we can represent as decomposed into infinite terms, that is, as an integral sum of impulses centered at \\(\\tau\\) (variable) and area \\(x(\\tau) d\\tau\\) (infinitesimal):\n\\[x(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) \\delta(t - \\tau) d\\tau\\]\nThis expression is formally similar to the discrete expression above.\nThe behavior of the output quantity will therefore be equal to the superposition of infinite impulse responses, each related to a different input value:\n\\[y(t) = \\int_{-\\infty}^{+\\infty} x(\\tau) h(t - \\tau) d\\tau\\]\nwhere \\(x(\\tau)d\\tau\\) is the infinitesimal area of the impulses that make up the input, and \\(h(t - \\tau)\\) is the output at time \\(t\\) caused by the input impulse centered at time \\(\\tau\\).\nThe result obtained, formally similar to the previous discrete expression, is called the convolution integral.\nWe note that \\(h(t)\\) completely characterizes the physical system, as it allows us to calculate its output for any input.\nSee also here\n\nFortunately we have a few aces up our sleeve!\n\nIf we use the Laplace transform and move to the s-domain however, convolutions become multiplications, and things are simpler.\nAs a control engineers we would like to design our systems to be as close as possible to be LTI, or so that the non LTI parts can be ignored.\nSo that the standard LTI tools can be used.\n\nThink of operating regions of a spring. What happens to the relation between force and distance when you pull it too much or compress it too tight vs what happens when you use it in its operating regime\n\n\n\n\"\"\"\nNice animation here: http://195.134.76.37/applets/AppletConvol/Appl_Convol2.html\n\"\"\"\n\n# Define time\nt = np.linspace(0, 29, 30)\n\n# Define the impulse response\nimpulse_response = np.zeros(30)\nimpulse_response[5:25] = 1\n\n# Define the input signal\ninput_signal = np.zeros(30)\ninput_signal[10:20] = 1\n\n# Calculate the convolution of the input signal and the impulse response\noutput_signal = np.convolve(input_signal, impulse_response, mode='full')\n\n# Set up the plot\nfig, axs = plt.subplots(3, 1, figsize=(8, 8))\n\n# Plot the impulse response\naxs[0].plot(t, impulse_response, color='g')\naxs[0].set_xlim(0, len(impulse_response))\naxs[0].set_ylim(0, 1.1)\naxs[0].set_xlabel('Time')\naxs[0].set_ylabel('Amplitude')\naxs[0].set_title('Impulse Response')\n\n# Plot the input signal\naxs[1].plot(t, input_signal, color='r')\naxs[1].set_xlim(0, len(input_signal))\naxs[1].set_ylim(0, 1.1)\naxs[1].set_xlabel('Time')\naxs[1].set_ylabel('Amplitude')\naxs[1].set_title('Input Signal')\n\n# Plot the output signal (initially empty)\naxs[2].plot(output_signal, color='k')\naxs[2].set_xlim(0, len(output_signal))\naxs[2].set_ylim(0, 11)\naxs[2].set_xlabel('Time')\naxs[2].set_ylabel('Amplitude')\naxs[2].set_title('Output Signal')\nline, = axs[2].plot([], [], 'k', label='Sliding window')\n\n# Define the animation function\ndef animate(i):\n    # Calculate the convolution for the current time step\n    print('hee')\n    convolved_signal = np.convolve(input_signal, impulse_response[:i], mode='full')\n    \n    # Update the plot for the output signal\n    line.set_data(range(len(convolved_signal)), convolved_signal)\n    return (line,)\n\n# Set up the animation\nanim = FuncAnimation(fig, animate, frames=len(impulse_response)+1, interval=20)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "stability_margins.html",
    "href": "stability_margins.html",
    "title": "Stability Margins",
    "section": "",
    "text": "Given an LTI system with transfer function: \\(G(s) = C(sI - A)^{-1}B\\)\nand an input signal: \\(u(t)=Asin(\\omega t)\\)\nAssuming that the system is asymptotically stable, it is possible to verify that:\n\\[y(t)=A|G(j\\omega)|sin(\\omega t + \\angle G(j\\omega) ) \\]\nThis means that the output \\(y(t)\\) converges to a sinusoidal signal that has the same frequency of the input, and that has magnitude \\(A|G(j\\omega)|\\) and is shifted in phase by \\(\\angle G(j\\omega)\\).\nNote also that this is true independently of the initial state \\(x_0\\). In fact, since the system is asymptotically stable, the effect of the initial state on the output will go to zero.\n\n\n\n\n\nAs a result: - Magnitude can change - Phase can change\n\nGain: magnitude of output wrt magnitude of input\nPhase: shift of the signal (measured as angles or relative to the signal wavelength)\n\ndelay in the signal is a negative phase sift (convention)\n\nThe output of a single input frequency is a combination of gain and phase shift\nGain and Phase depends on the frequency (can be different at different frequencies)\n\nSee Bode Plots.\n\nWhen we talk about gain and phase margins we need to consider all frequencies\n\nSee also notebook 06_Intro_to_freq_response_and_bode_plots"
  },
  {
    "objectID": "stability_margins.html#margins",
    "href": "stability_margins.html#margins",
    "title": "Stability Margins",
    "section": "Margins",
    "text": "Margins\nInformal definition: extra amount of a quantity that we can use if needed\nGain and Phase margin: extra gain or phase that we have available before the system starts to oscillates or become unstable\n\nRobustness of the system\nMakes it possible to quantify how stable a system is\n\nSystems (or designs) that have less margin could be considered as being “less” stable: smaller variations in the system could lead to instability\n\n\n\nWhy do we need margins\n\nWhen we design a control system we use a model\nTest is done in known conditions\nWe make assumptions on how the system behaves\nThings rarely behave the way we expect!\nThis leads to gain and phase errors\n\nIf we think of a brushless DC motor: - Rotor inertia is different: commanded torque generates a different acceleration (gain error) - Power is sensitive to temperature (gain error) - Higher drag in bearing: motor is slower to respond (gain and phase error) - Slower computer: adds delay (phase error) - etc.\nThese are called Process Variations: We want our controller to be robust to these because we cannot know our system perfectly and hence we need margins.\nThe more uncertainty we have the more margin we should design in."
  },
  {
    "objectID": "stability_margins.html#what-makes-a-system-unstable",
    "href": "stability_margins.html#what-makes-a-system-unstable",
    "title": "Stability Margins",
    "section": "What makes a system unstable",
    "text": "What makes a system unstable\n\n\\(G(s)\\) is unstable if at least one pole (root of the characteristic eq.) is in the RHP (Re \\(>\\) 0 )\nWe usually design the open loop system and then we use Bode, Nyquist, Root Locus to assess the closed loop stability and performance\nFor this reason is useful to link the open loop performance to their closed loop ones."
  },
  {
    "objectID": "stability_margins.html#what-makes-an-open-loop-system-unstable-in-closed-loop",
    "href": "stability_margins.html#what-makes-an-open-loop-system-unstable-in-closed-loop",
    "title": "Stability Margins",
    "section": "What makes an open loop system, unstable in closed loop",
    "text": "What makes an open loop system, unstable in closed loop\nGiven a system \\(G(s)\\)\n\n\n\n\n\n\n\n\n\nWe know that the closed loop equivalent is:\n\\[\nG_{c}(s) = \\frac{G(s)}{1+G(s)}\n\\]\n\n\\(G_{c}(s)\\) must have a pole in the RHP\nFor this to happen we know that \\(1+G(s)=0\\) or \\(G(s)=-1 \\Rightarrow G_c(s) \\rightarrow \\infty\\)\nWe should keep our open loop \\(G(s)\\) away from -1 then!\nWe saw this already with Nyquist\n\nIt is useful to consider the \\(-1\\) system in more details:\n\n\n\n\n\n\n\n\n\n\nGain is 1 (0 \\(dB\\))\nPhase is \\(-180^o\\)\nIf our system, at any one frequency produces 0 \\(dB\\) and a phase of \\(-180^o\\), then it is unstable\nOur system must stay away from that point throughout the frequency spectrum\n\nMargin: how far away our system is from the \\(0dB\\) and \\(-180^o\\) point\n\nGain margin is defined as the amount of change in open-loop gain needed to make a closed-loop system unstable.\nPhase margin is defined as the amount of change in open-loop phase needed to make a closed-loop system unstable.\n\nsee also stability margins"
  },
  {
    "objectID": "stability_margins.html#bode-plot",
    "href": "stability_margins.html#bode-plot",
    "title": "Stability Margins",
    "section": "Bode Plot",
    "text": "Bode Plot\n\nStart identifying our point on the diagrams:\n\n\n\n\n\n\n\n\n\n\n\nLet’s now suppose to increase gain\n\n\n\n\n\n\n\n\n\n\n\nIncreasing gain, does not modify the phase\nThe crossover frequency moves closer to where the phase diagram crosses the \\(-180^o\\) point.\nOur system has become less stable\n\n\n\n\n\n\n\n\n\n\n\nIf we choose the gain \\(K\\) wrongly the system becomes unstable\n\n\n\n\n\n\n\n\n\n\n\nThe gain required is the gain at the \\(-180^o\\) frequency. This is the gain margin.\nSimilarly for the phase (e.g., slower computers would add a delay which only affects the phase)\n\n\n\n\n\n\n\n\n\n\n\nPhase margin: how much phase delay takes to make \\(-180^o\\) phase at the 0 \\(dB\\) gain frequency.\n\\(90^o\\) in our toy example\nEasy to read the gain and phase margin directly from the Bode plot\n\n\nPython Control Library: Stability Margins\n\nimport control\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nLet’s define the open loop transfer function:\n\nG = control.tf([1.3, 2], [1, 1, 6, 1])\nprint(G)\n\n\n     1.3 s + 2\n-------------------\ns^3 + s^2 + 6 s + 1\n\n\n\nWe can now call the bode_plot function of the Python Control Library to plot the Bode plots.\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(G, dB=True);\n\n\n# Let's also plot lines corresponding to the 0 dB point (gain plot), and the -180 deg point (phase plot)\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--')\nplt.plot([0.3, 0.3],[-90, 10],'r--', linewidth=3)\nplt.plot([3.9, 3.9],[-90, 10],'g--', linewidth=3)\n\nplt.sca(ax2)                 # phase plot\nplt.plot(plt.xlim(),[-180, -180],'b--')\nplt.plot([0.3, 0.3],[-190, 0],'r--', linewidth=3)\nplt.plot([3.9, 3.9],[-190, 0],'g--', linewidth=3)\n\n\n\n\n\nApproximately 130 deg of phase margin (red)\nApproximately 15 dB of gain margin (green)\n\nThe command margin returns the gain and phase margins, and the corresponding crossover frequencies.\nNote: gain value returned by the margin command is not in dB\n\ngm, pm, wpc, wgc = control.margin(G);\nprint('Gain Margin {:.2f} at frequency (rad/sec) {:.2f}'.format(gm, wpc))\nprint('Gain Margin (dB) {:.2f} at frequency (rad/sec) {:.2f}'.format(20*np.log10(gm), wpc))\nprint('Phase Margin {:.2f} (deg) at frequency (rad/sec) {:.2f}'.format(pm, wgc))\n\nGain Margin 7.14 at frequency (rad/sec) 3.91\nGain Margin (dB) 17.08 at frequency (rad/sec) 3.91\nPhase Margin 127.69 (deg) at frequency (rad/sec) 0.31\n\n\n\n\nAdditional comments\n\nGain and phase margin refers to margin in the whole open loop system\nUncertainty in one specific parameters can affect you more than you think.\nTry and change on of the parameters of the denominator of the \\(G(s)\\) above and see what happens\n\nFor example, consider what happens with the following transfer function:\n\nG1 = control.tf([1.3, 2], [1, 1, 2, 1])\nprint(G1)\n\n\n     1.3 s + 2\n-------------------\ns^3 + s^2 + 2 s + 1\n\n\n\n\ngm, pm, wpc, wgc = control.margin(G1);\nprint('Gain Margin {:.2f} at frequency (rad/sec) {:.2f}'.format(gm, wpc))\nprint('Gain Margin (dB) {:.2f} at frequency (rad/sec) {:.2f}'.format(20*np.log10(gm), wpc))\nprint('Phase Margin {:.2f} (deg) at frequency (rad/sec) {:.2f}'.format(pm, wgc))\n\nGain Margin 1.43 at frequency (rad/sec) 1.96\nGain Margin (dB) 3.10 at frequency (rad/sec) 1.96\nPhase Margin 5.04 (deg) at frequency (rad/sec) 1.79\n\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(G, dB=True);\ncontrol.bode_plot(G1, dB=True);\n\n\n# Let's also plot lines corresponding to the 0 dB point (gain plot), and the -180 deg point (phase plot)\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--')\nplt.plot([1.79, 1.79],[-90, 10],'r--', linewidth=1)\nplt.plot([1.96, 1.96],[-3.10, 0],'g--', linewidth=2)\nplt.plot([1.96],[-3.10],'g.', markersize=10)\n\nplt.sca(ax2)                 # phase plot\nplt.plot(plt.xlim(),[-180, -180],'b--')\nplt.plot([1.79, 1.79],[-180, -180+5.04],'r--', linewidth=2)\nplt.plot([1.79],[-180+5.04],'r.', markersize=10)\nplt.plot([1.96, 1.96],[-190, 0],'g--', linewidth=1)\n\n\n\n\nEffectively we have substantially reduced our stability margins.\n\n\nReading stability margins: misleading values\nLet’s now consider the following transfer function:\n\nG2 = control.tf([0.38, 0.038, 0.38*0.55], [1, 1.06, .56, 0.5, 0])\nprint(G2)\n\n\n   0.38 s^2 + 0.038 s + 0.209\n---------------------------------\ns^4 + 1.06 s^3 + 0.56 s^2 + 0.5 s\n\n\n\nAnd as before we can plot the Bode plots and verify what stability margins we have:\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(G2, dB=True);\n\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--')\nplt.plot([0.41, 0.41],[-40, 10],'r--', linewidth=3)\n\nplt.sca(ax2)                 # phase plot\nplt.plot(plt.xlim(),[-180, -180],'b--')\nplt.plot([0.41, 0.41],[-180, -180+69.77],'r--', linewidth=3)\nplt.plot([0.41],[-180+69.77],'r.', markersize=15, linewidth=3)\n\n\n\n\n\nInfinite gain margin: phase diagram never crosses the -180 deg. This would mean that you can add any gain you desired and the closed loop system would never go unstable.\nPhase margin is approximately: 70 deg\n\nWe can confirm this with the Python Control Library:\n\ngm, pm, wpc, wgc = control.margin(G2);\nprint('Gain Margin {:.2f} at frequency (rad/sec) {:.2f}'.format(gm, wpc))\nprint('Gain Margin (dB) {:.2f} at frequency (rad/sec) {:.2f}'.format(20*np.log10(gm), wpc))\nprint('Phase Margin {:.2f} (deg) at frequency (rad/sec) {:.2f}'.format(pm, wgc))\n\nGain Margin inf at frequency (rad/sec) nan\nGain Margin (dB) inf at frequency (rad/sec) nan\nPhase Margin 69.77 (deg) at frequency (rad/sec) 0.41\n\n\n\nHowever, consider the dip in the phase plot: a small phase lag into the system it would bring the phase diagram to cross the -180 deg line, and the gain, for that frequency is very close to 0dB.\nWe do not have 70deg of phase margin, in practise we only have ~2 deg.\nThis are important considerations when designing a controller"
  },
  {
    "objectID": "stability_margins.html#using-the-nyquist-plot-and-the-system-sensitity-revised",
    "href": "stability_margins.html#using-the-nyquist-plot-and-the-system-sensitity-revised",
    "title": "Stability Margins",
    "section": "Using the Nyquist Plot and the System Sensitity Revised",
    "text": "Using the Nyquist Plot and the System Sensitity Revised\nInserire spiegazione Margine fase e Guadagno nel Diagramma di Nyquist (see for ex. CA_AUT_L06_Marg_Stab_1.pdf\nTo try an understand what is happening a little more, let’s draw the Nyquist plot:\n\nfig = plt.figure(figsize=(10,5))\ncontrol.nyquist_plot(G2)\n# plt.xlim((-1.2, 0))\n# plt.ylim((-1.2, 1.2))\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo amount of gain can move the plot around -1 (the gain is 0 when the plot crosses the real axis)\nIn addition to the gain and phase margins we can consider the behaviour of the point that is closer to the -1 point.\nProcess variations that affect that part of the plot are worse than pure gain or phase alone.\nThis is where the sensitivity function of the system can help us.\n\nWe saw that the sensitivity function is:\n\\[\nS(s) = \\frac{1}{1+GR}\n\\]\n\nDescribes how feedback influences the disturbances\nIf \\(|GR|\\) is large at frequencies where the power of the disturbance is concentrated, then \\(|S|\\) is small and the effect of the disturbance on the output is attenuated\nLower values of \\(|S|\\) means higher attenuation of the external disturbance.\nTypically, plan disturbances are low frequency, and one would like \\(|GR|\\) to be large at low frequency\n\nSee notebook 08_Main_types_of_loops_and_transfer_functions.\nWe can also define the Nominal Sensitivity Peak:\n\\[\nM_s = \\max_{0 \\leq \\omega \\leq \\infty} | S(j\\omega) | = \\max_{0 \\leq \\omega \\leq \\infty} \\Big| \\frac{1}{1+G(j\\omega)R(j\\omega)} \\Big|\n\\]\n\nClosest point of the Nyquist plot to -1\n\nThis is the point that is most in danger of causing instability\nRember that the Nyquist plot shows the open loop transfer function: \\(G(j\\omega)R(j\\omega)\\)\nWe want to calculate the minimum distance between -1 and the curve \\(G(j\\omega)R(j\\omega)\\):\n\n\\[\nd = \\min(|G(j\\omega)R(j\\omega) - (-1)|)\n\\]\n\nNote that if we use the metric above, we have something that tells us the smaller the number is the larger its sensitivity to variations (similar to robustness)\nIf we take the reciprocal insted we have a metric such that the larger it is, the larger the sensitivity\n\n\\[\n\\max_{-\\infty < \\omega < \\infty} \\Big| \\frac{1}{G(j\\omega)R(j\\omega) +1} \\Big|\n\\]\n\nSince Nyquist is symmetrical w.r.t the real axis we can restrict it to \\({0 \\leq \\omega \\leq \\infty}\\).\nWe have now a new definition for our sensitivity function: it is the minimum distance between the point -1 and the Nyquist curve of \\(R(j\\omega)G(j\\omega)\\)\n\nLet’s now consider the block diagram of a standard “servo” or tracking configuration of classical feedback control:\n\n\n\n\n\n\nLet’s restrict ourselves to the inputs \\(y_{ref}\\) and \\(d\\). In this case we saw that we can write:\n\\[\nY(s) = \\frac{RG}{1+RG}Y_{ref}(s) + \\frac{1}{1+RG}D(s)\n\\]\n\nAs we saw the sensitivity function captures what happens to the system when we have disturbances or process variation\nWe can determine sensitivity across the entire spectrum using the Bode plot.\n\nLet’s revisit our system\n\nG2 = control.tf([0.38, 0.038, 0.38*0.55], [1, 1.06, .56, 0.5, 0])\nprint(G2)\n\n\n   0.38 s^2 + 0.038 s + 0.209\n---------------------------------\ns^4 + 1.06 s^3 + 0.56 s^2 + 0.5 s\n\n\n\nWe can calculate the sensitivity function \\[\n\\frac{1}{1+G}\n\\]\nas:\n\nsensitivity = control.tf([1, 53/50, 14/25, 0.5], [1, 1.0595, 0.94, 0.5378, 0.2090])   \nprint(sensitivity)\n\n\n       s^3 + 1.06 s^2 + 0.56 s + 0.5\n--------------------------------------------\ns^4 + 1.06 s^3 + 0.94 s^2 + 0.5378 s + 0.209\n\n\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(sensitivity, dB=True);\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--') # 0 dB line\nplt.plot([0.71],[14],'r.', markersize=15, linewidth=3)\nplt.text(0.8, 13, 'Max sensitivity');\n\nText(0.8, 13, 'Max sensitivity')\n\n\n\n\n\n\nThe Max Sensitivity Peak is approximately \\(13\\) dB\nIf we convert it back out of dB: 10^(13/20) = 4.5\nThis is a very high value - Typical sensitivity peak should stay between 1.3 and 2\n\nCan we improve?\n\nHalf the gain\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(G2, dB=True);\ncontrol.bode_plot(0.5*G2, dB=True);\n\n\n\n\n\nThe gain plot in the open loop dropped by 2 or 6 dB\n\nWe can calculate the sensitivity function \\[\n\\frac{1}{1+0.5*G}\n\\]\nNote that we cannot simply multiply the previous sensitivity by 0.5. It matters where the gain is!\n\nnew_sensitivity = control.tf([1, 1.06, 0.56, 0.5, 0], \n                             [1, 1.05995, 0.749986425, 0.518980167284, 0.104492961264])\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(sensitivity, dB=True); # omega_limits=[0.5, 1]\ncontrol.bode_plot(new_sensitivity, dB=True);\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--'); # 0 dB line\n#plt.plot([0.71],[14],'r.', markersize=15, linewidth=3)\n#plt.text(0.8, 13, 'Max sensitivity');\n\n\n\n\n\nWe substantially improved the sensitivity\nPeak now less than 5 dB\nWhich corresponds to 1.7\n\nWhat did we do to our system? Let’s now analyse the step response\n\nt, yout = control.step_response(G2/(1+G2))\n\nt_new, yout_new = control.step_response(0.5*G2/(1+0.5*G2))\n\n\nfig = plt.figure(figsize=(15, 5))\n\nplt.plot(t, yout, 'b')\nplt.plot(t_new, yout_new, 'r')\nplt.xlim(0, 50)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Amplitude')\nplt.grid();\n\n\n\n\n\nWe have slowed down the response of the system. The rise time is now much slower.\nReducing the gain across the frequency spectrum, affects low frequencies, which determine our rise time and our ability to follow slow moving reference signals\n\nCan we do better?\nLet’s apply a more complex controller (more to come later):\n\nnotch = control.tf([1, 0, 0.7**2], [1, 0.7/2, 0.7**2])\n# notch = (s**2 + 0.7**2)/(s**2 + 0.7/2*s + 0.7**2)\n#formally this is a notch filter centered at 0.7 rad/s with a Q = 2. \n# This is pretty steep, so it should only affect a narrow frequency range\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(notch);\n\n\n\n\nLet’s see the bode plot of the new system\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(G2, dB=True);\ncontrol.bode_plot(0.5*G2, dB=True);\ncontrol.bode_plot(notch*G2, dB=True);\n\n\n\n\n\nAs expected only frequencies around 0.7 rad/s are affected.\n\nWe can calculate the new sensitivity function:\n\\[\nS = \\frac{1}{1+notch \\cdot G}\n\\]\n\nnotch_sensitivity = control.tf([1, 1.41, 1.421, 1.2154, 0.4494, 0.245, 0], \n                               [1, 1.40979, 1.801045539, 1.253306815398, 0.8446772388437, 0.263622089835, 0.10243055628])\n\n\nfig = plt.figure(figsize=(15, 10))\ncontrol.bode_plot(sensitivity, dB=True); # omega_limits=[0.5, 1]\ncontrol.bode_plot(new_sensitivity, dB=True);\ncontrol.bode_plot(notch_sensitivity, dB=True);\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'b--'); # 0 dB line\n\n\n\n\n\nThe sensitivity peak has been moved to lower frequencies, but it still around \\(5\\) dB\n\n\nt_notch, yout_notch = control.step_response(notch*G2/(1+notch*G2))\n\n\nfig = plt.figure(figsize=(15, 5))\n\nplt.plot(t, yout, 'b')\nplt.plot(t_new, yout_new, 'r')\nplt.plot(t_notch, yout_notch, 'g')\n\nplt.xlim(0, 50)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Amplitude')\nplt.grid();\n\n\n\n\n\nThe step response rise time is now closer to our original rise time\nThe notch filter is not affecting low frequencies as much as the simple constant gain did\nMuch more methods can be used and we will investigate other methods in the future\nDepends on performance requirements\nBut also on the complexity of the control that we can have\nRegardless of the method however, taking into account the sensitivity is important!\n\n\n\nConsiderations on stability margins, relative stability and Nyquist plots\nLet’s consider two systems:\n\nG1 = control.tf([1], [30, 5, 4])\nG2 = control.tf([1], [2, 5, 4])\n\nprint('G1:', G1), print('G2:', G2);\n\nG1: \n       1\n----------------\n30 s^2 + 5 s + 4\n\nG2: \n       1\n---------------\n2 s^2 + 5 s + 4\n\n\n\nPoles of the system G1\n\nnp.roots([30, 5, 4])\n\narray([-0.08333333+0.35551215j, -0.08333333-0.35551215j])\n\n\n\ncontrol.pzmap(G1, plot=True);\n\n\n\n\nPoles of the system G2\n\nnp.roots([2, 5, 4])\n\narray([-1.25+0.66143783j, -1.25-0.66143783j])\n\n\n\ncontrol.pzmap(G2, plot=True);\n\n\n\n\nTo confirm, let’s evaluate the step response of the two systems:\n\n# Step response\nt1, y1 = control.step_response(G1)\nt2, y2 = control.step_response(G2)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\naxs[0].plot(t1, y1)\naxs[0].set_title('step response for G1')\naxs[1].plot(t2, y2)\naxs[1].set_title('step response for G2');\n\n\n\n\n\nLet’s now look at the Nyquist plots\n\nprint('G1:', G1), print('G2:', G2);\n\nG1: \n       1\n----------------\n30 s^2 + 5 s + 4\n\nG2: \n       1\n---------------\n2 s^2 + 5 s + 4\n\n\n\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=0\\) (start of the plot)\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=\\infty\\) (mid point of the plot)\n\n\nIntersections with the imaginary axis\n\n\nIntersections with the real axis\n\n\n\n\n\n\n\n\nFor the system G1:\n\ncontrol.nyquist_plot(G1)\n\n0\n\n\n\n\n\n\ncontrol.nyquist_plot(G2)\n\n0\n\n\n\n\n\n\nimport control\nG_s = control.tf([1, 1], [1, 2, 2])\nprint(G_s)\n\n\n    s + 1\n-------------\ns^2 + 2 s + 2\n\n\n\n\ncontrol.nyquist(G_s)\n\n0\n\n\n\n\n\n\nG_s = control.tf([1, 1], [1, 2, 1])\nprint(G_s)\ncontrol.nyquist(G_s)\n\n\n    s + 1\n-------------\ns^2 + 2 s + 1\n\n\n\n0"
  },
  {
    "objectID": "part_2_recap.html#nyquist-stability-criterion",
    "href": "part_2_recap.html#nyquist-stability-criterion",
    "title": "Part 2 Recap",
    "section": "Nyquist Stability Criterion",
    "text": "Nyquist Stability Criterion\n\nGoal\n\nWhen we want to study the stability of the closed loop system:\n\nWe want to find the roots of \\(1+𝐺(𝑠)𝐻(𝑠)=0\\),\nthis corresponds to take the open loop transfer function \\(G(s)H(s)\\), add 1 and find its zeros.\n\n\nWhy is this difficult\n\nThe system can be high order (e.g. order 50),\nFinding zeros would only give us stability information,\nOther information could be useful (e.g. stability margins).\n\n\n\nCauchy’s argument principle\n\n\nWe can tell the relative difference between the number of poles and zeros inside of a contour by counting how many time the plot circles the origin and in which direction\n\n\n\nApply Caushy’s argument principle to know if there are zeros of \\(1+GH\\) in the right half plane (in which case the system is unstable)\nPlot \\(GH\\) and shift the origin to the left by 1: we can look at how many circling of the point \\(-1+0j\\) the plot of \\(GH\\) does.\n\n\n\nThe Nyquist Plot\nSteps:\n\nTake the open loop transfer function \\(GH\\)\nPlot the Nyquist plot of \\(GH\\)\n\n\nSet \\(s=j\\omega\\) in the transfer function\n\n\nSweep \\(\\omega \\in [0, \\infty]\\) and plot the resulting complex numbers\n\n\nDraw the reflection about the real axis to account for negative \\(\\omega\\)\n\nNote that the Nyquist contour is traced in the clock-wise direction (by convention)\n\n\nKey is step #2:\n\nFor simple transfer functions there are only four points that we need to solve for\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=0\\) (start of the plot)\n\n\n\\(|G|\\) and \\(\\angle G\\) at \\(\\omega=\\infty\\) (mid point of the plot)\n\n\nIntersections with the imaginary axis\n\n\nIntersections with the real axis\n\n\nCount the number of times the point \\(-1\\) is encircled and in which direction\nDetermine the relative number of poles and zeros inside the nyquist contour.\n\nTherefore: \\[\nZ = N + P\n\\]\nwhere - \\(Z\\) is the number of zeros in the right half plane (or poles in closed loop) - \\(N\\) is the number of clockwise encirclements of -1 - \\(P\\) is the number of open loop right half plane poles"
  },
  {
    "objectID": "part_2_recap.html#stability-margins",
    "href": "part_2_recap.html#stability-margins",
    "title": "Part 2 Recap",
    "section": "Stability margins",
    "text": "Stability margins\n\nGain and Phase margin: extra gain or phase that we have available before the system starts to oscillates or become unstable\nRobustness of the system\nMakes it possible to quantify how stable a system is\n\nSystems (or designs) that have less margin could be considered as being “less” stable: smaller variations in the system could lead to instability\n\nWe want our controller to be robust to these because we cannot know our system perfectly and hence we need margins.\nThe more uncertainty we have the more margin we should design in.\nGain margin: The gain required to cross the 0dB line at the frequency where phase is \\(-180^o\\)\nPhase margin: how much phase delay takes to make \\(-180^o\\) phase at the 0 \\(dB\\) gain frequency.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in one specific parameters can affect you more than you think."
  },
  {
    "objectID": "part_2_recap.html#loop-analysis-and-loop-shaping",
    "href": "part_2_recap.html#loop-analysis-and-loop-shaping",
    "title": "Part 2 Recap",
    "section": "Loop analysis and loop shaping",
    "text": "Loop analysis and loop shaping\nFeedback Goals\n\nStability of the closed loop system\nPerformance while tracking inputs\n\ntracking error, typically specified in terms of steady state error to specific inputs:\n\ne.g., error to step inputs less than \\(5\\%\\)\n\nbehaviour of the transient, typically specified in terms of bandwidth, rise time, settling time, damping ratio, overshoot (these are requirements for the transfer function between \\(Y\\) and \\(Y_{ref}\\)).\n\nRobustness to noise measurement and disturbances\nOne advantage of the Nyquist stability theorem is that it is based on the loop transfer function \\(L = GR\\),\nEasy to see how the controller influences the loop transfer function.\nLoop shaping: Choose a compensator that gives a loop transfer function with a desired shape."
  },
  {
    "objectID": "part_2_recap.html#sensitivity-and-complementary-sensitivity-functions",
    "href": "part_2_recap.html#sensitivity-and-complementary-sensitivity-functions",
    "title": "Part 2 Recap",
    "section": "Sensitivity and Complementary Sensitivity Functions",
    "text": "Sensitivity and Complementary Sensitivity Functions\nWe want to design \\(R\\) so that all transfer functions have good properties - tracking (at freq. where this is important) - disturbance rejection (at freq. where this is important) - noise attenuation (at freq. where this is important)\n\n\\(L(s)=G(s)R(s)\\)\n\\(S(s)=\\frac{1}{1+L(s)}\\)\n\\(T(s)=\\frac{L}{1+L(s)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs a trial-and-error procedure\nWe typically start with a Bode plot of the process transfer function \\(G\\)\nChoosing the gain crossover frequency \\(\\omega_{gc}\\) is a major design decision: a compromise between attenuation of load disturbances and injection of measurement noise\nFinally shape the loop transfer function by changing the controller gain and adding poles and zeros to the controller transfer function\nThe controller gain at low frequencies can be increased by so-called lag compensation, and the behavior around the crossover frequency can be changed by so-called lead compensation.\n\nNominal sensitivity peak \\[\nM_s = \\max_{0 \\leq \\omega \\leq \\infty} | S(j\\omega) | = \\max_{0 \\leq \\omega \\leq \\infty} \\Big| \\frac{1}{1+G(j\\omega)R(j\\omega)} \\Big|\n\\]\n\n\\(M_s\\) gives us an indication of how far \\(L\\) is from -1"
  },
  {
    "objectID": "part_2_recap.html#relationship-between-loop-function-harmonic-response-and-closed-loop",
    "href": "part_2_recap.html#relationship-between-loop-function-harmonic-response-and-closed-loop",
    "title": "Part 2 Recap",
    "section": "Relationship between loop function harmonic response and closed loop",
    "text": "Relationship between loop function harmonic response and closed loop\n\nRelationship between the harmonic response of the loop transfer function \\(L(j\\omega)\\) and the closed loop \\(H(j\\omega)\\)\n\n\\[\n|H(j\\omega)| = \\frac{|L(j\\omega)|}{|1+L(j\\omega)|} = \\frac{|L(j\\omega)|}{\\sqrt{1+|L(j\\omega)|^2}}\n\\]\n\nTranslation of design requirements into requirements on the Bode plot of the loop funtion (barriers)\nThe “barriers” on the Bode plots are there to help us shape the desired harmonic response of the loop function\nThe controller is designed so that the loop function always stays within the admissible regions"
  },
  {
    "objectID": "part_2_recap.html#root-locus",
    "href": "part_2_recap.html#root-locus",
    "title": "Part 2 Recap",
    "section": "Root Locus",
    "text": "Root Locus\n\nSystem with multiple known parameters, and one unknown or varying parameter (K)\nChanging \\(K\\) changes the locations of the poles\nHow to design a system that meets the requirements:\n\nWhat value of \\(K\\) should I choose to meet the requirements (i.e., that places the poles at the correct location in the \\(s\\) plane)\n\nWhat is the effect of variations on a control system that has been already designed:\n\nHow sensitive is the system to a value of \\(K\\) that is slightly different than what we have estimated (or predicted).\n\nRules to sketch the root locus\nTranslate requirements into s-plane requirements:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe angle condition: \\(\\angle (G(s)) = (2n + 1)\\pi\\)\nThe magnitude condition: \\(|KG(s)|=1\\)"
  },
  {
    "objectID": "part_2_recap.html#phase-leadphase-lag-compensators",
    "href": "part_2_recap.html#phase-leadphase-lag-compensators",
    "title": "Part 2 Recap",
    "section": "Phase Lead/Phase Lag Compensators",
    "text": "Phase Lead/Phase Lag Compensators\n\nA lead compensator can increase the stability or speed of reponse of a system;\nA lag compensator can reduce (but not eliminate) the steady-state error.\n\n\\[\nR(s) = \\frac{\\frac{s}{w_z}+1}{\\frac{s}{w_p}+1} = \\frac{w_p}{w_z}\\frac{s + w_z}{s + w_p}\n\\]\nLead compensator: - \\(w_z < w_p\\) - \\(K=\\frac{w_p}{w_z}\\) (gain)\nLag compensator - \\(w_z > w_p\\) - \\(K=\\frac{w_p}{w_z}\\) (gain)\n\nMultiplying the two T.F. together means adding everything together on the Bode plot\nLead/Lag compensator:\n\nBehaves like a real zero early on, at low frequency\nUntil the real pole pulls it back at high frequency\nSee blue line for its approximate representation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the root locus and Bode plots to place the zero and pole"
  },
  {
    "objectID": "part_2_recap.html#pid-controllers",
    "href": "part_2_recap.html#pid-controllers",
    "title": "Part 2 Recap",
    "section": "PID Controllers",
    "text": "PID Controllers\n\nPID = Proportional-Integral-Derivative\n\nDescribes how the error term is treated before being sum and sent into the plant\n\nIt is a simple and effective controller in a wide range of applications\nMajority of controllers in industrial applications are PIDs\n\nThe general structure of a PID controller is:\n\n\n\n\n\n\n\n\n\n\nThe three gains \\(K_p, K_i, K_d\\) are adjustable and can be tuned to the specific application\nVarying \\(K_p, K_i, K_d\\) means adjusting how sensitive the system is across the three paths\nZiegler Nichols tuning rules\nPractical problems:\n\nDerivative is sensitive to noise\n\nFiltering and Set Point Weighting\n\nIf the integral of the error grows too much, the control output might hit actuation limits\n\nIntegral windup: Initializing the controller integral to a desired value or zeroing the integral value every time the error is equal to, or crosses zero reduces the problem.\n\n\nSolving the Cruise Control Problem deploying a PID controller on a car\n\n\nThe Map of Control Theory\n\n\n\n\n\n\n\n\n\nFrom Engineeringmedia.com\nFin"
  },
  {
    "objectID": "pid_control.html",
    "href": "pid_control.html",
    "title": "PID Control",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport control"
  },
  {
    "objectID": "pid_control.html#introduction",
    "href": "pid_control.html#introduction",
    "title": "PID Control",
    "section": "Introduction",
    "text": "Introduction\n\nThe PID controller is the most common form of feedback.\nStandard tool when process control emerged in the 1940s.\nIn process control today, more than 95% of the control loops are of PID type\n\nMost loops are PI control.\n\nPID control is often combined with logic, sequential functions, selectors, and simple function blocks to build more complicated automation systems\nSophisticated control strategies, such as model predictive control, are also organized hierarchically.\nPID control is used at the lowest level: high level controller gives the setpoints (the reference variables) to the controllers at the lower level."
  },
  {
    "objectID": "pid_control.html#standard-feedback-loop",
    "href": "pid_control.html#standard-feedback-loop",
    "title": "PID Control",
    "section": "Standard Feedback Loop",
    "text": "Standard Feedback Loop\n\n\n\n\n\n\n\n\n\n\nA common way to design a control system is to use PID control.\nPID = Proportional-Integral-Derivative\n\nDescribes how the error term is treated before being sum and sent into the plant\n\nIt is a simple and effective controller in a wide range of applications\nMajority of controllers in industrial applications are PIDs\n\nThe general structure of a PID controller is:\n\n\n\n\n\n\n\n\n\n\nThe three gains \\(K_p, K_i, K_d\\) are adjustable and can be tuned to the specific application\nVarying \\(K_p, K_i, K_d\\) means adjusting how sensitive the system is across the three paths\n\nThe “textbook” version of the PID algorithm is:\n\\[\nu(t) = K\\Big(e(t) + \\frac{1}{T_i}\\int_0^t e(\\tau)d\\tau + T_d\\frac{de(t)}{dt} \\Big)\n\\]\nWill consider each in turn, using an example transfer function\n\\[\nG(s) = \\frac{A}{s^2+a_1s+a_2}\n\\]"
  },
  {
    "objectID": "pid_control.html#proportional-p-control",
    "href": "pid_control.html#proportional-p-control",
    "title": "PID Control",
    "section": "Proportional (P) control",
    "text": "Proportional (P) control\nIn proportional control, the control law is simply a gain \\(K_p\\), so that \\(u\\) is proportional to \\(e\\):\n\n\n\n\n\n\n\n\n\n\\[\nu(t) = K_p e(t)\n\\]\nLet’s consider our system and its characteristic equation:\n\\[ 1 + K_p G(s) = 0 \\]\n\\[ 1 + K_p \\frac{A}{s^2+a_1s+a_2} = 0 \\]\n\\[ \\Rightarrow s^2+a_1s+a_2 + K_pA = 0 \\]\nSince we know:\n\\[\ns^2+2\\zeta\\omega_n s+ \\omega_n^2 = 0\n\\]\n\\[\ns_{1,2} = -\\zeta\\omega_n \\pm \\omega_n\\sqrt{(1-\\zeta^2)}j\n\\]\nThe resulting natural frequency is:\n\\[ w_n = \\sqrt{a_2+K_pA} \\]\n\nIncreasing \\(K_p\\) increases the natural frequency,\nNote that the dumping ratio is reduced (we do not change \\(a_1\\)).\n\nIf we plot the position of the poles in the Root Locus:\n\n\n\n\n\n\n\n\n\nProportial error and output:\n\n\n\n\n\n\n\n\n\n\nOutput is the error scaled by the gain \\(K_p\\)\nWhen error is large, output is large\nWhen error is small, output is small"
  },
  {
    "objectID": "pid_control.html#derivative-d-control",
    "href": "pid_control.html#derivative-d-control",
    "title": "PID Control",
    "section": "Derivative (D) control",
    "text": "Derivative (D) control\nTo add damping to a system, it is often useful to add a derivative term to the control,\n\\[\nu(t) = K_p e(t) + K_d\\dot{e}(t)\n\\]\nor\n\\[\nU(s) = K_p E(s) + K_d sE(s)\n= (K_p+K_ds)E(s)\n= K(s) E(s)\n\\]\nAnd the characteristic equation is:\n\\[\n0 = 1+K(s)G(s)\n\\]\n\\[\n1 + \\frac{(K_p + K_ds) A}{s^2+a_1s+a_2}\n\\]\n\\[\n0 = s^2 + (a_1+K_dA)s + (a_2 + K_p)\n\\]\n\nIn this example, increasing K_d increases the damping ratio without changing the natural frequency.\nFor other \\(G(s)\\) the result might differ!\n\nWhen we keep \\(K_p\\) fixed, and vary \\(K_d\\) the Root Locus changes:\n\n\n\n\n\n\n\n\n\n\nIn the derivative path is the rate of change of the error that contributes to the output of the controller\nThe faster the change the larger the output"
  },
  {
    "objectID": "pid_control.html#integral-i-control",
    "href": "pid_control.html#integral-i-control",
    "title": "PID Control",
    "section": "Integral (I) control",
    "text": "Integral (I) control\nEspecially if the plant is a type 0 system, we may want to add integrator to controller to drive steady-state error to zero:\n\\[\nU(s) = (K_p + \\frac{K_I}{s} + K_d s) E(s)\n\\]\n\nIn the integral path, as the error moves over time the integral continually sum it up (multiplying it by the constant \\(K_I\\))\n\n\n\n\n\n\n\n\n\n\n\nThe integral path is used to remove constant errors\nEven small errors accumulates and add up to adjust the controller output"
  },
  {
    "objectID": "pid_control.html#comments",
    "href": "pid_control.html#comments",
    "title": "PID Control",
    "section": "Comments",
    "text": "Comments\n\nNot all PID paths must be present (set the corresponding constant to zero)\nMaking a simple controller is better when all requirements are met\n\nEasy to implement\nEasy to test, tune and troubleshoot\nEasy to understand by other people\n\nThis is also why PID are so widdespread even if there are a lot of controller types available"
  },
  {
    "objectID": "pid_control.html#diving-a-little-deeper",
    "href": "pid_control.html#diving-a-little-deeper",
    "title": "PID Control",
    "section": "Diving a little deeper",
    "text": "Diving a little deeper\n\nThe PID transfer function:\n\n\\[\nPID(s) = K_p + \\frac{K_I}{s} + K_d s = \\frac{K_ds^2 + K_p s + K_I}{s} = K_p\\Big( 1+\\frac{1}{\\tau_I s} + \\tau_d s \\Big) = K_p \\frac{\\tau_I\\tau_ds^2 + \\tau_Is+1}{\\tau_Is}\n\\]\n\n\\(\\tau_I, \\tau_D\\): integral and derivative time constant (paramenters)\nNote that the PID has one pole at the origina and two zeros in:\n\n\\[\ns_{1,2} = \\frac{-\\tau_I \\pm \\sqrt{\\tau_I(\\tau_I-4\\tau_d)}}{2\\tau_I\\tau_d}\n\\]\n\nThis is an ideal PID;\nA real PID has high frequency poles (high frequency roll off of the frequency response)\n\n\nFiltering and Set Point Weighting\n\nDifferentiation is always sensitive to noise\nThink of \\(G(s)=s\\), whose response goes to infinity for large \\(s\\).\nIn practice, when there is a derivative action we need to limit the high frequency gain.\nThis can be done implementing the derivative term as:\n\n\\[\nD = \\frac{K_d s}{1+s\\hat{K}_d/N}\n\\]\n\nWe are filtering the ideal derivative \\(K_d s\\) with a first order system with time constant \\(\\hat{K}_d/N\\).\nActs as a derivative for low-frequency signal components\nGain is limited (depends on \\(\\frac{K_d}{\\hat{K}_d/N}\\)) and so is high-frequency noise\n\n\n\nComments\n\nPole at the origin ensures steady state performance\nTwo zeros and gain \\(K_p\\) makes it possible to achieve the desired transient behaviour\nUsing a PID makes is possible to use a standard controller, but this also limits the design freedom\n\n\n\n\n\n\n\n\n\n\n\nThe derivative action might lead to large control outputs at high frequency\nA step change in the reference signal will result in an impulse in the control signal\n\\[\nK_d \\frac{d e(t)}{dt} \\rightarrow \\infty\n\\]\nA similar issue is also present in the proportional term \\(K_p e(t)\\) which undergoes a step change in value\nIn process control applications, a step change in the manipulated variable may require sudden and abrupt changes in valve position, process flows, or pressures, all of which can cause significant strain on very large devices.\nThis phenomenon is called setpoint kick (or sometimes derivative kick) which is generally to be avoided.\n\n\n\nSet Point Weighting\n\nSetpoint Kick can be avoided filtering the reference before feeding it to the controller\nor changing the structure of the PID controller:\n\n\\[\nu(t) = K_p\\Big( \\beta r(t) - y(t) + \\frac{1}{T_i} \\int_0^te(\\tau)d\\tau + T_d\\Big(\\gamma\\frac{dr(t)}{dt} - \\frac{dy(t)}{dt} \\Big) \\Big)\n\\]\n\n\\(\\beta\\) and \\(\\gamma\\) are two additional parameters\nThe effect is to introduce a different error term for each term in the control equation\nNote that the integral term must be based on error feedback to ensure the desired steady state\nThe error terms for the proportional and derivative terms now include constants \\(\\beta\\) and \\(\\gamma\\) that are used to modify the response to setpoint changes.\nCommon practice is to set \\(\\gamma=0\\)\nEntirely eliminates derivative action based on change in the setpoint.\nWhen incorporated in PID control, this is also called derivative on output (and \\(e(t)=y(t)\\))\nAlmost always a good idea in process control because it eliminates the ‘derivative kick’ associated with a quick change in setpoint.\n\n\n\n\n\n\n\n\n\n\n\nIn practice, the term \\(\\beta\\) is generally tuned to meet the specific application requirements.\nIf setpoint tracking is not a high priority, setting \\(\\beta=0\\) is a reasonable starting point."
  },
  {
    "objectID": "pid_control.html#integral-windup",
    "href": "pid_control.html#integral-windup",
    "title": "PID Control",
    "section": "Integral Windup",
    "text": "Integral Windup\n\nIf the integral of the error grows too much, the control output might hit actuation limits\nThe integral output produces control outputs even when error is zero.\n\n\ntime = np.arange(0, 10, 0.1)\n\ne = []  # error\nint_e = [] # integral of the error\nint_e.append(0)\n\nfor t in time:\n    e.append(0.3*(-t+5))\n    int_e.append((int_e[-1]+e[-1]))\n\n\nfig, axs = plt.subplots(1, figsize=(10,5))\nplt.plot(time, e, color='b', label='error', linewidth=3)\nplt.plot(time, int_e[:-1], color='r', label='int(e)', linewidth=3)\nplt.plot(time, np.clip(np.array(int_e[:-1]), -10, 10), color='g', label='clip(int(e))', linewidth=3)\nplt.grid()\nplt.legend();\nplt.xlabel('time');\n\n\n\n\n\nSolutions\n\nInitializing the controller integral to a desired value, for instance to the value before the problem\nZeroing the integral value every time the error is equal to, or crosses zero. This avoids having the controller attempt to drive the system to have the same error integral in the opposite direction as was caused by a perturbation\n\n\ntime = np.arange(0, 10, 0.1)\n\ne = []\nint_e = []\nint_e.append(0)\nfor t in time:\n    e.append(0.3*(-t+5))\n    if abs(e[-1]) < 0.1:\n        int_value = 0\n    else:\n        int_value = int_e[-1]+e[-1]\n    int_e.append(int_value)\n\n\nfig, axs = plt.subplots(1, figsize=(10,5))\nplt.plot(time, e, color='b', label='error', linewidth=3)\nplt.plot(time, int_e[:-1], color='r', label='int(e)', linewidth=3)\nplt.plot(time, np.clip(np.array(int_e[:-1]), -10, 10), color='g', label='clip(int(e))', linewidth=3)\nplt.grid()\nplt.legend();\nplt.xlabel('time');"
  },
  {
    "objectID": "pid_control.html#pid-tuning-ziegler-and-nichols",
    "href": "pid_control.html#pid-tuning-ziegler-and-nichols",
    "title": "PID Control",
    "section": "PID Tuning: Ziegler and Nichols",
    "text": "PID Tuning: Ziegler and Nichols\n\n\\(G(s)\\) BIBO stable\n\\(G(0)>0\\)\n\n\n\n\n\n\n\n\n\n\nSteps: - Close the loop with the proportional part only - Apply step input and increase the gain until output starts to oscillate - Record \\(K^*\\) (critical gain) and \\(T_p^*\\) (oscillation period) of the output. - Choose the PID parameters as:\n\n\n\n\\(R(s)\\)\n\\(K_p\\)\n\\(\\tau_I\\)\n\\(\\tau_d\\)\n\n\n\n\n\\(P\\)\n\\(0.5K^*\\)\n\n\n\n\n\\(PI\\)\n\\(0.45K^*\\)\n\\(0.8T_p^*\\)\n\n\n\n\\(PID\\)\n\\(0.6K^*\\)\n\\(0.5T_p^*\\)\n\\(01258T_p^*\\)\n\n\n\n\nThese rules do not squeeze out the best possible performance\n\\(K^*\\) is the gain margin with a proportional controller\nCritical frequency (for which \\(|L(jw)|=1\\) is \\(\\frac{2\\pi}{T_p^*}\\)\nThe rules means having a 6dB gain margin\nIntegral term better steady state but slows down the system (reduces bandwidth and phase margin)\nDerivative term increases the bandwidth and phase margin\nNot possible to use when the plant is potentially dangerous\nPlants that are difficult to bring to oscillate using a proportional controller only (1st and 2nd order with infinite gain margin)\n\n\nZiegler and Nicols in action\n\ns = control.tf([1, 0], [1])\n\nG_s = 6.2/(2*s**3 + 3*s**2 +s + 1)\n\nprint(G_s)\n\n\n         6.2\n---------------------\n2 s^3 + 3 s^2 + s + 1\n\n\n\nLet’s see what happens when we close the loop:\n\\[\nR(s) = 1\n\\]\nMeasurement \\[\nH(s) = 0.042\n\\]\nStep response with no controller:\n\nfig, ax = plt.subplots(1, figsize=(10,5))\nt_out, y_out = control.step_response(control.feedback(G_s, 0.042, sign=-1), T=100)\nplt.plot(t_out, y_out, linewidth=3)\nplt.grid()\nplt.xlabel('time');\n\n\n\n\n\nfig, ax = plt.subplots(1, figsize=(10,5))\n\nK_p_range = [0.1, 0.5, 1, 1.5, 1.92]\nfor K_p in K_p_range:\n    t_out, y_out = control.step_response(control.feedback(K_p*G_s, 0.042, sign=-1), T=100)\n    plt.plot(t_out, y_out, linewidth=3, label='K={}'.format(K_p))\n    plt.grid()\n    plt.xlabel('time');\nplt.legend();\n\nplt.plot(5.0, 15, marker='.', markersize=15, color='r')\nplt.plot(13.95, 15, marker='.', markersize=15, color='r')\n\n\n\n\n\nCritical gain \\(K^*=1.92\\)\nPeriod \\(T_p^*=8.95\\)\n\n\nK_star = 1.92\nT_p_start = 8.95\n\nAnd we can then calculate the parameter of the PID from the ZN table\n\nK_p = 0.6*K_star\ntau_I = 0.5*T_p_start\ntau_d = 0.125*T_p_start\n\nprint('K_p:', K_p)\nprint('tau_I:', tau_I)\nprint('tau_d:', tau_d)\n\nK_p: 1.152\ntau_I: 4.475\ntau_d: 1.11875\n\n\n\nPID = K_p*(tau_I*tau_d*s**2 + tau_I*s + 1)/(tau_I*s)\n\n# Or in the standard form:\n# K_I = K_p/tau_I\n# K_d = K_p/tau_d\n# PID = K_p + K_I/s + K_d*s\n\nprint(PID)\n\n\n5.767 s^2 + 5.155 s + 1.152\n---------------------------\n          4.475 s\n\n\n\n\ncontrol.feedback(PID*G_s, 0.042, sign=-1)\n\n\\[\\frac{35.76 s^2 + 31.96 s + 7.142}{8.95 s^4 + 13.42 s^3 + 5.977 s^2 + 5.817 s + 0.3}\\]\n\n\n\nfig, ax = plt.subplots(1, figsize=(10,5))\nt_out, y_out = control.step_response(control.feedback(PID*G_s, 0.042, sign=-1), T=100)\n\nplt.plot(t_out, y_out, linewidth=3, label='PID')\nplt.grid()\nplt.xlabel('time');\n\n\n\n\n\nPerformance can also be improved manually tuning the parameters further"
  },
  {
    "objectID": "pid_control.html#example",
    "href": "pid_control.html#example",
    "title": "PID Control",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\\[m\\frac{dv}{dt} + \\alpha|v|v+\\beta v = \\gamma u-mgsin(\\theta)\\]\nWe can linearise and calculate the transfer function from gas pedal and car velocity:\n\\[\nG(s) = \\frac{\\gamma}{m}\\frac{1}{s+\\frac{\\beta}{m}}\n\\]\nAnd the disturbance transfer function, between slope of the road (\\(\\theta\\)) and car velocity:\n\\[\nG_d(s) = \\frac{-g}{s+\\frac{\\beta}{m}}\n\\]\n\\(g = 9.8m/s^2\\)\n(see also 02_Intro_to_control_theory)\nClosed loop transfer function:\n\n\n\n\n\n\n\n\n\n\\[\nG(s) = \\frac{1}{10}\\frac{1}{s+\\frac{1}{10}}\n\\]\n\nfrom feedback_control.intro_to_control_theory import LinearCar\n\n\n# We assume we have identified the model parameters\nm = 10\nalpha = 1\nbeta = 1\ngamma = 1\nparams = (m, alpha, beta, gamma)\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = LinearCar(x_0, params)\n\n\n# We define out inputs:\ntheta = np.radians(20) # disturbance\nu = 0                  # Input, constant and set to 0\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 10, 0.1 # time\n\nposition = []\nvelocity = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    car.step(dt, u, theta)\n    x, y, v = car.sensor_i()\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('simulation complete')\n\nsimulation complete\n\n\n\nfig, ax = plt.subplots();\n\nplt.plot(time, velocity, linewidth=3)\nplt.xlabel('time (s)')\nplt.ylabel('speed (m/s)');\n\n\n\n\n\nConstant slope, the car rolls down\nWhat happens when there is no disturbance (road is flat)?\n\nSet theta = np.radians(0) # disturbance in the cell above\n\n\n\nUsing a proportional controller\n\n\n\n\n\n\n\n\n\n\ns = control.tf([1,0], [1])\nG_s = 0.1/(s+0.1)\n\nK = 5\nt_out, y_out = control.step_response(control.feedback(K*G_s, 1))\n\nprint('G(s):', G_s)\nprint('Gcc(s):', control.feedback(K*G_s, 1))\n\nG(s): \n  0.1\n-------\ns + 0.1\n\nGcc(s): \n  0.5\n-------\ns + 0.6\n\n\n\n\n# fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n# control.rlocus(G_s);\n\n\nfig, axs = plt.subplots(1, 1, figsize=(10, 5))\nplt.plot(t_out, y_out, linewidth=3)\nplt.grid()\nplt.title('y(end) = {}'.format(y_out[-1]));\n\nText(0.5, 1.0, 'y(end) = 0.832499999999999')\n\n\n\n\n\nLet’s define a proportional controller in python:\n\ndef proportional(Kp, MV_bar=0):\n    \"\"\"Creates proportional controllers with specified gain (Kp) and setpoint (SP). \n    The output is MV (manipulated variable)\"\"\"\n    MV = MV_bar\n    while True:\n        r, y = yield MV\n        MV = Kp * (r - y)\n\nand use the car now.\nWe will verify how the car responds: - with no disturbance (i.e., the slope of the road) - without disturbance.\nWe will also change the controller \\(K\\) value (e.g., 10, 20)\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = LinearCar(x_0, params)\n\n# We define out inputs:\ntheta = np.radians(0) # disturbance\n\n# Proportional Controller\ncontroller = proportional(Kp=5)\ncontroller.send(None) # initialise\n\n# Desired set point (we would like the vehicle not to move)\ndesired_velocity = 1\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 10, 0.1 # time\n\n# Simulation cycle\nposition = []\nvelocity = []\ntime = []\nfor t in np.arange(t0, tf, dt):\n    \n    u = controller.send((desired_velocity, car.speedometer()[0]))\n    car.step(dt, u, theta)\n    \n    # for plotting reasons\n    x, y, v = car.sensor_i()\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('simulation complete')\n\nsimulation complete\n\n\n\nfig, ax = plt.subplots(1, figsize=(10, 5));\n\nplt.plot(time, velocity, linewidth=3)\nplt.xlabel('time (s)')\nplt.ylabel('speed (m/s)');\nplt.grid()\nplt.title('velocity(end)={}'.format(velocity[-1]));\n\n\n\n\n\nComments?\n\n\n\nUsing a PID controller\nLet’s define a PID controller in python:\n\ndef PID(Kp, Ki, Kd, MV_bar=0):\n    # initialize stored data\n    e_prev = 0\n    t_prev = -100 # we can also set t_prev = -0.05 because in this example we are using dt=0.05\n    y_prev = 0\n    I = 0\n    \n    # initial control\n    MV = MV_bar\n    \n    while True:\n        # yield MV, wait for new t, PV, SP\n        t, y, ref = yield MV\n        \n        # PID calculations\n        e = ref - y\n        \n        P = Kp*e\n        if t_prev >= 0:\n            I = I + Ki*e*(t - t_prev)\n            D = Kd*(e - e_prev)/(t - t_prev)\n        else:\n            I = 0\n            D = 0\n        \n        MV = MV_bar + P + I + D\n        \n        # update stored data for next iteration\n        e_prev = e\n        t_prev = t\n\nAnd simulate the PID control loop applied to the car.\nWe will choose arbitrary parameters \\((Kp=1, Ki=1, Kd=1)\\) for now, and then we will try and tune them.\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = LinearCar(x_0, params)\n\n# We define out inputs:\ntheta = np.radians(0) # disturbance\nu = 0                 # Initial Input\n\ncontroller = PID(1, 1, 1)        # create pid control (Kp, Ki, Kd)\ncontroller.send(None)               # initialize\n\ndesired_velocity = 1\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 150, 0.05 # time\n\nposition = []\nvelocity = []\ntime = []\nfor t in np.arange(t0, tf, dt):    \n    # feedback loop\n    u = controller.send((t, car.speedometer()[0], desired_velocity))\n    car.step(dt, u, theta)\n    \n    # plot \n    x, y, v = car.sensor_i()\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('simulation complete')\n\nsimulation complete\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5));\n\nplt.plot(time, velocity, linewidth=3)\nplt.xlabel('time (s)')\nplt.ylabel('speed (m/s)');\nplt.grid();\n\n\n\n\n\nWhat happens when we add the disturbance?\n\nSet theta = np.radians(20) # disturbance in the code cell to simulate the PID control loop applied to the case"
  },
  {
    "objectID": "pid_control.html#first-order-systems-with-pi-control",
    "href": "pid_control.html#first-order-systems-with-pi-control",
    "title": "PID Control",
    "section": "First order systems with PI control",
    "text": "First order systems with PI control\nGiven the plant:\n\\[\nG_{plant}(s) = \\frac{K}{\\tau s+1}\n\\]\nClosed loop transfer function:\n\\[\nG(s) = \\frac{K\\Big(K_p + \\frac{K_I}{s} \\big)}{\\tau s+1+K\\Big(K_p + \\frac{K_I}{s} \\big)}\n\\]\nwhich we can write as:\n\\[\nG(s) = \\frac{K\\Big(K_p s + K_I\\Big)}{s^2 + (1+K K_p) s + K_I K}\n\\]\nwith natural frequency:\n\\[\nw_n = \\sqrt{\\frac{K_IK}{\\tau}}\n\\]\nand\n\\[\n\\zeta = \\frac{KK_p + 1}{2\\sqrt{K_IK\\tau}}\n\\]\nThe steady state error for a step input of magnitude \\(R\\) is:\n\\[\ne_{ss} = \\lim_{s\\rightarrow0} \\Big[ s\\Big( \\frac{R}{s}-\\frac{R}{s}G(s)\\Big) \\Big ] = 0\n\\]\n\nThe PI control removes the step response steady state error and allows for more control over the transient control, at least when compared with a proportional only, or integral only controller.\nFor ex. it is now possible to reduce the rise time and max overshoot simultaneously.\nGiven:\n\\[\n\\large\nS = 100e^{-\\frac{\\zeta\\pi}{\\sqrt{1-\\xi^2}}}\n\\]\nwe can then solve for \\(\\zeta\\):\n\\[\n  \\Big(\\ln{\\frac{S}{100}}\\Big)^2 = {\\zeta^2\\pi^2} + \\Big(\\ln{\\frac{S}{100}}\\Big)^2 \\zeta^2\n\\]\n\\[\\Downarrow\\]\n\\[\n  \\zeta \\ge \\sqrt{ \\frac{\\Big(\\ln{\\frac{S}{100}}\\Big)^2}{\\Big(\\ln{\\frac{S}{100}}\\Big)^2+\\pi^2}}  \\approx 0.5\n\\]\n\nMaximum desired overshoot: S=15%\n\nS = 15 # 15%\n\n# damping ratio:\nzeta = np.sqrt(np.log(S/100)**2/(3.14**2+np.log(S/100)**2))\nzeta\n\n0.5171229589315239\n\n\nGiven:\n\\[\nt_r \\approx \\frac{1.8}{w_n}\n\\]\nWe can calculate the desired natural frequency for our system:\n\\[\nw_n \\approx \\frac{1.8}{t_r}\n\\]\nRise time equal to \\(t_r=15 s\\)\n\nw_n = 1.8/15\nw_n\n\n0.12000000000000001\n\n\nOur plant is:\n\\[G(s) = \\frac{1}{10s+1}\\]\n\nK = 1\ntau = 10\n\n\nKi = tau*w_n**2/K\nKi\n\n0.14400000000000002\n\n\n\nKp = (-1+2*np.sqrt(Ki*K*tau)*zeta)/K\nKp\n\n0.2410951014356577\n\n\n\nKd = 0 # we do not want to use the derivative path.\n\nWe can verify our performance with the Python Control Library:\n\nPID_s = Kp + Ki/s + Kd*s\n\nprint(PID_s)\n\n\n0.2411 s + 0.144\n----------------\n       s\n\n\n\nThe closed loop transfer function is:\n\ncontrol.feedback(PID_s*G_s, 1)\n\n\\[\\frac{0.02411 s + 0.0144}{s^2 + 0.1241 s + 0.0144}\\]\n\n\nAnd the step response:\n\nt_out, y_out = control.step_response(control.feedback(PID_s*G_s, 1, sign=-1), T=150)\n\n\nfig, ax = plt.subplots(figsize=(10, 5));\n\nplt.plot(t_out, y_out, linewidth=3)\nplt.title('max(y_out)={}'.format(max(y_out)))\nplt.xlabel('time (s)')\nplt.ylabel('output (m/s)');\nplt.grid();\n\n\n\n\nWe can also plot the control command using the associated transfer function\n\nPID_s/(1+PID_s*G_s)\n\n\\[\\frac{0.2411 s^3 + 0.1681 s^2 + 0.0144 s}{s^3 + 0.1241 s^2 + 0.0144 s}\\]\n\n\n\nt_out, y_out = control.step_response(PID_s/(1+PID_s*G_s), T=150)\n\nfig, ax = plt.subplots(figsize=(10, 5));\n\nplt.plot(t_out, y_out, linewidth=3)\nplt.xlabel('time (s)')\nplt.ylabel('control action');\nplt.grid();\n\n\n\n\nAnd finally, let’s apply the calculated controller to our LinearCar\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0)\n\n# We create our car\ncar = LinearCar(x_0, params)\n\n# We define out inputs:\ntheta = np.radians(0) # disturbance\nu = 0                 # Initial Input\n\ncontroller = PID(Kp=Kp, Ki=Ki, Kd=0)        # create pid control (Kp, Ki, Kd)\ncontroller.send(None)               # initialize\n\ndesired_velocity = 1\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 150, 0.01 # time\n\nposition = []\nvelocity = []\ntime = []\ncontrol_action = []\nfor t in np.arange(t0, tf, dt):    \n    # feedback loop\n    u = controller.send((t, car.speedometer()[0], desired_velocity))\n    car.step(dt, u, theta)\n    \n    # plot\n    x, y, v = car.sensor_i()    \n    control_action.append(u)\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('simulation complete')\n\nsimulation complete\n\n\n\nfig, axs = plt.subplots(2, 1, figsize=(10, 10));\n\naxs[0].plot(time, velocity, linewidth=3)\naxs[0].set_title('max(y)={}'.format(max(velocity)))\naxs[0].set_ylim(0, 1.2)\naxs[0].set_xlabel('time (s)')\naxs[0].set_ylabel('speed (m/s)');\naxs[0].grid();\n\n\naxs[1].plot(time, control_action, linewidth=3)\naxs[1].set_xlabel('time (s)')\naxs[1].set_ylabel('control action');\naxs[1].grid();\n\nfig.tight_layout()\n\n\n\n\n\nApplying the PID control to the real Car\n\nWe have calculated a PID controller based on a linear model of the car\nWhat happens when we apply it to the real, non-linear car?\n\n\n\n\n\n\n\n\n\n\n\\[m\\frac{dv}{dt} + \\alpha|v|v+\\beta v = \\gamma u-mgsin(\\theta)\\]\n\nfrom feedback_control.intro_to_control_theory import Car\n\n\nKi = 0.1440000\nKp = 0.2410951014356577\nKd = 0\nPID_s = Kp + Ki/s + Kd*s\nprint(PID_s)\n\n\n0.2411 s + 0.144\n----------------\n       s\n\n\n\n\n# We select the car initial conditions (position and velocity)\nx_0 = (0,0,0)\n\n# We create our car\ncar = Car(x_0, params) # before we had: LinearCar(x_0, params)\n\n# We define out inputs:\ntheta = np.radians(0) # disturbance\nu = 0                 # Initial Input\n\ncontroller = PID(Kp=Kp, Ki=Ki, Kd=0)        # create pid control (Kp, Ki, Kd)\ncontroller.send(None)               # initialize\n\ndesired_velocity = 1\n\n# And finally we define the simulation parameters\nt0, tf, dt = 0, 150, 0.01 # time\n\nposition = []\nvelocity = []\ntime = []\ncontrol_action = []\nfor t in np.arange(t0, tf, dt):  \n    # feedback loop\n    u = controller.send((t, car.speedometer()[0], desired_velocity))    \n    car.step(dt, u, theta)\n    \n    # plot\n    x, y, v = car.sensor_i()    \n    control_action.append(u)\n    position.append((x,y)) \n    velocity.append(v)\n    time.append(t)\n    \nprint('simulation complete')\n\nsimulation complete\n\n\n\nfig, axs = plt.subplots(2, 1, figsize=(10, 10));\n\naxs[0].plot(time, velocity, linewidth=3)\naxs[0].set_title('max(y)={}'.format(max(velocity)))\naxs[0].set_ylim(0, 1.2)\naxs[0].set_xlabel('time (s)')\naxs[0].set_ylabel('speed (m/s)');\naxs[0].grid();\n\n\naxs[1].plot(time, control_action, linewidth=3)\naxs[1].set_xlabel('time (s)')\naxs[1].set_ylabel('control action');\naxs[1].grid();\n\nfig.tight_layout()\n\n\n\n\n\nThe response is different. No overshoot, and no steady state error.\nWhat happens when we add noise? Try set theta = np.radians(20) # disturbance in the simulation cell. You will probably need to set axs[0].set_ylim(-5, 1.2) in the plot cell above.\nLook at the control action needed when a disturbance is present.\n\nfin"
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#final-value-theorem-and-steady-state-error",
    "href": "final_value_theorem_and_steady_state_error.html#final-value-theorem-and-steady-state-error",
    "title": "Final Value Theorem and Steady State Error",
    "section": "Final Value Theorem and Steady State Error",
    "text": "Final Value Theorem and Steady State Error\nBefore we can talk about control systems there is one more concept that we need to understand"
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#what-does-it-mean-to-have-a-final-value",
    "href": "final_value_theorem_and_steady_state_error.html#what-does-it-mean-to-have-a-final-value",
    "title": "Final Value Theorem and Steady State Error",
    "section": "What does it mean to have a final value?",
    "text": "What does it mean to have a final value?\n\n\n\n\n\nIntuitively, if the output converges to a single value then the final value exists, if instead the output oscillates indefinitely or blows up to infinity then talking abut the final value is meaningless."
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#how-do-we-calculate-the-final-value",
    "href": "final_value_theorem_and_steady_state_error.html#how-do-we-calculate-the-final-value",
    "title": "Final Value Theorem and Steady State Error",
    "section": "How do we calculate the final value?",
    "text": "How do we calculate the final value?\n\nTime domain\n\nFinal Value = \\(\\lim_{t\\rightarrow\\infty} f(t)\\)\n\nThis means, for example that given a differential equation: - \\(\\large \\ddot{x}(t) + 4\\dot{x}(t) + 2x(t) + \\delta(0)\\) - We can solve it and calculate \\(\\large \\lim\\limits_{t\\rightarrow\\infty} x(t)\\).\nS-Domain\n\nWe can work directly with the transfer function: \\(\\large F(s) = \\mathcal{L}(f(t))\\) (s-domain representation of differential equations)\nImportantly, working in the S-domain simplifies the math: differential equations in the time domain become algebraic equations in the s-domain:\n\n\\(\\large s^2X(s)+3sX(s) + 2X(s) = 1\\)\n\nWe would like to have a way to calculate the final time value of a function using the s-domain representation of that function\nThis is where the Final Value Theorem comes into play."
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#final-value-theorem",
    "href": "final_value_theorem_and_steady_state_error.html#final-value-theorem",
    "title": "Final Value Theorem and Steady State Error",
    "section": "Final Value Theorem",
    "text": "Final Value Theorem\nIf and only if the linear time invariant system producing \\(x(t)\\) is stable then\n\n\\(\\large \\lim\\limits_{t \\rightarrow \\infty} x(t) = \\lim\\limits_{s \\rightarrow 0} sX(s)\\)\n\n\nWe can use the Laplace transform directly\nMost of the time we already have the Laplace transform\nThe Final Value Theorem does not work on every single transfer function\n\nWe can differentiate four cases in the s-plane: - Right Half Plane (Real > 0) - Imaginary axis (Real = 0) - Left Half Plane (Real < 0) - The Origin\nAnd understand what it means to have a pole in each one of these regions.\n\nRight Half Plane (Real > 0): In this case, the system is unstable: The real component is positive, and \\(e^{+st}\\) goes to infinity.\n\nThe final value of a system with a pole in the RHP does not exist\n\nFor ex. \\[G(s) = \\frac{1}{s-2}\\] FVT: \\[\\lim\\limits_{s \\rightarrow 0} \\frac{s}{s-2}=0\\]\n\nThe FVT produces the wrong value if you use it.\n\nImaginary axis (Real = 0)\n\nWe know that the system will have oscillatory modes (\\(e^{(jwt)}\\) produces sin/cos)\nThe final value is undefined.\n\nFor ex. \\[G(s) = \\frac{1}{s^2+4}\\] FVT: \\[\\lim\\limits_{s \\rightarrow 0} \\frac{s}{s^2+4}=0\\]\n\nThe FVT produces the wrong value if you use it.\n\nLeft Half Plane (Real < 0): In this case, the impulse response of the system is stable and eventually will go to zero.\nFor ex. \\[G(s) = \\frac{1}{s+2}\\] FVT: \\[\\lim\\limits_{s \\rightarrow 0} \\frac{s}{s+2}=0\\]\n\nThe FVT produces the correct value. Note that the value will be zero for every transfer function with poles only in the left half plane.\n\nThe Origin: In this case we are looking at a system like the integrator and the impulse response of an integrator is the integral of the impulse, which is 1.\nFor ex. \\[G(s) = \\frac{1}{s}\\] FVT: \\[\\lim\\limits_{s \\rightarrow 0} \\frac{s}{s}=1\\]\n\nThe FVT produces the correct value.\n\n\nThe number of poles at the origin is called System Type\n\nType 0:\n\nno poles at the origin\nFV = 0 (if all poles are in the LHP)\n\nType 1:\n\none pole at the origin\nthe final value is a real number (if all the other poles are in the LHP)\n\nType 2:\n\ntwo poles at the origin\nthe final value is \\(\\large \\inf\\) (the integral of a step is a ramp)\n\nType 3 and above:\n\nthree or more poles at the origin\nthe final value is \\(\\large \\inf\\) (we are now integrating a ramp, etc.)\n\n\n\n\n\n\n\n\nRecap\n\nWe can use the Final Value Theorem if all poles are in the Left Half Plane or at the origin (aymptotically stable system).\nIf there is even a single pole with \\(\\Re > 0\\), or a pair of complex conjugated poles (on the imaginary axis) then we cannot use the FVT."
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#examples",
    "href": "final_value_theorem_and_steady_state_error.html#examples",
    "title": "Final Value Theorem and Steady State Error",
    "section": "Examples",
    "text": "Examples\nLet’s consider:\n\n\\(\\large G(s)=\\frac{1}{s^2+s}\\)\n\\(\\large u(t) = \\delta(t) \\rightarrow U(s)=1\\)\nThis is type 1 system:\n\n$G(s)= = $\n\n\n\n\n\n\n\nWe can apply the Final Value Theorem:\n\\[ G(0) = \\lim\\limits_{s \\rightarrow 0} s \\frac{1}{s}\\frac{1}{s+1} = 1\\]\nAnd in fact if we plot the impulse response: - \\(\\large y(t) = 1-e^{-t}\\)\nAnd if we plot it:\n\ntime = np.linspace(0, 10, 20)\ny_t = 1 - np.exp(-time)\nplt.plot(time, y_t, color='red');\n\n\n\n\n\nLet’s now consider the same system, but with a different input: - \\(\\large u(t) = 1(t)\\) - a step input - \\(\\large U(s) = \\frac{1}{s}\\)\nWhen we do this, we are adding another pole at the origin:\n\n\n\n\n\n\nThis is now a type 2 system\nWe can apply the Final Value Theorem:\n\n\\[ G(0) = \\lim\\limits_{s \\rightarrow 0} s \\frac{1}{s}\\frac{1}{s^2+s} = \\inf\\]"
  },
  {
    "objectID": "final_value_theorem_and_steady_state_error.html#steady-state-error",
    "href": "final_value_theorem_and_steady_state_error.html#steady-state-error",
    "title": "Final Value Theorem and Steady State Error",
    "section": "Steady state error",
    "text": "Steady state error\nLet’s know see what happens when we apply these concepts to a feedback system:\n\n\n\n\n\n\nWe could apply the Final Value Theorem to find the final value of \\(Y(s)\\)\n\nNote that we need to reduce the system into a single transfer function first.\n\nHowever, if the system is correctly designed we want \\(Y(s) \\rightarrow U(s)\\), so the output follows the input as closely as possible\n\ne.g. We input a ramp, we would expect a ramp at the output.\nIn this case the final value theorem would give us \\(\\large \\infty\\), but regardless what information have we gained?\n\nInstead, we design feedback control systems to drive the error between the reference input and the output to zero.\nThe final value of the error is a much better indicator of the performance of our controller.\nThis is called the Steady State Error and we can use the FVT to obtain it.\n\nFirst, we need to write the transfer function from the input \\(U(s)\\) to the error \\(Err(s)\\). - To emphasise what we are doing, let’s re-write the block diagram:\n\n\n\n\n\nWe know how to write the transfer function for this already: \\[E(s)=U(s)-Y(s)\\] \\[Y(s)=G(s)E(s)\\]\n\\[\\Rightarrow E(s)=U(s)-G(s)E(s) \\rightarrow E(s)+G(s)E(s) = U(s) \\] \\[ E(s)= \\frac{U(s)}{1+G(s)}\\]\nSteady state error\n\\[E_{ss} = \\lim \\limits_{s\\rightarrow0} s\\frac{U(s)}{1+G(s)}\\]\nWe can now figure out what the steady state error is replacing \\(U(s)\\) with the appropriate input we want to study the response of (e.g. 1 (impulse) \\(\\frac{1}{s}\\) (step), \\(\\frac{1}{s^2}\\) (ramp)).\nThe same observations we did before apply: - Depending on the input we are adding poles at the origin, and hence increasing the type of the system. - This means that the system might not be able to follow you inputs perfectly. - This depends on the design specifications and we might need to change the controller - More on this later, for now you might need to modify the system adding a zero at the origin."
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#frequency-response",
    "href": "intro_to_freq_response_and_bode_plots.html#frequency-response",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Frequency response",
    "text": "Frequency response\nGiven an LTI system with transfer function: \\(G(s) = C(sI - A)^{-1}B\\)\nand an input signal: \\(u(t)=Asin(\\omega t)\\)\nAssuming that the system is asymptotically stable, it is possible to verify that:\n\\[y(t)=A|G(j\\omega)|sin(\\omega t + \\angle G(j\\omega) ) \\]\nThis means that the output \\(y(t)\\) converges to a sinusoidal signal that has the same frequency of the input, and that has magnitude \\(A|G(j\\omega)|\\) and is shifted in phase by \\(\\angle G(j\\omega)\\).\nNote also that this is true independently of the initial state \\(x_0\\). In fact, since the system is asymptotically stable, the effect of the initial state on the output will go to zero.\n\n\n\n\n\nLet’s see what this means with our LinearCar:\n\nfrom feedback_control.intro_to_control_theory import *\n\n\n# Let's define its initial conditions\nx_0 = (0,0)\n\n# no slope terrain\ntheta = np.radians(0)\n\n# Define the car parameters\nm = 1\nalpha = 1\nbeta = 1\ngamma = 1\nparams = (m, alpha, beta, gamma)\n\n# Create the car\ncar = LinearCar(x_0, params)\n\n# run it!\nt0, tf, dt = 0, 30, 0.1\ntime = np.arange(t0, tf, dt)\n\nw = 1 # rad/s\nu = 1*np.sin(w*time) # SINUSOIDAL INPUT\n\nposition = []\nvelocity = []\ntime = []\nfor i, t in enumerate(np.arange(t0, tf, dt)):\n    car.step(dt, u[i], theta)\n    x_i, y_i, v = car.sensor_i()\n    position.append((x_i,y_i)), velocity.append(v)\n    time.append(t)\n\nAnd now we plot it:\n\n# First set up the figure, the axis, and the plot element we want to animate\nfig, ax = plt.subplots();\n\nax.set_xlim((min(position)[0], max(position)[0]));\nax.set_ylim((-50, 10));\nline, = ax.plot([], [], lw=4);\n\nx_range = np.linspace(int(position[0][0]), int(position[-1][0]), num=20)\n\n# animation function. This is called sequentially\ndef animate(i):    \n    x_min, x_max = position[max(0,i-2)][0], position[i][0]\n    y_min, y_max = position[max(0,i-2)][1], position[i][1]\n    line.set_data([x_min, x_max], [y_min, y_max])\n    return (line,)\n\n\n# call the animator. blit=True means only re-draw the parts that have changed.\nanim = animation.FuncAnimation(fig, animate,\n                               frames=len(time), interval=40, blit=True,\n                               repeat_delay=10000, repeat=True);\n\nplt.close()\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nAnd if we plot the speed with respect to the input signal:\n\nfig = plt.figure(figsize=(10,5))\n\nplt.plot(u, linewidth=4)\nplt.plot(velocity, linewidth=4)\nfig.legend(['input: u', 'output: v'], loc='upper left')\n\nplt.title('Frequency response')\nplt.xlabel('time (s)')\nplt.grid()\n\n\n\n\n\nComments\nGiven an LTI system with transfer function: \\(G(s) = C(sI - A)^{-1}B\\)\nand an input signal: \\(u(t)=Asin(\\omega t)\\)\nAssuming that the system is asymptotically stable, it is possible to verify that:\n\\[y(t)=A|G(j\\omega)|sin(\\omega t + \\angle G(j\\omega) ) \\]\n\nThe complex function \\(G(j\\omega)\\) is called frequency response (or harmonic response) of the system\n\nIf we know magnitude and phase of \\(G(j\\omega)\\), as \\(\\omega\\) varies, then effectively we know how the system behaves for all sinusoidal inputs with different driving frequencies. - It is possible to experimentally determine \\(G(j\\omega)\\) which means that we can also reconstruct the transfer function - \\(G(j\\omega)\\) can be represented graphically through Magnitude and Phase diagrams (each one is a Real Function of the real variable \\(\\omega\\)): \\[G(j\\omega)=|G(j\\omega)|e^{j\\angle G(j\\omega)}\\] - This makes it possible to analyse the gain and the phase shift across the full frequency spectrum\n\nFinally, as a side observation, note that if we have a sinusoidal input \\(sin(\\omega_0 t)\\), the output can be zero only if \\(G(j\\omega_0)=0\\). This happens if the T.F. \\(G(s)\\) has a pair of imaginary zeros in \\(j\\omega_0\\).\n\nNote: - The time of the transient before the output of the system converges towards $y(t)=A|G(j)|sin(t + G(j) ) $ depends on dynamics of the system and can be evaluated using the settling time \\(t_s\\)."
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#complex-numbers-and-their-representation-through-magnitude-and-phase",
    "href": "intro_to_freq_response_and_bode_plots.html#complex-numbers-and-their-representation-through-magnitude-and-phase",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Complex numbers and their representation through magnitude and phase",
    "text": "Complex numbers and their representation through magnitude and phase\nComplex Numbers are a very popular and frequently used aspect of Mathematics. Composed of a real part and an imaginary part, they are written in the form \\(\\mathit{\\mathbf{x + iy}}\\).\n\n\\(\\mathit{\\mathbf{x}}\\) denotes the real part and \\(\\mathit{\\mathbf{iy}}\\) denotes the imaginary part.\nComplex numbers can be represented on an Argand Diagram.\nAn Argand Diagram is similar to the Cartesian Coordinate System except that the Real axis and Imaginary axis replace the \\(\\mathit{\\mathbf{X}}\\) and \\(\\mathit{\\mathbf{Y}}\\) axis respectively which you would usually expect see on the Cartesian system. For example:\n\n\n\n\n\n\nThe complex number \\(\\mathit{\\mathbf{z = x + iy}}\\) can hence be also represented as: \\[\\large \\mathit{\\mathbf{|z|e^{j\\angle z}}}\\]\n\nThe Modulus of the Complex Number gives the straight line distance from the origin to the point: \\(\\mathit{\\mathbf{|z|=\\sqrt{x^2+y^2}}}\\)\nThe Argument gives the angle between the line representing the complex number and the positive real axis: \\(\\angle z = arctan \\frac{y}{x}\\)\n\n\n\n\n\n\n\nSinusoids can be represented as complex numbers called phasors.\nThe magnitude of the complex number is the amplitude of the sinusoid,\nThe angle of the complex number is the phase angle of the sinusoid.\n\nThus:\n\\[\nM_1 cos(\\omega t + \\phi_1)\n\\]\nCan be represented as:\n\\[\nM_1\\angle \\phi_1\n\\]\nwhere the frequency \\(\\omega\\) is implicit."
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#frequency-response-and-bode-plots",
    "href": "intro_to_freq_response_and_bode_plots.html#frequency-response-and-bode-plots",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Frequency Response and Bode Plots",
    "text": "Frequency Response and Bode Plots\nThe objective is that of analysing the gain and the phase shift across the full frequency spectrum.\nLet’s now suppose that we have one simple system:\n\n\n\n\n\nWhen we have \\(u(t)=\\sin(0.5t)\\), the output of this system is:\n\\[y(t)=2\\sin(0.5t)+\\int \\sin(0.5t)dt = 2\\sin(0.5t) - \\frac{1}{0.5}\\cos(0.5t)\\]\nSince (Trigonometry Identity): > \\(a \\sin x + b \\cos x = \\sqrt{a^2+b^2}\\sin(x+\\phi)\\), where \\(\\phi=\\tan^{-1}\\frac{b}{a}\\)\nWe can then write: \\[y(t)=2\\sin(0.5t)+\\int \\sin(0.5t)dt = 2\\sin(0.5t) - 2\\cos(0.5t) = \\sqrt{8} \\sin(0.5t + \\tan^{-1}(-1)) \\approx 2.83 \\sin(0.5t-0.785)\\]\nThis gives the output for one frequency \\(\\omega\\).\nMore often though we are going to be interested in a range of frenquecies, and to represent the gain and phase shift across the frequency spectrum we can use Bode Plots - They plot frequency response information to be displayed graphically on two diagrams: gain and phase (plotted against frequency on a logarithmic horizontal axis)\nNote - To plot the gain on a Bode diagram we do not plot the gain directly - We first convert the gain into Power, and then we represent the gain in dB (a logarithmic unit) - To convert the amplitude to decibel: \\(20\\log_{10}\\frac{A}{A_0}\\) - This comes from quantifying loss in audio levels through telephone circuits - units of measurement that could cover a large range of audio power differences. Since the ear responds to sound pressure logarithmically using a log scale corresponds to the way humans perceive sound. - \\(1 \\text{TU} = 10\\log_{10} \\Delta \\text{Power Loss}\\) - \\(1 \\text{TU}\\) was the smallest power attenuation detectable by the average listener - \\(1 \\text{TU}\\) was later renamed after Alexander Bell as decibel (1/10 bel): \\(1 TU = \\frac{1}{10}\\text{bel} = 1 \\text{dB}\\) - Since \\(\\text{Power} \\propto \\text{Amplitude}^2\\) we can write the equation with respect to amplitude: - \\(1 \\text{dB} = 10\\log_{10} \\text{Amplitude}^2 = 20\\log_{10} \\text{Amplitude}\\) (properties of logarithms)\nIn our case: - \\(y(t) = 2.83 \\sin(0.5t-0.785)\\) - Gain: \\(20 \\log_{10}(2.83) \\approx 9 \\text{dB}\\) - Phase: \\(-0.785 \\text{rad} = -45 \\text{deg}\\)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5));\n\naxs[0].plot(0.5, 20*np.log10(2.83), marker='.', markersize=15)\naxs[0].set_xlabel('Frequency (rad/s)')\naxs[0].set_ylabel('Gain (dB)');\naxs[0].grid()\n\naxs[1].plot(0.5, np.degrees(-0.785), marker='.', markersize=15)\naxs[1].set_xlabel('Frequency (rad/s)')\naxs[1].set_ylabel('Phase (deg)');\naxs[1].grid()\n\n\n\n\n\nThis is the frequency response for a single sine wave.\nWe are interested in plotting the gain and phase across the entire frequency spectrum\n\nWe can manually calculate the frequency response for all possible values of \\(\\omega\\):\n\\[\ny_t = \\sqrt{2^2+\\frac{1}{w}^2} sin(wt + \\tan^{-1} \\big(\\frac{-1/w}{2}\\big) )\n\\]\nAnd we can plot for a finite number of values of \\(\\omega\\)\n\nfig, axs = plt.subplots(1, 2, figsize=(20,5));\n\nws = np.logspace(-3, 3, 100) # logspace sets the exponents\n\ngain, phase = [], []\nfor w in ws:\n    # y_t = np.sqrt(2**2+1/w**2) * sin(w*t + np.atan2(-1/w, 2))\n    gain.append(np.sqrt(2**2 + (1/w)**2))\n    phase.append(np.arctan2(-1/w, 2))\n    \n# Gain diagram\naxs[0].plot(ws, 20*np.log10(gain), linewidth=3)\naxs[0].plot(0.5, 20*np.log10(2.83), marker='.', markersize=15, color='red')    \naxs[0].set_xscale('log')\naxs[0].grid('log')\naxs[0].set_xlabel('Frequency (rad/s)')\naxs[0].set_ylabel('Gain (dB)');\naxs[0].text(10**3, 20*np.log10(2), '6dB')\n\n# Phase diagram\naxs[1].plot(ws, np.degrees(phase), linewidth=3)\naxs[1].plot(0.5, np.degrees(-0.785), marker='.', markersize=15, color='red')    \naxs[1].set_xscale('log')\naxs[1].grid('log')\naxs[1].set_xlabel('Frequency (rad/s)')\naxs[1].set_ylabel('Phase (deg)');\n\n\n\n\n\nFor this simple example, solving the response to the sine wave input is quite straightforward. But what about a more general case?\nWe can obtain the result using directly the transfer function.\n\nThe system we had was:\n\n\n\n\n\nThis can be written as:\n\\[G(s)= \\bigg (2 + \\frac{1}{s} \\bigg ) = \\frac{2s+1}{s}\\]\nwhich means:\n\\[y(t) = \\frac{2s+1}{s} u(t) \\]\nRecall that: \\(s=\\sigma + j\\omega\\) (complex variable)\n\nWhen we calculate the frequency response we are interested in the state state response, once all the transients have died out (or in other words, when \\(\\sigma=0\\)).\nSo for steady state: \\(s=j\\omega\\)\n\nWe can then calculate steady state gains and phases directly substituting in the transfer functions \\(s=j\\omega\\):\n\\[\\frac{2s+1}{s} \\big |_{s=j\\omega} \\rightarrow \\frac{2j\\omega+1}{j\\omega} = 2-\\frac{1}{\\omega}j\\]\nWe can then plot these real and imaginary components on the Real-Imaginary axis, and we can calculate gain and phase geometrically from these plots.\nAnd for \\(\\omega=0.5\\)\n\nfig = plt.figure()\n\nw = 0.5 # THIS IS OUR SPECIFIC FREQUENCY OF INTEREST\n\ndef plot_imre(w, display_text=1):\n    plt.plot([-3, 6], [0, 0], color='black', linestyle='--') # this plots the real axis\n    plt.plot([0, 2], [0, -1/w], color='red')                 # THIS PLOTS OUR VECTOR\n    plt.plot(0, 0, marker='.', markersize=12, color='black'); # Make it prettier: plot the black dot\n    plt.plot(2, -1/w, marker='.', markersize=12, color='red'); # Make it prettier: plot the red dot\n    if display_text: plt.text(2+0.1, -1/w+0.1, '2-{:.1f}j'.format(1/w), size=16) # add a text\n\n\nplot_imre(w)\nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.axis([-3, 3, -3, 1])\nplt.grid()\n\n\n\n\n\nThe length of the line is the Gain of the system: \\(l=\\sqrt{real^2+imag^2}\\)\nThe angle between the line and the positive real line is the Phase: \\(\\angle=\\text{atan2(imag, real)}\\)\n\nAnd if we speed across the frequencies, from \\(0\\) to \\(\\inf\\), we can obtain a visual understanding of what the gain and phase are doing for this system: - \\(w \\rightarrow 0\\), \\(\\Rightarrow\\), \\(imag \\rightarrow \\inf\\) - \\(w \\rightarrow \\inf\\), \\(\\Rightarrow\\), \\(imag \\rightarrow 0\\)\nAnd we can plot a few examples:\n\nfig = plt.figure()\nws = np.logspace(0.001, 10, 100)\n\nws = [0.01, 0.1, 1]\nfor w in ws:\n    plot_imre(w)\n    \nplt.xlabel('Real')\nplt.ylabel('Imaginary')\nplt.axis([-3, 6, -120, 10])\nplt.grid()\n\n# add a note on the plot to show the direction of w\nplt.arrow(5, -40, 0, 30, head_width=0.4, head_length=10, length_includes_head=True, color='k', linestyle=':')\nplt.arrow(5, -70, 0, -30, head_width=0.4, head_length=10, length_includes_head=True, color='k', linestyle=':')\nplt.text(4.5, -8, '$Imag\\; to\\; 0$ for $\\omega=\\infty$', fontsize=14)\nplt.text(4.5, -110, '$Imag\\; to\\; \\infty$ for $\\omega=0$', fontsize=14);\n\n\n\n\nThe real part never changes.\nFor this system:\nGain - As \\(\\omega\\) decreases, the Gain increases (\\(\\inf\\) for \\(\\omega=0\\)) - As \\(\\omega\\) increases, the Gain decreases (2 for \\(\\omega=\\inf\\))\nPhase\n\nAs \\(\\omega\\) decreases, the Phase increases (\\(-90^o\\) for \\(\\omega=0\\))\nAs \\(\\omega\\) increases, the Phase decreases (0 for \\(\\omega=\\inf\\))\n\n\n\nLet’s be more formal\n\nThe transfer functions of linear systems are polynomial fractions in \\(s\\)\nIt is always possible to factor polynomials in terms of their roots (zeros and poles)\nEach polynomial is the product of first-order or second-order (potentially with multiplicity \\(>\\) 1)\nTo build the frequency response of the system is useful to have simple rules to represent each term\nWe can leverage the properties of the logarithms: the logarithm of a product is equal to a sum of logarithms:\n\n$ (AB) = (A)+(B) $\n$ (A/B) = (A)-(B)$\n$ (y^x) = x(y)$\n\n\n\n\nMagnitude and Phase\n\\[\\large G(s) = \\frac{(s-z_1)(s-z_2)...}{(s-p_1)(s-p_2)...}\\]\n\\[\\Downarrow\\]\n\\[\\large G(jw) = \\frac{|j\\omega-z_1|e^{j\\angle j\\omega -z_1} |j\\omega-z_2|e^{j\\angle j\\omega -z_2} ...}{|j\\omega-p_1|e^{j\\angle j\\omega -p_1} |j\\omega-p_2|e^{j\\angle j\\omega -p_2} ...} \\]\n\\[\\Downarrow\\]\n\\[\\large G(jw) = \\frac{|j\\omega-z_1||j\\omega-z_2|...e^{j(\\angle j\\omega -z_1 + \\angle j\\omega -z_2 + ...)}}{|j\\omega-p_1||j\\omega-p_2|...e^{j(\\angle j\\omega -p_1 + \\angle j\\omega -p_2 + ...)}} \\]\nand:\n\\[\\large |G(jw)| = \\frac{|j\\omega-z_1||j\\omega-z_2|...}{|j\\omega-p_1||j\\omega-p_2|...}\\]\n\\[\\large \\angle  G(jw)| = +\\angle (j\\omega -z_1)+\\angle (j\\omega -z_2)+ ... -\\angle (j\\omega -p_1)-\\angle (j\\omega -p_2) - ...\\]\n\n\n\nBode Amplitude Plot\n\nIn the amplitude plot, it is convenient to report the value of the magnitude in decibel or dB.\nBy convention, the dB value of a positive quantity \\(x\\) is: \\(20\\log_{10}(x)\\).\nThis means: \\(|G(j\\omega)|_{dB}=20log|G(j\\omega)|\\)\nNote that when:\n\n\\(|G(j\\omega)|_{dB} > 0 \\rightarrow |G(j\\omega)| > 1\\)\n\\(|G(j\\omega)|_{dB} < 0 \\rightarrow |G(j\\omega)| < 1\\)\n\\(|G(j\\omega)|_{dB} = 0 \\rightarrow |G(j\\omega)| = 1\\)\n\n\nWhen we use dB things become simpler:\n\\(\\large |G(jw)| = \\frac{|j\\omega-z_1||j\\omega-z_2|...}{|j\\omega-p_1||j\\omega-p_2|...}\\) \\(\\Rightarrow\\)\n\\[\\large |G(j\\omega)|_{dB}=20\\log|G(j\\omega)| = 20\\log(|j\\omega-z_1|) + 20\\log(|j\\omega-z_2|) + ... \\\\\n               \\large - 20\\log(|j\\omega-p_1|) - 20\\log(|j\\omega-p_2|) ...\\]\n\nIt is convenient to use a logarithmic scale for the \\(w\\)-axis.\nLogarithmic scales are useful when plotting functions that vary over many orders of magnitude.\nOne term we will use in our discussion of frequency response plots is “decade”. A decade change in frequency is a factor of ten. So, for example, 1 kHz is a decade above 100 Hz and a decade below 10 kHz.\n\n\n\nSketching Bode Plots\n\nBeing able to quickly draw Bode plots is important\n\nThey provide an intuitive understanding how poles and zeros affect the frequency response\nMake it possible to estimate the transfer function looking at the frequency response (e.g. estimate of the transfer function from the output of a frequency sweep that we are providing to the system)\n\n\nRecall\n\nGiven a transfer function \\(G(s)\\) we can obtain the (steady state) frequency response setting \\(s=j\\omega \\rightarrow G(j\\omega)\\)\nRemember also that $G(j) = + j $\nIf we plot it on the Real-Imag diagram:\n\n\n\n\n\n\n\nGain: \\(\\sqrt{\\text{real}^2 + \\text{img}^2} = |G(j\\omega)|\\)\nPhase: \\(atan2(\\text{img}, \\text{real}) = arg(G(j\\omega)) = \\angle G(j\\omega)\\)\nThe sign of the phase is determined based on which side of the real line it appears"
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#representing-the-simplest-transfer-function-on-a-bode-plot",
    "href": "intro_to_freq_response_and_bode_plots.html#representing-the-simplest-transfer-function-on-a-bode-plot",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Representing the simplest transfer function on a Bode Plot",
    "text": "Representing the simplest transfer function on a Bode Plot\n\n\\(G(s) = K\\)\nwhere \\(K \\in R\\) can be positive of negative\n\nWe apply the definition - Gain = \\(|G(j\\omega)| = |K| = \\text{positive K}\\) - Phase = \\(\\text{atan2}(img, real)= \\text{atan2}(0, K)\\)\n- The img part is zero in our case\nIf we plot this on the real-img axis, all values would lie on the real axis (imaginary component is zero)\n\n\n\n\n\n\nThe gain is always \\(K\\) (distance from the origin)\n\\(K\\) can be positive or negative,\n\nFor \\(K > 0\\), Phase = \\(0\\),\nFor \\(K < 0\\), Phase = \\(180^o=-180^o\\), but traditionally use \\(-180^o\\)\n\nif we use \\(\\text{atan2}\\) we already keep track of the sign.\n\n\n\n\nRepresent on a Bode Plot\nGain - We need to remember to convert the gain to dB: \\(|H(j\\omega)| = 20\\log_{10}|K|\\) - Then it is just a constant for all frequencies, so it is a straight line\nPhase - Constant for all frequencies - \\(K>0\\), Phase = \\(0^o\\) - \\(K<0\\), Phase = \\(-180^o\\)\n\n\n\n\n\nWhat is happening in time?\nThe system that we are considering is:\n\n\n\n\n\n\nInput is \\(\\sin(\\omega t)\\)\nOutput is \\(K\\sin(\\omega t)\\), which is a sinusoid with the same frequency, but scaled by \\(K\\) (and shifted in phase if \\(K<0\\) by -180 deg)."
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#bode-plot-of-the-integrator",
    "href": "intro_to_freq_response_and_bode_plots.html#bode-plot-of-the-integrator",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Bode Plot of the Integrator",
    "text": "Bode Plot of the Integrator\n\nConsider a system that has a single pole at the origin.\nThe steady-state frequency-response is determined by letting \\(s \\rightarrow j\\omega\\):\n\n\\[G(s) = \\frac{1}{s} \\Rightarrow G(j\\omega)=\\frac{1}{j\\omega} = -\\frac{1}{w}j\\]\n\n\n\n\n\n\nSide note \\[\nj=\\sqrt{-1}\\] Therefore: \\[\n\\frac{1}{j} = \\frac{1}{\\sqrt{-1}}\\frac{\\sqrt{-1}}{\\sqrt{-1}} = \\frac{\\sqrt{-1}}{-1} = -\\sqrt{-1}=-j\n\\]\n\n\n\nGiven \\(G(j\\omega)= -\\frac{1}{w}j\\)\n\nReal component is 0\nImaginary component is \\(-\\frac{1}{w}\\)\nGain = \\(|G(j\\omega)| = \\big | -\\frac{1}{w}j \\big | = \\frac{1}{\\omega}\\):\nPhase = \\(\\arg G(j\\omega) = \\arg \\big(-\\frac{1}{w}, 0 \\big) = -90^o\\)\n\nWe can see this graphically on the real-img axis:\n\n\n\n\n\nAnother way to look at this is through a block diagram\n\n\n\n\n\n\nAnd if we plot it, we see that the resulting signal has a -90 deg phase shift:\n\n\nfig = plt.figure(figsize=(10,5))\n\nt = np.linspace(0, 30, 50)\nw = 0.5\n\nplt.plot(t, np.sin(w*t), color = 'green', label='sin(wt)', linewidth=4)\nplt.plot(t, -1/w*np.cos(w*t), color = 'blue', label='-1/w cos(wt)', linewidth=4)\n\nrc('font', **{'size':22})\nplt.grid()\nplt.legend(loc=\"lower right\", bbox_to_anchor=(1.4,0.7));\n\n\n\n\nLet’s se what the Bode plot looks like:\n\nGain = \\(\\large |G(j\\omega)| = \\big | -\\frac{1}{w}j \\big | = \\frac{1}{\\omega}\\):\n\nwhen \\(\\omega = 1 \\; \\text{rad/s}\\), then gain = 1, \\(\\rightarrow 20\\log_{10}1=0 \\text{dB}\\)\nwhen \\(\\omega = 10 \\; \\text{rad/s}\\), then gain = 1/10, \\(\\rightarrow 20\\log_{10}1=-20 \\text{dB}\\)\n\\(|G(j\\omega)|_{dB}=20\\log\\big|\\frac{1}{j\\omega}\\big| = -20\\log(\\omega)\\)\nGiven that we are using a logarithmic scale on both axis, this corresponds to a straight line with a -20dB slope every “decade”.\n\nPhase = \\(\\arg \\big(-\\frac{1}{w}, 0 \\big) = -90^o\\) at all frequencies\n\nWe can plot it\n\n\n\n\n\nWe can also use the Python Control Systems Library for this.\n\nThe Python Control Systems Library is a Python module that implements basic operations for analysis and design of feedback control systems.\n\n\nimport control\n\n\nfig = plt.figure(figsize=(15,10))\n\n# integrator: 1/s = 1/(s+0)\nnum = [1]\nden = [1, 0]\n\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.1, 100));\n\n\n\n\n\nWhat about a zero?\n\n\\(G(s)=s\\)\nIt is easy to calculate this from what we know already.\nFor example we can use the property of logarithms to say: \\(G(s)=s = \\frac{1}{1/s} \\Rightarrow 1 - \\frac{1}{s}\\)\nSo the gain of a zero is simply the negative of the gain of a pole (reflection about the real axis)\nAnd the phase of a zero is +90 deg"
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#frequency-response-of-simple-poles-and-zeros",
    "href": "intro_to_freq_response_and_bode_plots.html#frequency-response-of-simple-poles-and-zeros",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Frequency Response of Simple Poles and Zeros",
    "text": "Frequency Response of Simple Poles and Zeros\n\nTransfer function with a single real pole\n\nGiven a transfer function G(S), re-write the transfer function in the Bode form: \\[\n\\frac{1}{1+\\frac{s}{w_0}}\n\\]\n\nex.\n\n\\(\\large G(s)=\\frac{1}{s+1} \\Rightarrow \\frac{1}{1+\\frac{s}{w_0}}\\)\nwe call \\(w_0\\) “break frequency” or “corner frequency”\n\nSometime you also see it written as:\n\n\\(\\large G(s)=\\frac{w_0}{s+w_0} = \\frac{1}{1+\\tau s}\\)\nwhere \\(\\tau = \\frac{1}{w_0}\\) is called Time constant\n\nAll these representations are equivalent\n\nTo be clear:\n\nA real pole has an effect on the frequency response\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, given a transfer function with a single real pole:\n\\[\n\\large G(s)= \\frac{1}{1+\\frac{s}{w_0}} \\hspace{0.2cm}  \\large \\xrightarrow{Set\\; s=j\\omega} \\hspace{0.2cm} \\large\\frac{1}{1+\\frac{j\\omega}{w_0}}\n\\]\nWe are trying to separate the real part and the imaginary part:\n\\[\\large\\frac{1}{1+\\frac{j\\omega}{w_0}} = \\frac{1}{1+j\\frac{\\omega}{w_0}} \\frac{1-j\\frac{\\omega}{w_0}}{1-j\\frac{\\omega}{w_0}} = \\frac{1-j\\frac{\\omega}{w_0}}{1+\\frac{\\omega^2}{w_0^2}}\\]\nNote: \\(1-j\\frac{\\omega}{w_0}\\) is the complex conjugate of \\(1+j\\frac{\\omega}{w_0}\\)\n\nReal part = \\(\\large \\frac{1}{1+\\frac{\\omega^2}{\\omega_0^2}}\\)\nImaginary part = \\(\\large \\frac{-\\frac{\\omega}{\\omega_0}}{1+\\frac{\\omega^2}{\\omega_0^2}}\\)\n\nAnd now we can solve for the gain and phase as we did before:\n\nGain: \\[ 20\\log_{10}\\big | H(j\\omega) \\big | = \\sqrt{ real^2 + img^2 } = \\sqrt{ \\frac{1}{1+\\frac{\\omega^2}{\\omega_0^2}} } \\xrightarrow{in \\; \\text{dB}} -20\\log_{10}\\bigg( \\sqrt{1+\\frac{\\omega^2}{\\omega_0^2}} \\bigg)\\]\nPhase: \\[\\arctan\\frac{\\text{img}}{\\text{real}}=\\arctan\\frac{-w}{w_0}\\]\n\nAnd now we can plot them:\n\ncase 1: \\(w \\ll w_0 \\rightarrow \\frac{\\omega^2}{\\omega_0^2} \\rightarrow 0\\),\n\nGain: \\(-20\\log_{10} 1 = 0\\; \\text{dB}\\)\nPhase: \\(\\arctan(0) = 0\\; \\text{degrees}\\)\n\n\n\n\n\n\n\n\ncase 2: \\(w = w_0 \\rightarrow \\frac{\\omega^2}{\\omega_0^2} = 1\\),\n\nGain: \\(-20\\log_{10}\\sqrt{2} = -3 \\; \\text{dB}\\)\nPhase: \\(\\arctan(-1) = -45\\; \\text{degrees}\\)\n\n\n\n\n\n\n\n\ncase 3: \\(w \\gg w_0\\),\n\nThe term \\(\\frac{\\omega^2}{\\omega_o^2}\\) dominates the gain equation\nGain: \\(-20\\log_{10}\\frac{\\omega}{\\omega_o}\\)\n\nEvery time \\(\\omega\\) gets 10 times bigger, there is a drop in gain of -20 dB\n\\(-20 \\; \\text{dB}/\\text{decade}\\) slope and intercepts 0 at \\(\\omega_0\\)\n\nPhase: \\(\\arctan(-\\frac{\\omega}{\\omega_o})=\\arctan(-\\inf) = -90\\; \\text{degrees}\\)\n\n\n\n\n\n\n\n\nThis is called the asymptotic Bode plot, which is an approximation of the actual plot.\n\nWhat does the actual Bode plot look like?\n\n\n\n\n\nAnd for this reason, sometimes we use a different approximation of the phase:\n\n\n\n\n\n\n\nTransfer function with a single real zero\n\n\\(G(s) = s + z\\)\nIt is easy from what we know already\nOne way to do it is to reshape it as: \\[G(s) = s + z = \\frac{1}{\\frac{1}{s+z}} = \\frac{1}{\\frac{1}{1+\\frac{s}{w_0}}} = 1 + \\frac{s}{w_0}\\]\nAnd now we can simply use the property of the logarithm:\n\nreal zero is - real pole when plotted on a log-log Bode plot.\n\nGain slops up at 20 dB/decade (at \\(w_0\\))\nPhase goes from 0 to 90 degrees.\n\n\n\nSummary\n\nWrite the transfer function as: \\(\\frac{1}{1+\\frac{s}{w_0}}\\) (pole) or as \\(1 + \\frac{s}{w_0}\\) (zero)\nFind the break frequency \\(w_0\\) on the log-log Bode plot\nGain: find and draw the two asymptotes (0 dB until \\(w_0\\), then -20dB/decade (pole) or 20dB/decade (zero))\nPhase: draw the two asymptotes (0 until \\(w_0\\), then -90 deg (pole), 90 deg (zero)\n\nOptional: use the better approximation\n\n\nOne more comment:\n\nIf you have a \\(G(s)=\\frac{1}{s+4}\\) you can always refactor it to the form we would like: \\[G(s)=\\frac{1}{s+4} = \\frac{1}{4}\\frac{1}{1+\\frac{s}{4}}\\]\n\\(\\frac{1}{4}\\) is the gain, and the break frequency \\(w_0=4\\) rad/s\n\n\n\n\nTransfer function with a pair of complex poles / zeros\nLet’s now consider a transfer function of the form:\n\\[H(s) = \\frac{\\omega_o^2}{s^2+2\\xi\\omega_o s+\\omega_o^2}\\]\nThe roots of the denominator (the poles) are:\n\\[s=-\\xi\\omega_o \\pm \\omega_o\\sqrt{\\xi^2-1}\\]\n\n\\(w_o\\) is the break frequency\n\\(\\xi\\) dumping ratio\nif \\(\\xi > 1\\), the second-order response has two simple real poles and we know how to deal with that already.\nif \\(0<\\xi < 1\\), we have stable complex-conjugatee pole pairs.\n\nFirst, like we did before, we want to manipulate the form to bring it to the Bode form:\n\\[H(s) = \\frac{1}{\\frac{s^2}{\\omega_o^2}+2\\xi\\frac{\\omega_o}{\\omega_o^2} s+ \\frac{\\omega_o^2}{\\omega_o^2}} = \\frac{1}{\\frac{s^2}{\\omega_o^2}+2\\frac{\\xi}{\\omega_o} s+ 1} \\]\nSecond, we substitute \\(s=j\\omega\\) and solve for the real and img parts:\n\\[H(j\\omega) = \\frac{1}{-\\frac{\\omega^2}{\\omega_o^2}+2j\\xi\\frac{\\omega}{\\omega_o} + 1}\\]\n\nReal = \\(\\frac{ 1 - \\big(\\frac{w}{w_0}\\big)^2}{ \\bigg[1 - \\big(\\frac{w}{w_0}\\big)^2 \\bigg]^2 + \\bigg[2\\xi \\frac{w}{w_0} \\bigg]^2}\\)\nImaginary = \\(\\frac{- 2\\xi \\frac{w}{w_0}}{ \\bigg[1 - \\big(\\frac{w}{w_0}\\big)^2 \\bigg]^2 + \\bigg[2\\xi \\frac{w}{w_0} \\bigg]^2}j\\)\n\nWe can use these to estimate the Gain and Phase.\n\nCase 1: \\(w \\ll w_0\\),\n\n\\(\\frac{w}{w_0}\\) is very small \\(\\Rightarrow\\) Real = 1, Img = 0\n\\(|H(j\\omega)|_{dB} = 20\\log(1) = 0\\) dB\n\\(\\arg H(j\\omega) = \\arctan\\frac{0}{1} = 0\\) deg\n\n\n\n\n\n\n\n\nCase 2: \\(w \\gg w_0\\),\n\n\\(1 - \\big( \\frac{w}{w_0} \\big)^2 \\approx - \\big( \\frac{w}{w_0} \\big)^2\\) \\(\\Rightarrow\\) Real = \\(-\\big( \\frac{w}{w_0} \\big)^{-2}\\), Img = \\(-2\\xi \\big( \\frac{w}{w_0} \\big)^{-3}\\)\n\\(|H(j\\omega)|_{dB} = 20\\log\\big( \\frac{w}{w_0} \\big)^{-2} = -40\\log(\\frac{w}{w_0})\\) dB\n\nNote that the real component dominates (there is a square) over the imaginary component\n-40 dB/decade drop in gain\n\n\\(\\arg H(j\\omega) = \\arctan2 \\big( -2\\xi \\big( \\frac{w}{w_0} \\big)^{-3}, -\\big( \\frac{w}{w_0} \\big)^{-2} \\big) = -180\\) deg\n\nThis is because both real and imaginary components are negative and the imaginary component goes to zero faster than the real component.\n\n\n\n\n\n\n\n\n-Case 3: \\(w = w_0\\), - \\(\\frac{w}{w_0} = 1\\), Real = 0, Img = \\(-\\frac{1}{2\\xi}\\) - \\(|H(j\\omega)|_{dB} = 20\\log_{10}(\\frac{1}{2\\xi})\\) dB = \\(-20\\log_{10}(2\\xi)\\) dB - Strongly depends on the parameter \\(\\xi\\), the damping ratio. - When \\(\\xi=0.5 \\rightarrow -20\\log_{10}(1)=0\\)dB (no peak) - When \\(\\xi > 0.5\\) then the peak goes slightly down at \\(w_0\\) - When \\(\\xi < 0.5\\) peak of magnitude \\(-20\\log_{10}(2\\xi)\\) - When \\(\\xi = 0\\) (no dumping) peak goes to \\(\\inf\\) - -40 dB/decade drop in gain - \\(\\arg H(j\\omega) = \\arctan(\\inf) = -90\\) deg (remember the quadrant) - The slope of the line however changes as a function of \\(\\xi\\)\nIn case \\(\\xi = 0.3\\):\n\n\n\n\n\nHow does the actual plot look like?\n\n\n\n\n\nRemember: - This is still an approximation!\nTo do a better approximation of the phase, sometime we identify \\(w_0^- = \\frac{w_0}{5^\\xi}, w_0^+ = w_05^\\xi\\) and we connect them with a straight line.\nHere is a more accurate representation of the Bode plots for a pair of complex poles and how their response changes with \\(\\xi\\).\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer function with a pair of complex zeros - The frequency response of complex zeros can be obtained from what we know already - Reflection of the response of a pole about the horizontal axis.\n\n\nFinal comments\n\nDrawing Body plots is easy once we understand the key rules\nMakes it possible to understand how adding poles/zeros changes the frequency response of the system\nReally useful when desigining controllers or filters\n\n\n\n\nOK, really the final comment - Bode plot of a delay\nThe transfer function of a delay is \\(G(s)=e^{-s\\tau}\\).\nThe frequency response \\(G(j\\omega)=1 e^{-j\\omega\\tau}\\) is already a phasor!\n\\[|e^{-j\\omega\\tau}| = 1\\] \\[\\angle e^{-j\\omega\\tau} = -\\omega \\tau\\]\n\nA delay \\(\\tau\\) introduces a phase shift proportional to the delay and to the frequency."
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#exercises",
    "href": "intro_to_freq_response_and_bode_plots.html#exercises",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nDraw the Bode plot of the system:\n\\[\nG(s) = \\frac{10^3(s+0.1)}{s(s+1)^2}\n\\]\n\nzeros: \\((s+0.1)=0 \\Rightarrow s=-0.1\\)\npoles: \\(s(s+1)^2=0 \\Rightarrow s=0, s=-1\\) (with multiplicity 2)\n\nWrite the frequency response, setting \\(s=j\\omega\\) as:\n\\[\nG(j \\omega) = \\frac{10^3 0.1 \\big (1+\\frac{j\\omega}{0.1} \\big )}{j \\omega( 1 + j \\omega )^2}\n\\]\n\nGiven that we have a pole at the origin we know that the plot starts with a slope of \\(-20 \\text{dB/decade}\\), and passing through \\(w=1 \\text{rad/s}\\)\nConstant Gain: \\(K=0.1 \\cdot 10^3 \\Rightarrow 20\\log_{10}(10^2)=40\\)\n\n\n\n\n\n\n\n\n\n\nZero in \\(s=0.1\\)\n\n\n\n\n\n\n\n\n\n\nPoles in \\(s=-1\\) with multiplicity 2\n\n\n\n\n\n\n\n\n\n\nSumming all contribution together:\n\n\n\n\n\n\n\n\n\n\nAnd we can refine our plot a bit:\n\n\n\n\n\n\n\n\n\nAnd finally we can ask the Python Control library to plot it for us:\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [10**3, 10**3*0.1]\nden = [1, 2, 1, 0]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.001, 1000));\n\n\n\n\n\n\nExercise 2\nDraw the Bode plot of the system:\n\\[\nG(s) = \\frac{10(s+1)}{(s+0.1)(s-1)}\n\\]\n\nzeros: \\((s+0.1)=0 \\Rightarrow s=-1\\)\npoles: \\((s+0.1)(s-1)=0 \\Rightarrow s=0.1, s=1\\)\n\nWrite the frequency response, setting \\(s=j\\omega\\) as:\n\\[\nG(j \\omega) = \\frac{10 \\big (1+\\frac{j\\omega}{1} \\big ) \\cdot 10 \\cdot -1}{ \\big (1+\\frac{j\\omega}{0.1} \\big ) \\big (1+\\frac{j\\omega}{-1} \\big )}\n\\]\n\nConstant Gain: \\(K=-1 \\cdot 10 \\cdot 10 \\Rightarrow 20\\log_{10}(10^2)=40\\)\nWe do not have any poles/zeros at the origin, the diagram starts constant.\n\n\n\n\nA pole in \\(s=-0.1\\), a pole in \\(s=1\\) and a zero in \\(s=-1\\)\n\n\n\n\n\n\n\n\n\n\nSumming all contribution together:\n\n\n\n\n\n\n\n\n\nLet’s verify that it is correct:\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [10, 10]\nden = [1, -0.9, -0.1]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.001, 1000));"
  },
  {
    "objectID": "intro_to_freq_response_and_bode_plots.html#final-notes-on-bode-plots",
    "href": "intro_to_freq_response_and_bode_plots.html#final-notes-on-bode-plots",
    "title": "Introduction to Frequency Response Methods: Bode Plots",
    "section": "Final notes on Bode Plots",
    "text": "Final notes on Bode Plots\n\nThe frequency response interpretation of a Bode Plot only holds for stable (or marginally stable) systems\nIf we apply a sinusoidal input to an unstable system, then the output is not sinuisoidal\nUnder additional technical hypotesis we can still recover some of the results discussed before, but we will not tackle this here\nNote that we can still draw Bode plots for unstable systems\n\nOther important properties of the frequency response are: - the resonant peak \\(M_r\\), the largest value of the frequency response, - and the peak frequency \\(\\omega_{mr}\\), the frequency where the maximum occurs, - Bandwidth\n\nBandwidth\n\nDefinitions may very with application\nFor us: Bandwidth (B) is the range of frequencies for which gain is significant\nWe need to define what significant means:\n\ntypically, gain greater than 1/sqrt(2) or a -3 dB on the Bode Plot, relative to low frequency gain\ni.e. 3dB drop off w.r.t. low frequency (or could be absolute)\nsometimes also gain greater than 0 dB line is used.\n\nOften the bandwidth of interest is for the closed-loop system, but there might be little difference\nBandwidth is an indication of the speed of response of the system: higher bandwidth implies faster transient response (i.e. can track higher frequencies)\n\nFinding the –3 dB bandwidth of an arbitrary system can be a difficult problem in general. Consider, for example, the standard recipe for computing bandwidth: - Derive the input-output transfer function - Set \\(s = j\\omega\\); - Find the magnitude of the resulting expression; - Set the magnitude = \\(1/\\sqrt{2}\\) of the “midband” value (or half of the power); and - Solve for \\(\\omega_b\\)\nGiven a transfer function:\n\\[\nG(s) = \\frac{N(s)}{D(s)}\n\\]\nwe obtain the frequence response as:\n\\[\nG(j\\omega) = \\frac{N(j\\omega)}{D(j\\omega)}\n\\]\nIf we take magnitude-squared of the frequency response (power):\n\\[\n|G(j\\omega)|^2 = \\bigg | \\frac{N(j\\omega)}{D(j\\omega)} \\bigg |^2\n\\]\nwe can calculate the \\(-3 \\text{dB}\\) frequency as the frequency which results in half of the power:\n\\[\n|G(j\\omega_b)|^2 = \\frac{1}{2}\\big | G(j0) \\bigg |^2\n\\]\nor\n\\[\n\\bigg | \\frac{N(j\\omega_b)}{D(j\\omega_b)} \\bigg |^2 = \\frac{1}{2}\\bigg | \\frac{N(j0)}{D(j0)} \\bigg |^2\n\\]\nand we can now solve for \\(\\omega_b\\) to find the bandwidth.\n\nIf the dominant pole approximation holds, the Bandwidth is approximated by the frequency of the dominant pole\nWe can read the bandwidth visually inspecting the Bode plot.\n\n\nDefinition 1: Gain greater than -3dB\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [10, 10]\nden = [1, -0.9, -0.1]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.001, 1000));\n\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[-3, -3],'r--')\nplt.plot([13.5, 13.5],plt.ylim(),'r--')\nplt.text(15, 2, \"-3 dB line\");\n\n\n\n\n-3 dB Bandwidth (range of freq. for which gain > -3dB): \\(13\\) rad/s\n\n\nDefinition 2: Gain greater than 0 dB\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [10, 10]\nden = [1, -0.9, -0.1]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.001, 1000));\n\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[0, 0],'r--')\nplt.plot([10, 10],plt.ylim(),'r--')\nplt.text(11, 2, \"0 dB line\");\n\n\n\n\n0 dB Bandwidth (range of freq. for which gain > 0): \\(10\\) rad/s\n\nNot too much difference\n\n\n\nDefinition 3: -3dB relative to steady state\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [10, 10]\nden = [1, -0.9, -0.1]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(0.001, 1000));\n\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[40-3, 40-3],'r--')\nplt.plot([0.1, 0.1],plt.ylim(),'r--');\n\n\n\n\n-3dB Bandwidth relative to low frequency, is \\(B=[0, 0.1] rad/s\\)\n\n\nA different example\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\nnum = [2.5, 0]\nden = [4e-5, 0.6, 2554, 250000]\ncontrol.bode_plot(control.tf(num, den), dB=True, omega_limits=(1, 100000));\n\n\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[-60, -60],'r--')\nplt.plot([100, 100],plt.ylim(),'r--')\nplt.plot([3000, 3000],plt.ylim(),'r--')\nplt.text(150, -57, \"Significant gain\");\n\n\n\n\n\nThe bandwidth \\(B=[100, 3000]\\) rad/s\nRange of frequency where the gain is significant\nUsually easy to visually identify it\nGiven a bandwidth \\(B=[0, \\omega_b]\\)\nIf the slope of the Gain diagram is -20 dB/decade beyond frequency \\(\\omega_b\\), then the system is behaving like a first-order system, with a pole in \\(\\omega_b\\).\nThis means that the system transient response is characterised by a time constant \\(\\tau = \\frac{1}{\\omega_b}\\)\nThe system is hence the faster, the larger its bandwidth is\nThe bandwidth is hence a good indicator of the velocity of the system\n\n\n\n\nBandwidth and system response\n\nFor first order systems the relationship between bandwidth and settling time is \\[T_s \\approx \\frac{1}{w_b}\\]\nSettling time is inversely proportional to the bandiwdth:\n\nFor first order systems the settling time \\(T_s\\) is proportional to the time constant \\(\\tau\\),\nThe bandwidth \\(w_b = \\frac{1}{\\tau}\\)\n\nBroadly speaking we can say (even for systems of order greater than 1) that the larger the bandwidth, the faster is the response (the shorter is the system settling time)\n\n\n\nBandwidth in feedback systems\nGiven the system:\n\n\n\n\n\n\\[\nY(s) = \\frac{GR}{1+GR} = G_{c} U(s)\n\\]\n\nFor a closed-loop system it is normal to expect no offset for step targets, in steady-state\nThis means \\(G_{c}(0) = 1\\) or \\(0 \\text{dB}\\)\nIn this case, \\(B\\) is when the gain drops \\(-3 \\text{dB}\\)\n\nFor example:\n\nGiven a system\n\n\\[ G(s) = \\frac{2}{s+4} \\]\n\nand a compensator\n\n\\[ R(s) = 2\\frac{s+2}{s} \\]\nThe closed loop system is:\n\\[\nG_{c} = \\frac{GR}{1+GR}\n\\]\nand is Bode plot is:\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\ncontrol.bode_plot(control.tf([4, 8],[1, 8, 8]), dB=True, omega_limits=(0.01, 10));\n\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.plot(plt.xlim(),[-3, -3],'r--')\nplt.plot([1.8, 1.8],plt.ylim(),'r--');\n\n\n\n\n\nBandwidth is about \\(2 \\text{rad/s}\\)\n\nIn feedbak systems, with high loop gain, the bandwidth of the closed loop system \\(G_c\\) can be approximated by the knowledge of the loop gain function \\(GR\\):\n\\[\nG_c = \\frac{RG}{1+RG} \\approx\n\\begin{cases}\n1 \\;\\; \\text{if} \\;\\; |RG| \\gg 1 \\newline\nRG \\;\\; \\text{if} \\;\\; |RG| \\ll 1\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nExample\n\\[\nG(s) = \\frac{1000}{(s+1)^3(s+10)}\n\\]\n\\[\nG_c(s) = \\frac{G(s)}{1+G(s)}\n\\]\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10))\n\nmag_ol, phase_ol, omega_ol = control.bode(control.tf([1000],[1, 13, 33, 31, 10]), dB=True, omega_limits=(0.01, 100));\nmag_cc, phase_cc, omega_cc = control.bode(control.tf([1000],[1, 13, 33, 31, 10+1000]), dB=True, omega_limits=(0.01, 100));\n\n# bandwidth\nax1,ax2 = plt.gcf().axes     # get subplot axes\n\nplt.sca(ax1)                 # magnitude plot\nplt.text(1, 40, '$G(s)$', color='blue')\nplt.plot(plt.xlim(),[40-3, 40-3],'b--')\nplt.plot([0.5, 0.5],plt.ylim(),'b--');\nplt.text(0.5, -100, '$\\omega_b$', color='blue')\n\nplt.plot(plt.xlim(),[-3, -3],'r--')\nplt.plot([5, 5],plt.ylim(),'r--')\nplt.text(0.01, 3, '$G_c(s)$', color='orange')\nplt.text(5, -100, '$\\omega_{bc}$', color='blue');"
  },
  {
    "objectID": "main_types_of_loops_and_transfer_functions.html",
    "href": "main_types_of_loops_and_transfer_functions.html",
    "title": "Main types of loops and transfer functions",
    "section": "",
    "text": "Standard “servo” or tracking configuration of classical feedback control:\n\n\n\n\n\n\n\\(R(s)\\): Controller/Compensator\n\\(G(s)\\): Plant\n\\(Y_{ref}(s)\\): input (reference)\n\\(e(s)\\): error\n\\(U(s)\\): control signal\nThis is the “standard control loop”\n\n\n\n\n\n\n\n\n\n\n\n\nWe can always reshape them into the standard form."
  },
  {
    "objectID": "main_types_of_loops_and_transfer_functions.html#disturbances",
    "href": "main_types_of_loops_and_transfer_functions.html#disturbances",
    "title": "Main types of loops and transfer functions",
    "section": "Disturbances",
    "text": "Disturbances\nMore in general:\n\n\n\n\n\n\nLoad disturbance \\(d\\): assumed to act on the process input (but can enter in many different ways)\nMeasurement noise \\(n\\)\nThe process is a system with three inputs (control signal, load disturbance and measurement noise) and one output, the measured signal.\n\nOr equivalently\n\n\n\n\n\n\n\n\n\nThe feedback loop is influenced by three signals: \\(y_{ref}\\), \\(d\\), and \\(n\\).\nThere are three interesting outputs: \\(e\\), \\(u\\), \\(y\\)\n\nComments - Attenuation of load disturbances is often a primary goal for control - Load disturbances are typically dominated by low frequencies (slow varying). - Example: car cruise control: disturbance is gravity and changes with the slope of the road - Measurement noise corrupts the information about the process variable that the sensors deliver - Measurement noise is typically higher frequency (average typically zero) - Sometimes sensors have dynamics: often very accurate values are provided by slow sensors\nIn a typical control design problem we would choose the compensator \\(R(s)\\) such as: - the closed-loop system is stable - the loop gain \\(R(s)G(s)\\) has large magnitude at frequencies (low frequencies typically) where the power of the reference input \\(r\\) (and the plant disturbance \\(d\\)) is concentrated - the loop gain has small magnitude at frequencies (high frequency typically) where the power of the measurement noise \\(n\\) is concentrated).\nNote To obtain all the previous requirements it is convenient to state the close loop stability in terms of the open loop loop gain. This is provided by the Nyquist stability criterion as we will see later.\nTo understand the second and third requirements more, we can write the transfer function between any input-output pair\n\\[\nE(s) = \\frac{1}{1+RG}Y_{ref}(s), \\hspace{2cm} Y(s) = \\frac{1}{1+RG}D(s)\n\\]\n\\[\nY(s) = \\frac{RG}{1+RG}Y_{ref}(s), \\hspace{2cm} Y(s) = \\frac{RG}{1+RG}N(s)\n\\]\n\\[\nU(s) = \\frac{R}{1+RG}Y_{ref}(s) \\hspace{2cm} \\hspace{4cm}\n\\]\n\nSome transfer functions are the same\nAll transfer functions have \\(1 + RG\\) at the denominator: we only study stability once\nTransfer functions on the left column give the response of process variable to the set point\nThe transfer function \\(\\frac{1}{(1 + RG)}\\) tells how the process variable reacts to load disturbances\nThe transfer function \\(\\frac{RG}{(1 + RG)}\\) gives the response of the output signal to measurement noise.\nThe transfer function \\(L(s)=R(s)G(s)\\) is called loop gain or loop transfer function.\n\n\n\n\n\n\n\n- Sensitivity function\n\n\n\n\n- Complementary sensitivity \\[\nT(s) = \\frac{RG}{1+RG}\n\\]\n\n\n- Maps the noise input \\(n\\) to the output \\(y\\) - Noise rejection defines high frequency specifications - \\(S+T=1\\)\n\n\n- Note that \\(T\\) is also the transfer function from \\(y_{ref}\\) to \\(y\\). - If \\(|RG|\\) is small at frequencies where the noise \\(n\\) is concentrated then \\(|T|\\) will be small and the effect of the noise on the output is minimised. - Measurement noise tend to occur at high frequency and this means that typically we would like \\(|RG|\\) to be small at high frequency - This constraint does not conflict with the low-frequency constraints for the disturbance \\(d\\) and the reference \\(y_{ref}\\).\n\n\n### Control design task\n\n\n- Given a plant \\(G\\), we need to design a compensator \\(R\\) such that: - The loop gain magnitude |RG| is large at low frequencies (to track the reference and reject disturbance) - The loop gain magnitude |RG| rolls off (steeply decreases) to low values at high frequencies (to reject measurement noise) - The stability of the system must be guaranteed\n\n\nFinal Note: - Remember that we can have additional requirements on: - steady state response. This is typically specified in terms of response to known signals (e.g., step response error less than desired threshold (e.g. <3%), etc) - transient response (e.g. settling time, raise time, maximum overshoot - see also notebook 05_system_response.ipynb"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "",
    "text": "Instructors:\nOffice hours:"
  },
  {
    "objectID": "syllabus.html#course-meeting-times",
    "href": "syllabus.html#course-meeting-times",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "Course Meeting Times",
    "text": "Course Meeting Times\nLectures: 3 sessions (8h) / week\n\nMonday: 14.00-17:00\nWednesday: 10.30-13:30\nThursday: 08.30-10:30"
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nUnderstand the fundamental principles of automatic control, including concepts such as feedback control, stability, and controllability.\nAnalyze and design control systems using mathematical models and tools, including Laplace transforms, transfer functions, and block diagrams.\nApply control strategies to various real-world engineering problems, such as temperature control, speed control, etc.\nEvaluate the performance of control systems and identify ways to improve them, including adjusting control parameters and modifying system components.\nDevelop critical thinking and problem-solving skills by applying control principles to novel and complex engineering problems, and communicating their findings effectively.\n\nYou will be well-prepared to apply the principles of automatic control to a variety of engineering fields, such as robotics, aerospace, and manufacturing, etc."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces the design of feedback control systems. Topics include the properties and advantages of feedback systems, time-domain and frequency-domain performance measures, stability and degree of stability, the Root locus method, Nyquist criterion, and frequency-domain design.\n\n\n\n\n\n\nA bit on Python/Jupyter notebook\n\n\nOpen loop vs Closed Loop\n\n\nTransfer functions and Laplace transform\n\n\nBlock Diagrams\n\n\nReponse of a system\n\n\nFrequency response and Bode plots\n\n\nFinal Value Theorem and Steady State\n\n\n\n\n\n\nSystem Stability and Control\n\n\nThe Root Locus Method\n\n\nPID controllers\n\n\nGain and phase margins\n\n\nSensitivity Functions\n\n\nLead/Lag compensators\n\n\nState space analysis"
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "Textbooks",
    "text": "Textbooks\nIn Italian\n\nBolzern, Scattolini, Schiavoni. Fondamenti di Controlli Automatici, 2nd ed. McGraw-Hill.\n\nA useful but not mandatory textbook for this class is:\n\nOgata, Katsuhiko. Modern Control Engineering. 4th ed. Prentice Hall, 2002.\nRichard C. Dorf and Robert H. Bishop IE, Modern Control Systems (13th Edition)\n\nOther texts which might be helpful:\n\nG. Marro, Controlli Automatici, Zanichelli\nFranklin, Gene, J. David Powell, and Abbas Emami-Naeini. Feedback Control of Dynamic Systems. 6th ed. Prentice Hall, 2009. ISBN: 9780136019695.\nVan de Vegte, John. Feedback Control Systems. 3rd ed. Prentice Hall, 1994. ISBN: 9780002085069.\nKuo, Benjamin. Automatic Control Systems. 8th ed. John Wiley & Sons, 2003. ISBN: 9780471381488.\nOgata, Katsuhiko. Solving Control Engineering Problems with MATLAB. Prentice Hall, 1993. ISBN: 9780130459077."
  },
  {
    "objectID": "syllabus.html#the-role-of-software-in-the-study-of-feedback-systems",
    "href": "syllabus.html#the-role-of-software-in-the-study-of-feedback-systems",
    "title": "Syllabus - Principles of Automatic Control / Fondamenti di Automatica 2023",
    "section": "The Role of Software in the Study of Feedback Systems",
    "text": "The Role of Software in the Study of Feedback Systems\n\nUsing a software package like Python or MATLAB is very helpful in the study of Feedback Systems.\nThe software can best be used initially to check work that is first done traditionally with pencil and paper.\nThis is particularly helpful when verifying polar plots (Nyquist plots), Bode diagrams and root loci when first attempting to sketch these functions.\nStep responses in the time domain can be examined in order to build an intuitive sense of the relations between time and frequency domain behavior.\nSimplifying approximations that are often made when carrying out preliminary designs can be checked for validity using Python.\nMore complex problems can be studied with a computer-aided design package without the enormous burden of doing extensive computations.\nWe suggest that everyone become familiar with the use of Python or MATLAB.\nRemember that the computer is to aid in achieving understanding and should be used intelligently as an engineering tool.\nThe goal is for you to come away with an understanding of feedback theory in some depth."
  },
  {
    "objectID": "block_diagrams.html",
    "href": "block_diagrams.html",
    "title": "Block Diagrams",
    "section": "",
    "text": "This notebook will talk about: - Basic components of block diagrams - How to compose block diagrams together - Block diagrams and system properties (e.g., Stability, etc)"
  },
  {
    "objectID": "block_diagrams.html#block-diagrams",
    "href": "block_diagrams.html#block-diagrams",
    "title": "Block Diagrams",
    "section": "Block Diagrams",
    "text": "Block Diagrams\n\nOne of the control system engineer’s favorite tools for organizing, communicating, simulating and solving problems\nStandard representation of interconnected systems and subsystems using Transfer Functions\nMakes it easier to identify inputs, outputs and dynamic systems\nBlock diagrams show us the interrelationship of systems and how signals flow between them.\n\nvariables or signals: arrows (\\(\\rightarrow\\))\nsystems: blocks with a transfer function, and arrows going in (inputs) and out (outputs)\nsummation node: circle with arrows going in (with the appropriate signs) and an arrow going out\nbranching points: represents two or more variables are replicas of the variable before the branching point\n\nAlways useful during the analysis.\n\nFor example:"
  },
  {
    "objectID": "block_diagrams.html#block-diagram-algebra",
    "href": "block_diagrams.html#block-diagram-algebra",
    "title": "Block Diagrams",
    "section": "Block Diagram Algebra",
    "text": "Block Diagram Algebra\n\nBlock diagrams can be quite complex, and when that happens it is useful to know how to calculate the transfer function between a specific input and a specific output.\nFor this, it is better to start with some easy cases: Cascaded, Parallel and Feedback Loop.\nMore complex cases are handled recursively applying these rules. Note that, given that we are restricting the analysis to linear systems, the impact of each input on a specific output is independent of other inputs that can be present (they can all be set to 0).\n\n\nCascaded Blocks\n\n\n\n\n\n\n\n\n\nThis can be easily calculated using algebric manipulation:\n\\[Y(s)=G_2(s)U(s)\\] \\[U(s)=G_1(s)E(s)\\]\nwhich means:\n\\[Y(s)=G_2(s)G_1(s)E(s) \\Rightarrow G(s) = G_1(s)G_2(s)\\]\n\n\nParallel\n\n\n\n\n\n\n\n\n\nThis can be easily calculated using algebric manipulation:\n\\[Y(s)=G_1(s)R(s) + G_2(s)R(s) + G_3(s)R(s) = (G_1(s) + G_2(s) + G_3(s))R(s)\\]\nwhich means:\n\\[ G(s) = G_1(s) + G_2(s) + G_3(s)\\]\nNote the presence of: - take off points: allow an unaltered signal to go along multiple paths. - summing junctions: sum or subtract signals\n\n\nFeedback Loop\n\n\n\n\n\n\n\n\n\nSo, how do we do this one?\n\\[Y(s)=G_1(s)*E(s)\\] \\[E(s)=R(s)-Y_m(s)\\] \\[Y_m(s)=G_2(s)Y(s)\\]\nor\n\\[Y(s)=G_1(s)*(R(s)-Y_m(s)) \\rightarrow Y(s)=G_1(s)*(R(s)-G_2(s)Y(s)) \\]\nand finally:\n\\[Y(s)+G_1(s)G_2(s)Y(s) = G_1(s)*R(s) \\]\n\\[ G(s) = \\frac{G_1(s)}{1 + G_1(s)G_2(s)} \\]"
  },
  {
    "objectID": "block_diagrams.html#zero-pole-cancellations",
    "href": "block_diagrams.html#zero-pole-cancellations",
    "title": "Block Diagrams",
    "section": "Zero-Pole Cancellations",
    "text": "Zero-Pole Cancellations\n\nWhen we compose systems together we expect the order of the system being equal to the sum of the order of each individual system.\nHowever, when we compose systems together and calculate the transfer function it might happen that the resulting function has less poles.\n\nIn this case, we have cancellations that correspond to “hidden” parts of the system.\n\n\nLet’s consider an example:\n\n\n\n\n\n\n\n\n\nwhere\n\\[\\begin{equation}\nG_1(s)=\\frac{s+1}{s+2}\\;\\; \\text{and}\\;\\; G_2(s)=\\frac{1}{s}\n\\end{equation}\\]\nWe reduce the feedback loop first:\n\\[G_3(s)=\\frac{1/s}{1+1/s} = \\frac{1/s}{\\frac{s+1}{s}} = \\frac{1}{s+1}\\]\nThen the serie: \\[G(s) = \\frac{s+1}{s+2} \\frac{1}{s+1} = \\frac{1}{s+2} \\]\nThis connection between two systems determined an “hidden” part, which is not visible in the input/output relationship and the system behaves like a first-order one.\n\nObservability and Controllobality of Interconnected Systems\n\nControllability measures the ability of a particular actuator configuration to control all the states of the system;\nObservability measures the ability of the particular sensor configuration to supply all the information necessary to estimate all the states of the system.\n\n\nCascaded Blocks\n\n\n\n\n\n\n\n\n\n\\[G(s) = G_1(s)G_2(s) = \\frac{N_1(s)}{D_1(s)}\\frac{N_2(s)}{D_2(s)}\\]\n\nIf \\(N_1(s)\\) and \\(D_2(s)\\) have roots in common, then \\(G(s)\\) is not controllable (zero-pole cancellation)\nIf \\(N_2(s)\\) and \\(D_1(s)\\) have roots in common, then \\(G(s)\\) is not observable (pole-zero cancellation)\n\n\n\nParallel Blocks\n\n\n\n\n\n\n\n\n\n\\[G(s) = G_1(s) + G_2(s) = \\frac{N_1(s)}{D_1(s)} + \\frac{N_2(s)}{D_2(s)} = \\frac{N_1(s)D_2(s) + N_2(s)D_1(s)}{D_1(s)D_2(s)}\\]\n\nCancellations can happen only when \\(D_1(s)\\) and \\(D_2(s)\\) have common roots (you can bring them out together in the numerator and then cancel them out with those in the denumerator). When this happens, we have a non-observable and non-controllable part of the system.\n\n\n\nFeedback Loop\n\n\n\n\n\n\n\n\n\n\\[G(s) = \\frac{G_1(s)}{1 + G_1(s)G_2(s)} =\\frac{\\frac{N_1(s)}{D_1(s)}}{1 + \\frac{N_1(s)}{D_1(s)}\\frac{N_2(s)}{D_2(s)}} = \\frac{N_1(s)D_2(s)}{D_1(s)D_2(s) + N_1(s)N_2(s)}\\]\n\nWe have the equivalent of a serie interconnection: \\(G_1(s)G_2(s)\\), same rules apply\nCancellations occur when \\(N_1(s)\\) and \\(D_2(s)\\) have common factors (you can bring them out together in the numerator and then cancel them out with those in the denumerator):\n\nWhen zero of \\(G_1(s)\\) cancels a pole of \\(G_2(s)\\) then we have a non-observable and non-controllable part of the system.\nWhen pole of \\(G_1(s)\\) cancels a zero of \\(G_2(s)\\) then we still have a fully observable and fully controllable system. This is because the denominator of \\(G(s)\\) does not lose its degree.\n\n\nExample:\n\nzero of \\(G_1(s)\\) cancels a pole of \\(G_2(s)\\) \\[G_1(s)=\\frac{s+1}{s+2},\\;\\; G_2(s)=\\frac{s+3}{s+1}\\]\n\n\\[\\Rightarrow G(s)= \\frac{\\frac{s+1}{s+2}}{1+\\frac{s+3}{s+2}} = \\frac{s+3}{1+(s+3)}\\]\n\npole of \\(G_1(s)\\) cancels a zero of \\(G_2(s)\\) \\[G_1(s)=\\frac{s+1}{s+2},\\;\\; G_2(s)=\\frac{s+2}{s+3}\\]\n\n\\[\\Rightarrow G(s)= \\frac{\\frac{s+1}{s+2}}{1+\\frac{s+1}{s+3}} = \\frac{\\frac{(s+1)(s+3)}{s+2}}{(s+3)+(s+1)} = \\frac{(s+1)(s+3)}{(s+2)((s+3)+(s+1))}\\]\n\n\n\nStability of Interconnected Systems\nLet’s now analyse the stability of a system which is obtained through a composition of subsystems.\nQuestion: - Is the asymptotic stability of each indidual subsystem necessary/sufficient to guarantee the stability of the entire system?\nAssumption: - Each subsystem are represented by rational functions with numerator and denominator prime between them.\n\nCascaded Blocks\n\n\n\n\n\n\n\n\n\n\\[G(s) = G_1(s)G_2(s) = \\frac{N_1(s)}{D_1(s)}\\frac{N_2(s)}{D_2(s)}\\]\nNo zero-pole cancellations - The denominator of \\(G(s)\\) is the product of each denominator, and the poles of \\(G(s)\\) are the union of the poles of each subsystem. - If and only if, the poles of \\(G_1(s)\\) and \\(G_2(s)\\) have Re \\(<0\\), then \\(G(s)\\) is asymptotically stable.\nZero-pole/pole-zero cancellations - In this case, the overall system has an hidden part corresponding to the zero-pole or pole-zero cancellation. In this case, if the cancellation is of a pole with Re \\(\\ge0\\) then the hidden part is not stable and the entire system is not stable. - We call these types of cancellations, critical. - Note that this is not visible directly from the transfer function!\n\n\nParallel Blocks\n\n\n\n\n\n\n\n\n\n\\[G(s) = G_1(s) + G_2(s) = \\frac{N_1(s)}{D_1(s)} + \\frac{N_2(s)}{D_2(s)} = \\frac{N_1(s)D_2(s) + N_2(s)D_1(s)}{D_1(s)D_2(s)}\\]\nThe analysis is similar to the previous case (serie).\nIf there are no cancellations, the poles of \\(G(s)\\) are the product of the poles of \\(G_1(s)\\) and of \\(G_2(s)\\). - \\(G(s)\\) is asymptotically stable, if and only if, the poles of \\(G_1(s)\\) and \\(G_2(s)\\) have Re \\(<0\\).\nIf there are cancellations, then the overall system has an hidden part corresponding to the zero-pole or pole-zero cancellation. If the cancellation is of a pole with Re \\(>=0\\) then the hidden part is not stable and the entire system is not stable.\nAgain, this is not visible analysing the final T.F.\n\n\nFeedback Loop\n\n\n\n\n\n\n\n\n\n\\[G(s) = \\frac{G_1(s)}{1 + G_1(s)G_2(s)} =\\frac{\\frac{N_1(s)}{D_1(s)}}{1 + \\frac{N_1(s)}{D_1(s)}\\frac{N_2(s)}{D_2(s)}} = \\frac{N_1(s)D_2(s)}{D_1(s)D_2(s) + N_1(s)N_2(s)}\\]\n\nIn this case, the poles of the entire system are not only dependednt on the poles of the two subsytems (the numerators are also playing a role).\n\nIf we do not have cancellations, then the poles of the systems are the roots of the equation:\n\\[D_1(s)D_2(s) + N_1(s)N_2(s) = 0\\]\nthis is called characteristic equation of the feedback system.\nWe can also write the equation above as:\n\\[ 1 + \\frac{N_1(s)D_2(s)}{D_1(s)D_2(s)} = 0\\]\nThe feedback system is asymptotically stable if and only if the roots of the previous equation are with Re \\(<0\\), but this does not depend on the stability property of the individual systems.\nCritical cancellations\n\nNote that when we calculate \\(\\frac{N_1(s)D_2(s)}{D_1(s)D_2(s)}\\) we can have critical cancellations and these would correspond to hidden parts of the system (Note that we are assuming to use \\(y\\) or \\(y_m\\) as output variables of the system).\nAs before, if these critical cancellations are of poles with Re \\(\\ge0\\), these hidden parts are not asymptotically stable and neither is the full system.\nThis is not visible from the analysis of the T.F.\nWe have to verify if there are critical cancellations before we analyse the stability of the feedback system.\n\nSince the poles of the feedback system are, in general, different from the poles of the individual subsystems, we can use this connection to effectively modify the dynamics of the system, placing the poles where we need them. For example, it is possible to stabilise a non-stable system, or obtain desired performance.\nNote that the opposite is also true: we can have a non-stable system even if we the individual subsystems are stable."
  },
  {
    "objectID": "block_diagrams.html#equivalent-block-diagrams",
    "href": "block_diagrams.html#equivalent-block-diagrams",
    "title": "Block Diagrams",
    "section": "Equivalent block diagrams",
    "text": "Equivalent block diagrams"
  },
  {
    "objectID": "block_diagrams.html#bibo-stability",
    "href": "block_diagrams.html#bibo-stability",
    "title": "Block Diagrams",
    "section": "BIBO Stability",
    "text": "BIBO Stability\n\nA system is BIBO stable (Bounded Input, Bounded Output) if for each limited input, there is a limited output\nFor linear systems: BIBO stability if and only if poles of the transfer functions have Re \\(<0\\)\nCaution! The poles of the T.F. are only those that are controllable and observable!\nBIBO stability only depends on the forced response of the system."
  },
  {
    "objectID": "block_diagrams.html#simplifying-block-diagrams",
    "href": "block_diagrams.html#simplifying-block-diagrams",
    "title": "Block Diagrams",
    "section": "Simplifying block diagrams",
    "text": "Simplifying block diagrams\nLet’s now apply the block diagram algebra to some examples:\n\nExample 1:\nLet’s suppose that we have a classic negative feedback:\n\n\n\n\n\n\n\n\n\n\\(B\\) and \\(C\\) are in series and we can combine them into a single block:\n\n\n\n\n\n\n\n\n\nWe can now reduce the feedback loop:\n\n\n\n\n\n\n\n\n\nAnd lastly we can combine the two blocks in series:\n\n\n\n\n\n\n\n\n\n\n\nCreating Unity Feedback\n\nSometimes when we manipulate block diagrams we do not want to remove blocks to simplify the diagram, but we would like to restructure the diagram to have it in a specific form.\nFor example we would like to make the following equivalent:\n\n\n\n\n\n\n\n\n\n\n\n\nWe need to determine an \\(R(s)\\) value that makes the two systems equivalent.\nWe need to write the transfer function for each system, set the two equations equal to each other, and then solving for \\(R(s)\\).\n\n\\[\n\\frac{G(s)}{1+G(s)H(s)} = \\frac{R(s)}{1+R(s)}  \n\\]\n\\[ \\Downarrow \\]\n\\[\n\\frac{G(s) (1+R(s))}{1+G(s)H(s)} = R(s)\n\\]\n\\[ \\Downarrow \\]\n\\[\n\\frac{G(s)}{1+G(s)H(s)} = R(s) \\bigg( 1- \\frac{G(s)}{1+G(s)H(s)} \\bigg)\n\\]\n\\[ \\Downarrow \\]\n\\[\nR(s) = \\frac{G(s)}{1+G(s)H(s)-G(s)}\n\\]\nNote that we would not be able to easily use this method to go from a unity feedback system to a non-unity feedback system (there will be two unknown variables)."
  },
  {
    "objectID": "transfer_functions.html#problem-settings",
    "href": "transfer_functions.html#problem-settings",
    "title": "Transfer Functions",
    "section": "Problem Settings",
    "text": "Problem Settings\nIt is key to be able to describe the system in an efficient and useful manner.\n\nTo do this, we describe the system mathematically writing the equations of motion in the form of differential equations.\nWe have seen how to model the simplified case of a car driving uphill.\n\nThe two most popular representations: - state space representation - transfer functions.\nLoosely speaking, transfer functions are a Laplace domain representation of your system and they are commonly associated with the era of control techniques labeled classical control theory.\nState space is a time domain representation, packaged in matrix form, and they are commonly associated with the era labeled modern control theory.\n\n\n\n\n\n\n\n\n\n\nEach representation has its own set of benefits and drawbacks and as a control engineer we need to use what is best for our problem.\nWe will focus on Transfer Functions."
  },
  {
    "objectID": "transfer_functions.html#definition-of-transfer-function",
    "href": "transfer_functions.html#definition-of-transfer-function",
    "title": "Transfer Functions",
    "section": "Definition of Transfer Function",
    "text": "Definition of Transfer Function\n\nA transfer function is the Laplace transform of the impulse response of a linear, time-invariant (LTI) system with a single input and single output when you set the initial conditions to zero.\n\n\nThe s domain and the Laplace Transform\n\nThe Laplace Transform takes the Fourier Transform one step forward\nDecomposes a time domain signal into both cosines and exponential functions\nFor the Laplace transform we need a symbol that represents more than just frequency, \\(\\omega\\)\nNeed to also account for the exponential aspect of the signal.\nThis is where the variable \\(s\\) comes in.\n\n\\(s\\) is a complex number, which means that it contains values for two dimensions: - one dimension that describes the frequency of a cosine wave - the second dimension that describes the exponential term.\nIt is defined as \\(s = \\sigma + j\\omega\\).\nExponential functions that have imaginary exponents, such as \\(e^{j2t}\\), produce two-dimensional sinusoids through Euler’s formula:\n\\[e^{j\\omega t} = cos(\\omega t) + jsin(\\omega t)\\]\nFor exponential functions that have real numbers for exponents: - Negative real numbers give us exponentially decaying signals - Positive real numbers give us exponentially growing signals.\nTwo examples are \\(e^{2t}\\), which grows exponentially for all positive time, and \\(e^{−5t}\\), which decays exponentially for all positive time.\n\nfig = plt.figure()\n\nt = np.linspace(-0.2, 1, 20)\nplt.plot(t, np.exp(2*t), linewidth=6)\nplt.plot(t, np.exp(-5*t), linewidth=6)\nplt.grid()\nplt.xlabel('time (s)')\n\nText(0.5, 0, 'time (s)')\n\n\n\n\n\nNow let’s think about our new variable \\(s\\) which has both a real and imaginary component.\n\nThe equation \\(e^{st}\\) is really just an exponential function multiplied by a sinusoid\n\n\\[e^{st} = e^{(\\sigma+j\\omega)t} = e^{\\sigma t}e^{j\\omega t}\\]\n\nimport cmath\n\nimport warnings\nwarnings.filterwarnings('ignore') # we know we are forcing our hands below, so no need to be reminded..\n\n\nfig, axs = plt.subplots(1,3, figsize=(20,5))\n\nt = np.linspace(-0.2, 10, 50) # time range\n\n# s = 1 + j2\ns = complex(1, 2) \n\naxs[0].plot(t[:10], np.exp(t[:10]), linewidth=6); axs[0].set_title('exp(t)')\naxs[1].plot(t, np.exp(complex(0,2)*t), linewidth=6); axs[1].set_title('exp(j2t)')\naxs[2].plot(t, np.exp(t)*np.exp(complex(0,2)*t), linewidth=6); axs[2].set_title('exp(st)')\n\nplt.grid()\nplt.xlabel('time (s)')\nfig.tight_layout()\n\n\n\n\n\nfig, axs = plt.subplots(1,3, figsize=(20,5))\n\nt = np.linspace(-5, 5, 50)\n\n# s = -2 + j\ns = complex(-2, 1) \n\naxs[0].plot(t[:10], np.exp(-2*t[:10]), linewidth=6); axs[0].set_title('exp(-2t)')\naxs[1].plot(t, np.exp(complex(0, 1)*t), linewidth=6); axs[1].set_title('exp(jt)')\naxs[2].plot(t, np.exp(-2*t)*np.exp(complex(0, 1)*t), linewidth=6); axs[2].set_title('exp(st)')\n\nplt.grid()\nplt.xlabel('time (s)')\nfig.tight_layout()\n\n\n\n\n\nWe can combine the two parts of \\(s = \\sigma + j\\omega\\) into a two-dimensional plane where the real axis is the exponential line and the imaginary axis is the frequency line.\nThe value of \\(s\\) provides a location in this plane and describes the resulting signal, \\(e^{st}\\), as function of the selected \\(\\omega\\) and \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Laplace Transform\nThe Laplace transform is an integral transform that converts a function of a real variable \\(t\\) (e.g., \\(time\\)) to a function of a complex variable \\(s = \\sigma + j\\omega \\in \\mathbb{C}\\) (complex frequency).\n\\[{\\displaystyle F(s)={\\mathcal {L}}\\{f\\}(s)=\\int _{0}^{\\infty }f(t)e^{-st}\\,dt.}\\]\nThe inverse Laplace transform is:\n\\[{\\displaystyle f(t)={\\mathcal {L}}^{-1}\\{F(s)\\}(t)={\\frac {1}{2\\pi j}}\\lim _{\\omega\\to \\infty }\\int _{\\sigma -j\\omega}^{\\sigma +j\\omega}e^{st}F(s)\\,ds}\\]\nThe Laplace transform is particularly important because it is a tool for solving differential equations: it transforms linear differential equations into algebraic equations, and convolution into multiplication!\n\nDerivatives and integrals become algebric operations\n\nFrom a system perspective, the Transfer function expresses the relation between the Laplace Transform of the input and that of the output:\n\\[ Y(s)=\\mathbf{c}^T(s\\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{b} U(s) \\]\n\n\n\nProperties of the Laplace Transform: relationships between time and frequency\nThe Laplace transform has a number of properties. These are really useful to calculate the transforms without using its integral definition.\n\n\n\n\n\n\n\n\nProperty\nTime domain\nFrequency domain (s)\n\n\n\n\nLinearity\n\\(af(t)\\) + \\(bg(t)\\)\n\\(aF(s)+bG(s)\\)\n\n\nTime shift (delay)\n\\(f(t-\\tau)\\)\n\\(e^{-\\tau s}F(s)\\)\n\n\nFrequency shift\n\\(f(t)e^{\\alpha t}\\)\n\\(F(s-\\alpha)\\)\n\n\nDerivative\n\\(\\frac{df}{dt}(t)\\)\n\\(sF(s) − f(0)\\)\n\n\nSecond Derivative\n\\(\\frac{df^2}{d^2t}(t)\\)\n\\(s^2F(s)-sf(0)-f'(0)\\)\n\n\nIntegral\n\\(\\int _{0}^{t}f(\\tau )\\,d\\tau\\) = \\((u*f)(t)\\)\n\\(\\frac{1}{s}F(s)\\)\n\n\nConvolution\n\\((f*g)(t)\\) = \\(\\int _{0}^{t} f(\\tau )g(t\\)-\\(\\tau )d\\tau\\)\n\\(F(s)G(s)\\)\n\n\n\n\\(t\\rightarrow\\inf\\)\n\\(s\\rightarrow 0\\)\n\n\n\n\\(t\\rightarrow 0\\)\n\\(s\\rightarrow \\inf\\)\n\n\n\nFrom this table it is easier to calculate Laplace transforms of known functions:\n\nsource\n\n\ndelta\n\n delta (t, delta_t=0, eps=None)\n\n\nsource\n\n\nramp\n\n ramp (t)\n\n\nsource\n\n\nstep\n\n step (t, step_time=0)\n\nHeaviside step function\n\nfig, axs = plt.subplots(1,4, figsize=(10, 5))\n\nt = np.linspace(-10,10,1000)\n\naxs[0].plot(t, delta(t), linewidth=3)\naxs[1].plot(t, step(t), linewidth=3)\naxs[2].plot(t, ramp(t), linewidth=3)\naxs[3].plot(t, ramp(t)**2, linewidth=3)\n\nfig.tight_layout()\n\n\n\n\nNote that the step function is the derivative of the ramp, and that the ramp is the derivative of the quadratic function.\n\n\n\nTransforms of known functions\n\n\nDelta function\nIf we then want to calculate the Laplace transform of the delta \\(\\delta(t)\\) function:\n\\[{\\displaystyle F(s)={\\mathcal {L}}\\{f\\}(s)=\\int _{0}^{\\infty }\\delta(t)e^{-st}\\,dt.}\\]\nwhere\n\\[\\delta(t)=0, \\forall t \\neq 0 \\] \\[\\int _{-\\infty}^{\\infty } \\delta(t)dt=1\\]\n\\[{\\displaystyle F(s)={\\mathcal {L}}\\{f\\}(s)=\\int _{0}^{\\infty }\\delta(t)e^{-st}\\,dt.}=e^{-s0}=1\\]\n\n\nStep function\nOr of the step \\(1(t)\\) function:\n\\[{\\displaystyle F(s)={\\mathcal {L}}\\{f\\}(s)=\\int _{0}^{\\infty }1(t)e^{-st}\\,dt.}\\]\nwhere\n\\[1(t)=0, \\forall t < 0 \\] \\[1(t)=1, \\forall t \\geq 0 \\]\n\\[{\\displaystyle F(s)={\\mathcal {L}}\\{f\\}(s)=\\int _{0}^{\\infty }1(t)e^{-st}\\,dt.} = \\int _{0}^{\\infty}e^{-st}\\,dt. = -\\frac{e^{-st}}{s} \\Bigg|^\\infty_0 = -\\frac{1}{s} \\bigg( \\lim_{t\\rightarrow \\infty} e^{-st} - \\lim_{t\\rightarrow 0} e^{-st} \\bigg) = \\frac{1}{s}\\]\nA great thing about working with linear systems is that usually we don’t have to perform the Laplace transform integration by hand.\nMost the Laplace transforms for most functions that we will encounter have been solved many times and are available already collected into tables.\n\n\nPutting this all Together\nLet’s consider the rolling cart example:\n\n\n\n\n\nWe can model this cart as:\n\\[m\\ddot{x} = F_{input}(t) - F_{dumper}(t) - F_{spring}(t)\\]\nAnd since: \\(F_{spring}(t)=kx(t)\\), \\(F_{dumper}(t)=b\\dot{x}(t)\\)\nWe obtain:\n\\[m\\ddot{x} + kx(t) + b\\dot{x}(t) -  F_{input}(t) = 0\\]\nWe can convert this system model into a transfer function: - it’s a linear, - time-invariant system, - there is a single input, \\(F_{input}\\), - and a single output, \\(x\\).\nTo calculate the Transfer Function we need to take the Laplace Transform of the impulse response of the system.\nTo do so, let’s set our input to \\(F_{input}(t)=\\delta(t)\\) and solve for the response \\(x(t)\\).\n\\[m\\ddot{x} + kx(t) + b\\dot{x}(t) -  \\delta(t) = 0\\]\n\nSolving linear, ordinary differential equations in the time domain can be time consuming.\nWe can make the task easier by taking the Laplace transform of the entire differential equation, one term at a time, and solve for the impulse response in the \\(s\\) domain directly.\nSimply take each term and replace with the corresponding \\(s\\) domain equivalent.\n\n\n\n\n\n\n\n\ntime domain\n\\(\\large m\\ddot{x} + kx(t) + b\\dot{x}(t) - \\delta(t) = 0\\)\n\n\n\\(s\\) domain\n$(ms^2 + bs + k)X(s) - 1 = 0 $\n\n\n\\(X(s)\\) is the impulse response in the \\(s\\) domain\n\\(\\large X(s) = \\frac{1}{m*s^2 + bs + k}\\)\n\n\n\nnote that the initial conditions are zeros\nTo go back to the time domain we could preform the inverse Laplace transform on \\(X(s)\\)."
  },
  {
    "objectID": "transfer_functions.html#table-of-laplace-transforms",
    "href": "transfer_functions.html#table-of-laplace-transforms",
    "title": "Transfer Functions",
    "section": "Table of Laplace transforms",
    "text": "Table of Laplace transforms"
  },
  {
    "objectID": "transfer_functions.html#inverse-laplace-transform-partial-fraction-decomposition",
    "href": "transfer_functions.html#inverse-laplace-transform-partial-fraction-decomposition",
    "title": "Transfer Functions",
    "section": "Inverse Laplace Transform: Partial fraction decomposition",
    "text": "Inverse Laplace Transform: Partial fraction decomposition\nTo find the inverse Laplace transform of a complicated function, we can convert the function to a sum of simpler terms for which we know the Laplace transform of each term. The result is called a partial-fraction expansion.\nGiven:\n\\[Y(s) = G(s)U(s) = \\frac{N(s)}{D(s)}U(s)\\]\nwhere the order of \\(N(s)\\) is less than the order of \\(D(s)\\), then a partial-fraction expansion can be made.\nIf the order of \\(N(s)\\) is greater than or equal to the order of \\(D(s)\\), then \\(N(s)\\) must be divided by \\(D(s)\\) successively until the result has a remainder whose numerator is of order less than its denominator.\nWe want to expand \\(G(s)\\) into the sum of functions for which we already know the inverse transform, then thanks to the linearity we can simply sum them all up to obtain the inverse of the entire function:\n\nCase 1. Roots of the Denominator of F(s) Are Real and Distinct\nLet’s suppose we have all distinct poles:\n\\[ D(s) = \\prod^{n}_{k=1}{(s-p_k)}\\]\nwe want to find the coefficient \\(P_k\\) such that:\n\\[ \\frac{N(s)}{\\prod^{n}_{k=1}{(s-p_k)}} = \\sum^{n}_{k=1}\\frac{P_k}{s-p_k}\\]\nMultiplying for \\((s-p_i)\\):\n\\[ (s-p_i)\\frac{N(s)}{\\prod^{n}_{k=1}{(s-p_k)}} = (s-p_i)\\sum^{n}_{k=1}\\frac{P_k}{s-p_k}\\]\nwe can obtain:\n\\[P_i = [(s-p_i)G(s)] \\big|_{s=p_i}\\]\nand finally:\n\\[ g(t) = \\mathcal {L}^{-1}[G(s)]=\\mathcal {L}^{-1} \\bigg[\\sum^{n}_{k=1}\\frac{P_k}{s-p_k}\\bigg] = \\sum^{n}_{k=1} P_k e^{p_kt}\\]\nFor example:\n\\[G(s) = \\frac{s-10}{(s+2)(s+5)}\\]\n\\[P_1=(s+2)\\frac{s-10}{(s+2)(s+5)}\\bigg|_{s=-2}=-\\frac{12}{3}=-4\\] \\[P_2=(s+5)\\frac{s-10}{(s+2)(s+5)}\\bigg|_{s=-5}=\\frac{-15}{-3}=5\\]\nwhich means that:\n\\[G(s) = \\frac{-4}{(s+2)} + \\frac{5}{(s+5)}\\]\nand finally:\n\\[ g(t) = \\mathcal {L}^{-1}[G(s)] = -4e^{-2t} + 5e^{-5t}\\]\n\n\nCase 2. Roots of the Denominator of F(s) Are Real and Repeated\nIf we have muliple poles the decomposition is similar.\nLet’s consider, as an example\n\\[\nY(s) =  \\frac{2}{(s+1)(s+2)^2}\n\\]\nThe roots of \\((s+2)^2\\) in the denominator are repeated, since the factor is raised to an integer power higher than 1. In this case, the denominator root at \\(-2\\) is a multiple root of multiplicity 2.\nWe can write the partial-fraction expansion as a sum of terms, where each factor of the denominator forms the denominator of each term.\nIn addition, each multiple root generates additional terms consisting of denominator factors of reduced multiplicity.\nIn our case\n\\[\nY(s) =  \\frac{2}{(s+1)(s+2)^2} = \\frac{K_1}{(s+1)} + \\frac{K_2}{(s+2)^2} + \\frac{K_3}{(s+2)}\n\\]\n\nWe obtain \\(K_1\\) as before. In this case \\(K_1=2\\)\nWe obtain \\(K_2\\) multiplying the previous equation by \\((s+2)^2\\):\n\n\\[\n\\frac{2}{(s+1)} = \\frac{K_1(s+2)^2}{(s+1)} + K_2 + K_3(s+2)\n\\]\nWhen \\(s \\rightarrow -2\\), \\(K_2=-2\\)\n\nWe obtain \\(K_3\\) differentiating the previous equation with respect to \\(s\\):\n\n\\[\n\\frac{-2}{(s+1)^2} = \\frac{2(s+2)K_1}{(s+1)^2} + K_3\n\\]\nFrom which \\(K_3\\) can be isolated and found if we let \\(s \\rightarrow -2\\). Hence, \\(K_3=-2\\).\nIn this case then:\n\\[\nY(s) =  \\frac{2}{(s+1)(s+2)^2} = \\frac{2}{(s+1)} + \\frac{-2}{(s+2)^2} + \\frac{-2}{(s+2)}\n\\]\nand the inverse transform is:\n\\[\ny(t) = 2e^{-t} - 2te^{-2t} -2e^{-2t}\n\\]\nIf the denominator root is of higher multiplicity than 2, successive differentiation would isolate each residue in the expansion of the multiple root.\nIn general, given a \\(H(s)\\) whose denominator has real and repeater roots:\n\\[H(s) = \\frac{N(s)}{(s+p_1)^r(s+p_2)...(s+p_n)}\\]\nWe can find the general expression for \\(K_1\\) (the coefficient of the roots with multiplicity greater than 1):\n\\[\nK_i = \\frac{1}{(i-1)!}\\frac{d^{i-1}(F(s)(s+p_1)^r)}{ds^{i-1}}\\Big|_{s\\rightarrow-p_1} \\;\\; i=1,2,...,r\n\\]\n\n\nCase 3. Roots of the Denominator of F(s) Are Complex or Imaginary\nThe technique used for the partial-fraction expansion of \\(F(s)\\) with real roots in the denominator can be used for complex and imaginary roots.\nHowever, the residues of the complex and imaginary roots are themselves complex conjugates.\nIn this case, the resulting terms can be identified as:\n\\[\n\\frac{e^{j\\theta}+e^{-j\\theta}}{2} = \\cos{\\theta}\n\\]\nand\n\\[\n\\frac{e^{j\\theta}-e^{-j\\theta}}{2j} = \\sin{\\theta}\n\\]\nFor example:\n\\[\nF(s) = \\frac{3}{s(s^2+2s+5)} = \\frac{3}{s(s+1+j2)(s+1-j2)} = \\frac{K_1}{s} + \\frac{K_2}{s+1+j2} + \\frac{K_3}{s+1-j2}\n\\]\n\\(K_1\\) is found as usual, and found \\(K_1=3/5\\).\nTo find \\(K_2\\):\n\\[\nK_2 = \\frac{3}{s(s+1-j2)}\\Big|_{s\\rightarrow -1-j2} = \\frac{-3}{20}(2+j1)\n\\]\n\\(K_3\\) is found to be the complex conjugate of \\(K_2\\).\n\\[\nF(s) = \\frac{3/5}{s} - \\frac{3}{20}\\Big(\\frac{2+j1}{s+1+2j} + \\frac{2-j1}{s+1-2j}\\Big)\n\\]\nwhich we can inverse transform to obtain:\n\\[\nf(t) = \\frac{3}{5} - \\frac{3}{20}\\Big[ (2+j1)e^{-(1+j2)t} + (2-j1)e^{-(1-j2)t} \\Big]\n\\]\n\\[\nf(t) = \\frac{3}{5} - \\frac{3}{20} e^{-t}\\Big[4 \\Big( \\frac{e^{j2t}+e^{-j2t}}{2}\\Big) + 2 \\Big(\\frac{e^{j2t}-e^{-j2t}}{2j} \\Big) \\Big]\n\\]\n\\[\nf(t) = \\frac{3}{5} - \\frac{3}{5} e^{-t} \\Big( cos(2t) + \\frac{1}{2}sin(2t) \\Big) = 0.6 - 0.671e^tcos(2t-\\phi)\n\\]\nwhere \\(\\phi = arctan0.5=26.57^o\\)\n\n\n\nSome more on the Laplace transform\n\nConvert dfferential problems into algebric ones (finding the poles of the system)\nMakes it possible to understand the system output for a given input\nUsing the transfer function it is possible to represent the input/output relationship\nInterconnected systems can be represented using block diagrams\nHowever, it does not allow analysing the controllability and observability of each single block!"
  },
  {
    "objectID": "transfer_functions.html#sympy",
    "href": "transfer_functions.html#sympy",
    "title": "Transfer Functions",
    "section": "Sympy",
    "text": "Sympy\nThe sympy Python module makes it easier to work with Laplace transforms. Let’s import it:\n\nimport sympy\n\n\nsympy.init_printing()\n\n\n# Let's also ignore some warnings here due to sympy using an old matplotlib function to render Latex equations.\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\n\nAnd let’s define the symbols with need to work with.\n\nt, s = sympy.symbols('t, s')\na = sympy.symbols('a', real=True, positive=True)\n\nSympy provides a function called laplace_transform to easily calculate Laplace transforms:\nFor example, if we want to know the Laplace transform of \\(e^{\\alpha t}\\)\n\nf = sympy.exp(a*t)\nf\n\n\\(\\displaystyle e^{a t}\\)\n\n\n\nF = sympy.laplace_transform(f, t, s, noconds=True)\nF\n\n\\(\\displaystyle \\frac{1}{- a + s}\\)\n\n\nWe can define a function to make it easier:\n\ndef L(f):\n    return sympy.laplace_transform(f, t, s, noconds=True)\n\ndef invL(F):\n    return sympy.inverse_laplace_transform(F, s, t)\n\n\nL(f)\n\n\\(\\displaystyle \\frac{1}{- a + s}\\)\n\n\n\ninvL(L(f))\n\n\\(\\displaystyle e^{a t} \\theta\\left(t\\right)\\)\n\n\nwhere \\(\\theta(t)\\) is the name used by Sympy for the unit step function.\nMore transforms:\n\nomega = sympy.Symbol('omega', real=True)\nexp = sympy.exp\nsin = sympy.sin\ncos = sympy.cos\nfunctions = [1,\n         t,  \n         t**2,\n         exp(-a*t),\n         t*exp(-a*t),\n         t**2*exp(-a*t),\n         sin(omega*t),\n         cos(omega*t),\n         1 - exp(-a*t),\n         exp(-a*t)*sin(omega*t),\n         exp(-a*t)*cos(omega*t),\n         ]\nfunctions\n\n\\(\\displaystyle \\left[ 1, \\  t, \\  t^{2}, \\  e^{- a t}, \\  t e^{- a t}, \\  t^{2} e^{- a t}, \\  \\sin{\\left(\\omega t \\right)}, \\  \\cos{\\left(\\omega t \\right)}, \\  1 - e^{- a t}, \\  e^{- a t} \\sin{\\left(\\omega t \\right)}, \\  e^{- a t} \\cos{\\left(\\omega t \\right)}\\right]\\)\n\n\n\nFs = [L(f) for f in functions]\n\n\nFs\n\n\\(\\displaystyle \\left[ \\frac{1}{s}, \\  \\frac{1}{s^{2}}, \\  \\frac{2}{s^{3}}, \\  \\frac{1}{a + s}, \\  \\frac{1}{\\left(a + s\\right)^{2}}, \\  \\frac{2}{\\left(a + s\\right)^{3}}, \\  \\frac{\\omega}{\\omega^{2} + s^{2}}, \\  \\frac{s}{\\omega^{2} + s^{2}}, \\  \\frac{a}{s \\left(a + s\\right)}, \\  \\frac{\\omega}{\\omega^{2} + \\left(a + s\\right)^{2}}, \\  \\frac{a + s}{\\omega^{2} + \\left(a + s\\right)^{2}}\\right]\\)"
  },
  {
    "objectID": "transfer_functions.html#transfer-function-matrix",
    "href": "transfer_functions.html#transfer-function-matrix",
    "title": "Transfer Functions",
    "section": "Transfer function matrix",
    "text": "Transfer function matrix\nWe can apply the Laplace transform to the state equation of LTI systems\n\nWe start from the state equations and use the derivate operator of \\(s\\)\nWe also consider only the forced output of the system\n\n\\[\\dot{x} = Ax + Bu;\\; y = Cx\\]\n\\[sX = AX + BU;\\; Y = CX\\]\n\\[(sI-A)X=BU;\\; Y = CX\\]\n\\[X = (sI-A)^{-1}BU;\\] \\[Y = C(sI-A)^{-1}BU\\]\nMatrix \\(G(s)=C(sI-A)^{-1}B\\) is the Transfer Function Matrix(or Transfer Matrix).\nEach element of \\(G(s)\\) expresses the dynamic relation (there is \\(s\\)) between an input channel and an output channel.\n\\[\n\\begin{equation}\nG(s)=\n\\begin{bmatrix}\ng_{11}(s) & ... & g_{1m}(s)\\\\\n... & ... & ... \\\\\ng_{p1}(s) & ... & g_{pm}(s)\n\\end{bmatrix}\n\\end{equation}\n\\]\n\\(g_{ij}(s)\\): Transfer function between \\(u_j\\) and \\(y_i\\), each \\(g_{ij}(s)=\\frac{N(s)}{D(s)}\\)\n\nMatrix transfer function and poles\n\\[G(s) = C(sI-A)^{-1}B\\]\n\nwe need to calculate the inverse of a matrix:\n\n\\[A^{-1} = \\frac{Adjugate(A)}{det(A)}\\]\n\\[\\Downarrow\\]\n\\[G(s) = \\frac{CAdjugate(sI-A)B}{det(sI-A)}\\]\n\n\nPoles and zeros\n\nRoots of the denominator of \\(G(s)\\) (values of \\(s\\) that make \\(den(G)=0\\)) are called poles of the system.\nA pole is a value of \\(s\\) that causes \\(G(s)\\rightarrow \\inf\\) (e.g. \\(\\frac{1}{s}\\), when \\(s=0\\), \\(G(s)\\rightarrow \\inf\\))\nA zero is a value of \\(s\\) that causes \\(G(s)\\rightarrow 0\\) (e.g. \\(s\\), when \\(s=0\\), \\(G(s)=0)\\)\nThe poles of the system \\(G(s)\\) are, in general, a subset of the roots of the eq. \\(det(sI-A)=0\\)\nwhich are the same as the roots of the characteristic equation: \\(det(A-\\lambda I)=0\\)\nthis means that the poles of the system are a subset of the eigenvalue of the matrix \\(A\\).\n\n\n\n\n\n\n\nSidenote - Adjugate Matrix\nThe adjugate (or adjoint) of a square matrix is a matrix obtained by taking the transpose of the matrix of cofactors of the original matrix.\nTo compute the adjugate of a matrix A, we first calculate the matrix of cofactors of A. The matrix of cofactors of A is obtained by multiplying each element of A by the corresponding determinant of the submatrix obtained by removing the row and column of the element, and then multiplying by -1 if the sum of the row and column indices is odd.\nOnce we have the matrix of cofactors, we take its transpose to obtain the adjugate of A.\nThe adjugate of a matrix A is often denoted as adj(A) or adjug(A). It has many applications in linear algebra and is particularly useful for computing the inverse of a matrix, as the inverse of A can be expressed as:\n\\[A^-1 = \\frac{adj(A)}{det(A)}\\]\nwhere \\(det(A)\\) is the determinant of A.\nExample:\n\\[\nA = \\begin{bmatrix}\na & b \\\\\nc & d \\\\\n\\end{bmatrix}\n\\]\nAnd\n\\[\nAdj(A) = \\begin{bmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "transfer_functions.html#transfer-function-controllability-and-observability",
    "href": "transfer_functions.html#transfer-function-controllability-and-observability",
    "title": "Transfer Functions",
    "section": "Transfer function, controllability and observability",
    "text": "Transfer function, controllability and observability\n\nFor each \\(g_{ij}(s)\\), the poles of the transfer function are only the poles that are controllable from input \\(u_j\\) and observable from the output \\(y_i\\)."
  },
  {
    "objectID": "transfer_functions.html#single-input-single-output-siso",
    "href": "transfer_functions.html#single-input-single-output-siso",
    "title": "Transfer Functions",
    "section": "Single Input, Single Output (SISO)",
    "text": "Single Input, Single Output (SISO)\nIt is similar to what we had before, but now we only have one input and one output:\n\\[\\dot{x} = Ax + bu;\\; y = c^Tx\\]\n\\[G(s) = c^T(sI-A)^{-1}b\\]\nAgain, the transfer function only includes poles that are controllable and observable."
  },
  {
    "objectID": "transfer_functions.html#stability-in-classical-control",
    "href": "transfer_functions.html#stability-in-classical-control",
    "title": "Transfer Functions",
    "section": "Stability in Classical Control",
    "text": "Stability in Classical Control\n\nStability of a system is a property that describes how the system behaves in response to small perturbations or changes.\nA system is considered stable if it is able to maintain a desired state or behavior in the face of disturbances or variations in its environment.\nStability is the property of the system to stay close to its equilibrium \\(\\bar{x}\\) when perturbed.\nAn equilibrium is stable if\n\n\\[ \\forall \\epsilon >0, \\exists \\delta_\\epsilon: ||\\bar{x}|| \\le \\delta_\\epsilon \\Rightarrow ||x(t)||\\le\\epsilon,\\;\\; \\forall t\\ge 0\\]\n\nAsymptotic stability: the property of the system to go back to its equilibrium when perturbed: \\(||x(t)|| \\rightarrow \\bar{x}\\), for \\(t\\rightarrow \\inf\\).\nAssumes zero input: it is a definition w.r.t. state perturbations\n\nBIBO stability - A system is BIBO stable (Bounded Input, Bounded Output) if for each limited input, there is a limited output - For linear systems: BIBO stability if and only if poles of the transfer functions have Re \\(<0\\)\nA system is said to be BIBO stable if there exists a finite positive constant \\(M\\) such that, for any input \\(u(t)\\) that satisfies \\(|u(t)| \\le B\\) (i.e., bounded), the corresponding output \\(y(t)\\) satisfies \\(|y(t)| \\le M\\), where \\(M\\) and \\(B\\) are finite positive constants.\nIntuition: You can think of stability as meaning that your system’s response decays over time - instability implies the opposite is true, that a finite input will cause your system to respond with increasing amplitude over time, leading to an unstable system.\nNote that it is possible to be maginally stable: the system has an infinite, oscillatory response (e.g., sinusoid at constant frequency and amplitude).\nStability margins: - Even if your system is stable, unmodelled dynamics or parameter variations could make it unstable. - Stability margins make it possible to evaluate the robustness of the system to these variations.\nNote that too much stability is not necessarily a good thing: a system that is too stable means that will tend not to move from its equilibrium even when you desired it to do so."
  },
  {
    "objectID": "transfer_functions.html#system-stability-and-system-matrix",
    "href": "transfer_functions.html#system-stability-and-system-matrix",
    "title": "Transfer Functions",
    "section": "System stability and system matrix",
    "text": "System stability and system matrix\nSystem stability depends on the eigenvalues of the system matrix \\(A\\) or on the poles of the transfer function: - eigenvalues with \\(Re < 0\\), system is asymptotically stable - eigenvalues with \\(Re \\le 0\\), and \\(Re=0\\) have multiplicity 1 then the system is stable\nWhen we analyse the stability of linearised systems things can be more complicated. - If the linear system is aymptotically stable, then the non-linear system is stable (around the equilibrium): we can always move close enough to the equilibrium to be inside its region of stability)\n\nIf the linear system is marginally stable, we cannot say anything on the non-linear system (other non-linear dynamics become key)"
  },
  {
    "objectID": "transfer_functions.html#system-stability-and-transfer-function",
    "href": "transfer_functions.html#system-stability-and-transfer-function",
    "title": "Transfer Functions",
    "section": "System stability and transfer function",
    "text": "System stability and transfer function\n\nThe stability of a linear system may be determined directly from its transfer function.\nAn \\(n\\)-th order linear system is asymptotically stable only if all of the components in the response from a finite set of initial conditions decay to zero as time increases, or: \\[\n\\lim \\limits_{t\\rightarrow\\inf} \\sum_{i=1}^{n} C_ie^{p_it}\n\\]\n\nwhere the \\(p_i\\) are the system poles.\nFor linear systems modelled through their transfer function \\(G(s)=\\frac{N(s)}{D(s)}\\), we need to analyse the roots of the characteristic equation \\(D(s)=0\\): - the system is stable if all the roots of the characteristic equation have negative real part - the system is stable if all its poles have negative real part.\nIn order for a linear system to be stable, all of its poles must have negative real parts, that is they must all lie within the left-half of the s-plane. An “unstable” pole, lying in the right half of the s-plane, generates a component in the system homogeneous response that increases without bound from any finite initial conditions. A system having one or more poles lying on the imaginary axis of the s-plane has non-decaying oscillatory components in its homogeneous response, and is defined to be marginally stable."
  },
  {
    "objectID": "transfer_functions.html#more-structural-properties",
    "href": "transfer_functions.html#more-structural-properties",
    "title": "Transfer Functions",
    "section": "More structural properties",
    "text": "More structural properties\n\nStability only depends on the structure of the system (matrix A)\nOther aspects:\n\nWhat input do we need to take the system into a given state?\nGiven the output of a system, how can we determine the state?\n\n\n\nControllability\n\nA state \\(x\\) is controllable if exists an input \\(u\\) that can take the trajectory of the system from state \\(0\\) to \\(x\\) in finite time.\nA system is controllable if all its states are controllable\n\n\n\nObservability\n\nAn initial state is observable if it is possible to determine \\(x\\) measuring the system output \\(y\\)\nA system is observable if all its states are observable"
  },
  {
    "objectID": "transfer_functions.html#laplace-transforms-examples",
    "href": "transfer_functions.html#laplace-transforms-examples",
    "title": "Transfer Functions",
    "section": "Laplace Transforms Examples",
    "text": "Laplace Transforms Examples\n\nExample 1\n\\[F(s)=\\frac{10}{s^2+12s+40}\\]\n\npoles: \\(-6 \\pm \\sqrt{36-40} = -6 \\pm \\sqrt{-4} = -6 \\pm j2\\)\nlet’s call \\(\\alpha_1=-6 + j2\\), \\(\\alpha^*_1=-6 - j2\\)\n\nWe can then re-write: \\[\nF(s) = \\frac{10}{[s - (-6+j2)][s - (-6-j2)]} = \\frac{10}{(s-\\alpha_1)(s-\\alpha^*_1)}\n\\]\nTo calculate the inverse transform we need to find the coefficients:\n\\[\nF(s) = \\frac{10}{(s-\\alpha_1)(s-\\alpha^*_1)} = \\frac{A}{s-\\alpha_1} + \\frac{B}{s-\\alpha^*_1}\n\\]\nwhere \\(A\\) and \\(B\\) must be complex conjugated: \\(B=A^*\\) (or we would not get real cofficient in \\(F(s)\\)).\n\\[\nF(s) = \\frac{10}{(s-\\alpha_1)(s-\\alpha^*_1)} = \\frac{A}{s-\\alpha_1} + \\frac{A^*}{s-\\alpha^*_1}\n\\]\n\\[\nA = \\lim \\limits_{s\\rightarrow \\alpha_1}  \\frac{10}{(s-\\alpha_1)(s-\\alpha^*_1)} (s-\\alpha_1) = \\frac{10}{\\alpha_1-\\alpha_1^*} = \\frac{10}{2j \\text{Img(}\\alpha_1)}\n\\]\nthis is because \\(a + jb - (a - jb) = 2jb\\)\n\\[\nB = \\lim \\limits_{s\\rightarrow \\alpha^*_1}  \\frac{10}{(s-\\alpha_1)(s-\\alpha^*_1)} (s-\\alpha^*_1) = \\frac{10}{\\alpha^*_1-\\alpha_1} = -\\frac{10}{2j \\text{Img(}\\alpha_1)}\n\\]\nLet’s see now the inverse transform:\n\\[\nF(s) = \\frac{A}{s-\\alpha_1} + \\frac{A^*}{s-\\alpha^*_1} \\rightarrow \\mathcal{L^{-1}}  \\rightarrow Ae^{\\alpha_1t} + A^*e^{\\alpha^*_1t}\n\\]\nwhere we could explicit \\(\\alpha_1 = \\sigma + j\\omega\\), and \\(A=|A|e^{j\\Phi_A}\\), \\(A^*=|A|e^{-j\\Phi_A}\\)\n\\[\nf(t) = Ae^{\\alpha_1t} + A^*e^{\\alpha^*_1t} = |A|e^{j\\Phi_A}e^{(\\sigma +j\\omega)t} + |A|e^{-j\\Phi_A}e^{(\\sigma -j\\omega)t} = |A|e^{j\\Phi_A}e^{\\sigma t} e^{+j\\omega t} + |A|e^{-j\\Phi_A}e^{\\sigma t}e^{-j\\omega t}\n\\]\nwe can group things together:\n\\[\nf(t) = |A|e^{j\\Phi_A}e^{\\sigma t} e^{+j\\omega t} + |A|e^{-j\\Phi_A}e^{\\sigma t}e^{-j\\omega t} = |A|e^{\\sigma t} \\big [   e^{j\\Phi_A}e^{j\\omega t} + e^{-j\\Phi_A}e^{-j\\omega t}  \\big ] = |A|e^{\\sigma t} \\big [   e^{j(\\Phi_A+\\omega t)} + e^{-j(\\Phi_A+\\omega t)}  \\big ]\n\\]\nand since \\(\\frac{e^{jx}+e^{-jx}}{2}=cos(x)\\), we can write the above expression as:\n\\[\nf(t) = |A|e^{\\sigma t} \\big [   e^{j(\\Phi_A+\\omega t)} + e^{-j(\\Phi_A+\\omega t)}  \\big ] = 2|A|e^{\\sigma t}\\cos(\\Phi_A+\\omega t)\n\\]\nAs expected we have an oscillatory term that could decrease or increase depending on the value of \\(\\sigma\\) (real part of the poles).\n\n\n\nExample 2\n\\[F(s) = \\frac{100}{(s+1)(s^2+4s+13)}\\]\n\npoles: \\(s=-1, s=-2\\pm j3\\)\n\nWe can write:\n\\[\nF(s) = \\frac{100}{(s+1)(s^2+4s+13)} = \\frac{A_1}{(s+1)} + \\frac{B_1}{(s-\\alpha_1)} + \\frac{B^*_1}{(s-\\alpha^*_1)}\n\\]\nWhere we can calculate the value of the coefficient \\(A_1\\):\n\nfor the pole in \\(s=-1\\) \\[\nA_1 = \\frac{100}{s^2+4s+13} \\bigg |_{s=-1} = \\frac{100}{1-4+13} = 10\n\\]\nfor the pole in \\(s=\\alpha_1=-2+j3\\) \\[\nB_1 = (s-\\alpha_1)\\frac{100}{(s-\\alpha_1)(s+1)(s-\\alpha^*_1)} \\bigg |_{s=-2+j3} =\n\\frac{100}{(-2+j3+1)(-2+j3-(-2+j3)} = \\frac{100}{(-1+j3)6j} = \\frac{100}{-18-6j} \\\\\n= \\frac{100}{-18-6j}\\frac{-18+6j}{-18+6j} = \\frac{100}{|-18-6j|^2}(-18+6j) = -\\frac{100}{360}6(3-j)=-\\frac{5}{3}(3-j)\n\\]\nnote: \\(z^{-1} = \\frac{z^*}{|z|^2}\\)\n\nand the inverse transform is:\n\\[\nf(t) = 10e^{-t} + 2|B_1|e^{-2t}cos(3t+\\arg(B_1))\n\\]\n\n\nExample 3\n\\[\nF(s) = \\frac{s+3}{s(s+7)(s+1)} = \\frac{A}{s} + \\frac{B}{s+7} + \\frac{C}{s+1}  \n\\]\nNote that when we apply partial fraction decomposition we do:\n\\[\n\\bigg [ \\frac{A}{s} + \\frac{B}{s+7} + \\frac{C}{s+1} \\bigg ]  (s+7) = \\frac{A(s+7)}{s} + \\frac{B(s+7)}{s+7} + \\frac{C(s+7)}{s+1} \\bigg | _{s=-7} = B\n\\]\nso if we calculate the limits of both sides we can exactly what we want.\nwe can then\n\\[\nB = \\lim \\limits_{s\\rightarrow-7} \\frac{s+3}{s(s+7)(s+1)} (s+7) = \\frac{-4}{-7(-6)} = \\frac{-4}{42} = \\frac{-2}{21}\n\\]\nwe can then calculate the other terms:\n\\[\nA = \\lim \\limits_{s\\rightarrow 0} \\frac{s+3}{s(s+7)(s+1)} (s) = \\frac{3}{7}\n\\]\n\\[\nC = \\lim \\limits_{s\\rightarrow -1} \\frac{s+3}{s(s+7)(s+1)} (s+1) = \\frac{2}{-6} = -\\frac{1}{3}\n\\]\nNow we can do the inverse transform of:\n\\[\nF(s) = \\frac{s+3}{s(s+7)(s+1)} = \\frac{3}{7}\\frac{1}{s} - \\frac{2}{21}\\frac{1}{s+7} - \\frac{1}{3}\\frac{1}{s+1}\n\\]\n\\[\\Downarrow\\]\n\\[\nf(t) = \\frac{3}{7} 1(t) - \\frac{2}{21}e^{-7t} - \\frac{1}{3}e^{-t}\n\\]"
  },
  {
    "objectID": "system_response.html",
    "href": "system_response.html",
    "title": "System Response",
    "section": "",
    "text": "This notebook analyses how different systems might respond to different classes of inputs.\n\nDominal pole approximation\nFirst and second order systems\nSystem response and performance requirements"
  },
  {
    "objectID": "system_response.html#first-and-second-order-systems-and-the-dominant-pole-approximation",
    "href": "system_response.html#first-and-second-order-systems-and-the-dominant-pole-approximation",
    "title": "System Response",
    "section": "First and second order systems, and the dominant pole approximation",
    "text": "First and second order systems, and the dominant pole approximation\nThe number of poles of the system determines its order\nLower order (1st and 2nd) are well understood and easy to characterize (speed of system, oscillations, damping…), but his is much more difficult with higher order systems.\nOne way to make many such systems easier to think about is to approximate the system by a lower order system using a technique called the dominant pole approximation. This approximation assumes that the slowest part of the system dominates the response, and that the faster part(s) of the system can be ignored.\nNotes: - In a transfer function representation, the order is the highest exponent in the transfer function. In a proper system, the system order is defined as the degree of the denominator polynomial. - A proper system is a system where the degree of the denominator is larger than or equal to the degree of the numerator polynomial. - A strictly proper system is a system where the degree of the denominator polynomial is larger than the degree of the numerator polynomial.\nLet’s take some examples and verify how a few systems behaves.\nWe will use Sympy to do it, so we need to define some variables first.\n\nt, K, tau = sympy.symbols('t, K, tau',real=True, positive=True)\ns = sympy.Symbol('s')\nu = sympy.Heaviside(t) # step function\n\ndef L(f):\n    return sympy.laplace_transform(f, t, s, noconds=True)\ndef invL(F):\n    return sympy.inverse_laplace_transform(F, s, t)\n\n\ndef evaluate(f, times):\n    res = []\n    for time in times:        \n        res.append(f.evalf(subs={t:time}).n(chop=1e-5))\n    return res\n\nSide note:\nchop makes it possible to round float numbers off to a desired precision. Below is an example\n\nq1 = (-1.53283653303955 + 6.08703605256546e-17*sympy.I)\nq2 = (-1.53283653303955 + 6.08703605256546e-5*sympy.I)\n\nprint('q1:', q1.n(chop=1e-5))\nprint('q2:', q2.n(chop=1e-5))\n\nq1: -1.53283653303955\nq2: -1.53283653303955 + 6.08703605256546e-5*I\n\n\n\nDominant Poles\n\nWhen we have a BIBO stable system and every mode of the system is an exponentially dumped signal.\nBeyond the initial transient, the main effect is driven by the slowest modes\n\nFor example let’s consider these poles:\n\\[\np_1 = -0.1\n\\]\n\\[\np_{2,3} = -5 \\pm 8.66j\n\\]\n\\[\np_4 = -15.5\n\\]\n\\[\np_{5,6} = -20 \\pm 34.64j\n\\]\n\nfig, axs = plt.subplots(1,1,figsize=(15,7))\n\n\nplt.plot(-0.1,    0,                  marker='.', markersize=25, color='blue')\nplt.plot([-5, -5],   [-8.66, 8.66],   marker='.', markersize=25, linestyle='', color='orange')\nplt.plot(-15.5,   0,                  marker='.', markersize=25, color='green')\nplt.plot([-20, -20], [-34.64, 34.64], marker='.', markersize=25, linestyle='', color='red')\n\n\naxs.set_xlim([-24, 1])\naxs.set_ylim([-36, 36])\naxs.set_xlabel('Re')\naxs.set_ylabel('Img')\n\nplt.legend(['$p_1$', '$p_{2,3}$', '$p_4$', '$p_{5,6}$'])\nplt.grid()\n\n\n\n\nWe can now verify what output is associated to each of these poles.\nTo do this we define four systems that have poles at the positions depicted above:\n\\[ G_1(s) = \\frac{1}{(s + 0.1)} \\]\n\\[ G_2(s) = \\frac{100}{(s^2 + 10s + 100)} \\]\n\\[ G_3(s) = \\frac{15.5}{(s + 15.5)} \\]\n\\[ G_2(s) = \\frac{1600}{(s^2 + 40s + 1600)} \\]\n\nfig, axs = plt.subplots(1,4,figsize=(15,7))\n\ntime = np.linspace(0,10,100)\n\n# Define our systems\nG1 = 1/(s + 0.1)\n\nzeta, w_n = 0.5, 10\nG2 = w_n**2/(s**2 + 2*zeta*w_n*s + w_n**2)\n\nG3 = 15.5/(s + 15.5)\n\nzeta, w_n = 0.5, 40\nG4 = w_n**2/(s**2 + 2*zeta*w_n*s + w_n**2)\n\n# plot them\naxs[0].plot(time, evaluate(invL(G1), time), color='blue',   linewidth=3)\naxs[0].set_ylim(-.5, 1)\naxs[0].set_title('G1'), axs[0].set_xlabel('time (s)')\n\naxs[1].plot(time, evaluate(invL(G2), time), color='orange', linewidth=3)\naxs[1].set_ylim(-1, 1), axs[1].set_title('G2')\naxs[1].set_xlabel('time (s)')\n\n\naxs[2].plot(time, evaluate(invL(G3), time), color='green',  linewidth=3)\naxs[2].set_ylim(-.5, 1), axs[2].set_title('G3')\naxs[2].set_xlabel('time (s)')\n\naxs[3].plot(time, evaluate(invL(G4), time), color='red',    linewidth=3)\naxs[3].set_ylim(-1, 1)\naxs[3].set_title('G4'), axs[3].set_xlabel('time (s)')\n\nfig.tight_layout()\n\n\n\n\n\ninvL(G2)\n\n\\(\\displaystyle 11.5470053837925 e^{- 5.0 t} \\sin{\\left(8.66025403784439 t \\right)}\\)\n\n\n\nDepending on how far they are in the \\(s\\) plane their influence in time is different.\nBeyond the initial transient, the slower modes are those the matter\n\n\n\nReduction of a second order system to first order\nConsider an overdamped second order system:\n\\[\nG(s) = K \\frac{\\alpha\\beta}{(s+\\alpha)(s+\\beta)}\n\\]\nwhose step response is:\n\\[\nY(s) = K \\frac{\\alpha\\beta}{(s+\\alpha)(s+\\beta)}\\frac{1}{s}\n\\]\nand hence, applying partial fraction expansion:\n\\[\ny(t) = K \\bigg ( 1 - \\frac{\\beta e^{-\\alpha t} - \\alpha e^{-\\beta t}}{\\beta - \\alpha}\\bigg )\n\\]\nIf the magnitude of \\(\\beta\\) is large compared to \\(\\alpha\\) (typically if \\(\\beta/\\alpha > 5\\)), and assuming \\(s\\) is sufficiently small compared to \\(\\beta\\), we can write the following approximation for the transfer function (and as well as an approximation for the step response):\n\\[\nG(s) \\approx K \\frac{\\alpha\\beta}{(s+\\alpha)(\\beta)}\n\\]\nat which corresponds the following step response:\n\\[\ny(t) \\approx K \\bigg ( 1 - \\frac{\\beta e^{-\\alpha t}}{\\beta} \\bigg ) = K(1-e^{-\\alpha t})\n\\]\nNote that \\(G(0)\\) is unchanged and this is necessary to ensure that the steady state value remains the same.\n\n\nExample 1, second order system\nLet’s now consider when we have two cascaded systems:\n\\[ G_1 = \\frac{0.1}{(s+0.1)} \\]\n\\[ G_2 = \\frac{1}{(s+1)} \\]\n\nG1 = 0.1/(s+0.1)\n\nG2 = 1/(s+1)\n\nIn this case, one pole is 10 times bigger than the other.\nThe entire system is then\n\\[ G(s) = G_1(s)G_2(s) = \\frac{0.1}{(s+0.1)(s+1)} \\]\n\nG = G1*G2\nprint(G)\n\n0.1/((s + 0.1)*(s + 1))\n\n\nAnd now we can plot their output:\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.linspace(0,40,100)\n\nax.plot(time, evaluate(invL(G), time), linewidth=3)\nax.plot(time, evaluate(invL(G1), time), linewidth=3)\nax.plot(time, evaluate(invL(G2), time), linewidth=3)\n\nax.set_ylim(-.1, 0.3)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);\n\n\n\n\nAnd we can also calculate the step response:\n\\[\nY(s)= G(s) U(s) = G(s)\\frac{1}{s}\n\\]\nAnd this is the output of our system when we have a step input:\n\n\n\n\n\n\n\n\n\n\n# We can plot the figure above running this cell. \n# Unfortunately, github CI throws an error and I still need to figure out why.\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.linspace(0,40,100)\n\nax.plot(time, evaluate(invL(G*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G1*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G2*1/s), time), linewidth=3)\n\nax.set_ylim(-.1, 1.1)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);\n\n\nThe step response plot shows three plots: the blue plot is the exact response, the orange plot is the approximation assuming the pole at -0.1 dominates; the green plot is the approximation assuming that the pole at 1 dominates (which clearly it doesn’t because the plot is nowhere near the exact response)\nNote that the blue and orange plots are very close to each other, so the dominant pole approximation is a good one.\nThe exact response has two exponentials, a fast one with a relatively short time constant of \\(\\tau_1 = 1/(p_1)\\) and a much slower exponential with time constant \\(\\tau_2 = 1/(p_2)\\).\nIf we look at the overall resonse, the fast exponential comes to equilibrium much more quickly than the slow explonential. From the perspective of the overall response, the faster exponential comes to equilibrium (i.e., has decayed to zero) instantaneously compared to the slower exponential.\nTherefore, the slower response (due to the pole closer to the origin — at s=-0.1) dominates.\n\nThe second order system \\(G(s)\\) behaves approximately like \\(G_1(s)\\) which is the one with the slower dynamics: \\[G(s) = \\frac{0.1}{(s+0.1)(s+1)} \\approx \\frac{0.1}{(s+0.1)}\\]\n\n\nExample 2, Second order, Pole dominates but not as strongly\nLet’s now consider a different case and build an overall system composing the following two systems in cascade:\n\\[\nG_1(s) = \\frac{0.2}{s+0.2}\n\\]\n\\[\nG_2(s) = \\frac{1}{s+1}\n\\]\nIn this case, one pole is only 5 times bigger than the other.\nThe entire system is then\n\\[ G(s) = G_1(s)G_2(s) = \\frac{0.2}{(s+0.2)(s+1)} \\]\n\nG1 = 0.2/(s+0.2)\nG2 = 1/(s+1)\nG=G1*G2\nprint('G:', G)\n\nG: 0.2/((s + 0.2)*(s + 1))\n\n\nIf we now evaluate the step response for this system:\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.linspace(0,40,100)\n\nax.plot(time, evaluate(invL(G*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G1*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G2*1/s), time), linewidth=3)\n\nax.set_ylim(-.1, 1.1)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);\n\n\n\n\nThe approximation is still fairly good, but not not quite as good as when \\(p_1=0.1\\), as we would expect."
  },
  {
    "objectID": "system_response.html#example-3-second-order-neither-pole-dominates",
    "href": "system_response.html#example-3-second-order-neither-pole-dominates",
    "title": "System Response",
    "section": "Example 3, Second Order, neither pole dominates",
    "text": "Example 3, Second Order, neither pole dominates\nIf we instead have two poles quite close to each other (note that it is their relative location - or ratio of the pole locations - that is of interest to determine if the dominant approximation is applicable).\nLet’s consider:\n\\[\nG_1(s) = \\frac{1.25}{s+1.25}\n\\]\n\\[\nG_2(s) = \\frac{1}{s+1}\n\\]\nand the final system is:\n\\[\nG(s) = G_1(s)G_2(s) = \\frac{1.25}{(s+1)(s+1.25)}\n\\]\n\nG1 = 1.25/(s+1.25)\nG2 = 1/(s+1)\nG = G1*G2\nprint('G:', G)\n\nG: 1.25/((s + 1)*(s + 1.25))\n\n\nIf we now calculate and plot the step response:\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.linspace(0,10,100)\n\nax.plot(time, evaluate(invL(G*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G1*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G2*1/s), time), linewidth=3)\n\nax.set_ylim(-.1, 1.1)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);\n\n\n\n\nIn this case, the two poles are very close to each other and the dominat pole approximation cannot be applied.\nThe three plots above are different: the blue (exact) response is not at all close the approximation of the approximation in which a first order pole dominates (orange or green).\nNote that the step response for this case has a different time scale than the other two.\n\nSimplifying Higher Order Systems\nThe dominant pole approximation can also be applied to higher order systems. Here we consider a third order system with one real root, and a pair of complex conjugate roots.\n\n\n\n\n\n\n\n\n\n\nIn this case the test for the dominant pole compare \\(\\alpha\\) against \\(\\zeta \\omega_0\\)\n\nNote: sometimes \\(\\zeta \\omega_0\\) is also written \\(\\xi \\omega_n\\) - they are the same thing\n\nThis is because \\(\\zeta \\omega_0\\) is the real part of the complex conjugate root\nWe only compare the real parts of the roots when determining dominance because it is the real part that determines how fast the response decreases.\nNote, that as with the previous case, the steady state gain \\(H(0)\\) of the exact system and the two approximate systems are equal. This is necessary to ensure that the final value of the step response (which is determined by \\(H(0)\\) is unchanged)."
  },
  {
    "objectID": "system_response.html#example-5-third-order-real-pole-dominates",
    "href": "system_response.html#example-5-third-order-real-pole-dominates",
    "title": "System Response",
    "section": "Example 5, Third order, Real Pole Dominates",
    "text": "Example 5, Third order, Real Pole Dominates\nLet’s now consider the following system\n\\[G(s) = \\frac{\\alpha \\omega_n^2}{(s+\\alpha)(s^2+2\\xi\\omega_n s + \\omega_n^2)} = \\frac{0.1\\cdot17}{(s+0.1)(s^2+2s+17)}\\]\n\\[ \\approx G_dp(s) = \\frac{\\alpha}{s+\\alpha} = \\frac{0.1}{s+0.1}\\]\nThe second order poles are at \\(s=-1 \\pm j4\\) (\\(\\xi\\)=0.24 and \\(\\omega_n=\\sqrt{17}\\)=4.1) and the real pole is at \\(s = -\\alpha = -0.1\\).\nAs before we write the entire system as a cascade of two systems\n\\[\nG_1(s)=\\frac{\\alpha}{s + \\alpha}\n\\]\nand\n\\[\nG_2(s)=\\frac{\\omega_n^2}{s^2+2\\xi\\omega_n s + \\omega_n^2}\n\\]\nwith\n\\[\nG(s) = G_1(s)G_2(s)\n\\]\n\nalpha = 0.1 # real pole that dominates\n\n\nG1 = 0.1/(s+0.1)\nG2 = 17/(s**2+2*s+17)\n\nG = G1*G2\nprint('G:', G)\n\nG: 1.7/((s + 0.1)*(s**2 + 2*s + 17))\n\n\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.arange(0,40,0.2)\n\n# The next line might is commented out only because github CI failes. \n# The response of G matches up the reponse of G1.\nax.plot(time, evaluate(invL(G*1/s), time), linewidth=5) \n\nax.plot(time, evaluate(invL(G1*1/s), time), linewidth=3)\nax.plot(time, evaluate(invL(G2*1/s), time), linewidth=3)\n\nax.set_ylim(-.1, 1.6)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);\n\n\n\n\n\nThe blue (exact) and orange (due to pole at \\(s=-\\alpha=-0.1\\)) lines are very close since that is the pole that dominates in this example.\nThe green line (corresponding to \\(G_2(s)\\), and that would correspond to the case where the second order poles are assumed to dominate) is obviously a bad approximation and not useful, as expected.\n\nDominant pole approximation can simplify systems analysis\nThe dominant pole approximation is a method for approximating a (more complicated) high order system with a (simpler) system of lower order if the location of the real part of some of the system poles are sufficiently close to the origin compared to the other poles.\nLet’s consider another example: \\[\nG1 = \\frac{150.5}{s+150.5}\n\\]\nwith pole at -150.5\n\\[\nG2 = \\frac{100}{s^2 + 10s +100}\n\\]\nwith poles at: \\(p = -5 \\pm 8.66j\\)\n\nG1 = 150.5/(s + 150.5)\nprint('G1: ', G1)\n\nzeta, w_n = 0.5, 10\nG2 = w_n**2/(s**2 + 2*zeta*w_n*s + w_n**2)\nprint('G2: ', G2)\n\nG = G1*G2\nprint('G: ', G)\n\nG1:  150.5/(s + 150.5)\nG2:  100/(s**2 + 10.0*s + 100)\nG:  15050.0/((s + 150.5)*(s**2 + 10.0*s + 100))\n\n\nThe step response is:\n\n\n\n\n\n\n\n\n\nWe can obtain the figure above uncommenting and running the cell below.\n\nfig, ax = plt.subplots(1,1,figsize=(8,5))\n\ntime = np.arange(0, 4, 0.05)\n\nax.plot(time, evaluate(invL(G*1/s), time),  linewidth=7, color = 'blue')\nax.plot(time, evaluate(invL(G1*1/s), time), linewidth=4, color = 'orange')\nax.plot(time, evaluate(invL(G2*1/s), time), linewidth=3, color = 'green')\n\nax.set_ylim(-.1, 1.6)\nax.set_xlabel('time (s)')\nax.grid()\nfig.legend(['y_g(t)', 'y_g1(t)', 'y_g2(t)']);"
  },
  {
    "objectID": "system_response.html#first-order-and-second-order-systems",
    "href": "system_response.html#first-order-and-second-order-systems",
    "title": "System Response",
    "section": "First-order and second-order systems",
    "text": "First-order and second-order systems\n\nGiven that the main effect is driven by the slowest modes, then the dynamics of the system can be approximated by the modes associated to the dominant poles\nTypically this ends up being a first-order system (real dominant pole) or a second-oder system (complex-conjugated poles), possibly with a constant delay\nIt is hence important to understand the response of the first-order and second-order systems as they can be representive of a broader class.\n\n\nStep response of first order systems\nA first order system has form: \\[G(s) = \\frac{1}{s+p} = \\frac{\\tau}{1+\\tau s}\\]\nwith pole in $s = -p = $\n\nLet’s hence consider\n\\[\nG(s) = \\frac{1}{1+\\tau s}\\frac{1}{s}\n\\]\nwhere we would like to have \\(G(0)=1\\).\nGiven an input: \\[u(t) = 1(t)\\]\nthe output of the system is: \\[ y(t) = 1 - e^{-\\frac{t}{\\tau}}\\]\nThis is obtained:\n\\[\\frac{1}{1+\\tau s}\\frac{1}{s} = \\frac{1/\\tau}{s + 1/\\tau} \\frac{1}{s} = \\frac{A}{s + 1/\\tau} + \\frac{B}{s}\\]\nwhere \\(A = -1\\), \\(B = 1\\)\nWe can also verify it with Sympy:\n\nF = 1/(K*s + 1)*1/s\ninvL(F)\n\n\\(\\displaystyle 1 - e^{- \\frac{t}{K}}\\)\n\n\nWe can then plot the output \\(y(t)\\) for a particular value of the time constant \\(\\tau=2\\):\n\ntime = np.linspace(0,20,100)\n\ntau = 2 # define the time constant\n\ny_t = 1 - np.exp(-time/tau)\n\n\nfig = plt.figure(figsize=(10,5))\n\nplt.plot(time, y_t, linewidth=3)\nplt.title('Step response')\nplt.xlabel('time (s)')\nplt.grid()\n\n\n\n\nWe call:\n\n\\(\\tau\\): time constant - Characterises completely the response of a first order system\nSettling time: the time it takes to get to 95% (or other times 90%, 98%) of the steady state value \\(y(t)=1\\):\n\n\\(t_s = -\\tau ln(0.05)\\)\nThis is because: \\[\ny(t) = 1 - e^{-\\frac{t}{\\tau}} \\Rightarrow 1 - y(t) = e^{-\\frac{t}{\\tau}} \\Rightarrow \\ln(1-y(t)) = -\\frac{t}{\\tau}, \\;\\; \\text{when} \\;\\; y(t)=0.95 \\Rightarrow t = -\\tau ln(0.05)\n\\]\n\n\n\n#y_t = -1 # Desired steady state value\nfinal_value_pc = 0.95 # percentage of y_t\n\n# final_value = 1 - np.exp(-t/tau)\n# np.log(1-final_value) = -t/tau\n\nt_s = -tau*np.log(1-final_value_pc)\n\nprint('Time to get to {}% of final value {:.1f}: {:.2f}s'.format(final_value_pc, y_t[-1], t_s))\n\nTime to get to 0.95% of final value 1.0: 5.99s\n\n\nNote that, after \\(\\tau\\) seconds, the system gets to the \\(64\\%\\) of the final value\n\n\nTo recap\n\nWe have defined two times in the response of a first order system:\n\nSettling time \\(t_s\\): time to get to 0.95% of the final value\nTime constant \\(\\tau\\), which is the time to get to the 64% of the final value.\n\n\nWe can plot them, together with the step response of the system:\n\\[ y(t) = 1 - e^{-\\frac{t}{\\tau}}\\]\n\nfig = plt.figure(figsize=(10,5))\n\n# step response\nplt.plot(time, y_t, linewidth=3)  \n\n# tau - time constant\nplt.plot(tau, 1 - np.exp(-tau/tau), marker='.', markersize=15)\n\n# settling time\nplt.plot(t_s, final_value_pc*y_t[-1], marker='.', markersize=15)\n\n\nfig.legend(['Step response', 'Tau seconds', '$t_s$ seconds'], loc='upper left')\nplt.title('Step response')\nplt.xlabel('time (s)')\nplt.grid()\n\n\n\n\n\n\n\nStep response of second order systems\nGiven a system: \\[G(s) = \\frac{1}{\\frac{s^2}{w_n^2} + \\frac{2\\xi}{w_n}s + 1}\\]\nwith $ 0 < < 1 $\n\n\\(\\xi\\) is called damping ratio\n\\(\\omega_n\\) (sometime also called \\(\\omega_0\\)) is the natural frequency of the system.\n\nThe system has two conjugate-complex poles:\n\\[s = -\\xi w_n \\pm j \\sqrt{1-\\xi^2} \\]\nAs before, we can calculate the step reponse:\n\ninput \\[u(t)=1(t)\\]\noutput \\[y(t) = 1 - \\frac{1}{\\sqrt{1-\\xi^2}} e^{-\\xi w_n t} \\sin\\Big( w_n\\sqrt{1-\\xi^2}t + arccos(\\xi)\\Big)\\]\n\nNote: Proving this is a useful exercise\nAs before we can plot it.\nWe will do it for a set of damping ratios: \\(\\xi = [0.1, 0.25, 0.5, 0.7]\\) and for a fixed value of \\(\\omega_n=1\\).\n\nxis = [0.1, 0.25, 0.5, 0.7] # damping ratios\nwn = 1 # natural frequency.\n\n\nfig = plt.figure(figsize=(15,5))\nlegend_strs = []\nfor xi in xis:\n    y_t = 1 - (1/np.sqrt(1-xi**2)) * np.exp(-xi*wn*time) * np.sin(wn*np.sqrt(1-xi**2)*time+np.arccos(xi))\n    plt.plot(time, y_t, linewidth=4)\n\n\n# this part is to have a legend\nfor xi in xis:\n    legend_strs.append('xi: ' + str(xi))\n\nfig.legend(legend_strs, loc='upper left')\n\nplt.xlim(0, 20)\nplt.title('Step response')\nplt.xlabel('time (s)')\nplt.grid()\n\n\n\n\nThere are a few notable points that we can identify for the response of a second order system:\n\nRise time (to 90%): \\[t_r \\approx \\frac{1.8}{w_n} \\]\nMaximum overshoot: \\[ S \\% = 100 e^{\\Large -\\frac{\\xi\\pi}{\\sqrt{1-\\xi^2}}} \\]\nTime of Maximum overshoot (Peak time): \\[t_{max} = \\frac{\\pi}{w_n\\sqrt{1-\\xi^2}} \\]\nSettling time (within a desired interval, e.g., 5%): \\[t_{s} \\approx -\\frac{1}{\\xi w_n}ln(0.05) \\]\nOscillation period: \\[T_{P} = \\frac{2\\pi}{w_n\\sqrt{1-\\xi^2}} \\]\n\nWe can connect \\(\\zeta\\) and \\(w_n\\) to \\(S, T_p, t_{max}, t_s\\) with two important caveats: - some of the relationships are approximate - additional poles and zeros will change the results, so all of the results should be viewed as guidelines.\nWe can obtain the previous relationships using the step response of \\(G(s)\\) (see above)\n\nRise time, peak time, and settling time yield information about the speed of the transient response.\nThis information can help a designer determine if the speed and the nature of the response do or do not degrade the performance of the system.\n\nFor example: - The speed of an entire computer system depends on the time it takes for a hard drive head to reach steady state and read data - Passenger comfort depends in part on the suspension system of a car and the number of oscillations it goes through after hitting a bump.\nLet’s see where they are on the plot for one specific choise of \\(\\xi=0.2\\) and \\(\\omega_n=1\\).\n\nxi = 0.2\nwn = 1\n\n# Maximum overshoot\nS = np.exp(-xi*3.14/(np.sqrt(1-xi**2)))\n# Time of maximum overshoot\nt_max = 3.14/(wn*np.sqrt(1-xi**2))\n# Settling time within 5%\nt_s = -1/(xi*wn)*np.log(0.05)\n# Period of the oscillations\nTp = 2*3.14/(wn*np.sqrt(1-xi**2))\n\n# time vector\ntime = np.linspace(0,30,100)\n\n\n# create the figure\nfig = plt.figure(figsize=(15,5))\n\n# overall response y(t)\ny_t = 1 - (1/np.sqrt(1-xi**2)) * np.exp(-xi*wn*time) * np.sin(wn*np.sqrt(1-xi**2)*time+np.arccos(xi))\nplt.plot(time, y_t, linewidth=4)\n\n# maximum overshoot\nplt.plot([t_max, t_max], [y_t[-1], y_t[-1]+S], marker='.', markersize=12)\nplt.text(t_max+0.2, y_t[-1]+S/2, 'S', fontsize=15) # text box\n\n# Time of maximum overshoot\nplt.plot([t_max, t_max], [0, 1], markersize=12, linestyle='--')\nplt.text(t_max+0.2, 0.0, 't_max', fontsize=15) # text box\n\n# Let's also plot +-0.05 boundary lines around y(t)\nplt.plot([0, time[-1]], [1-0.05, 1-0.05], linestyle='--', color='k')\nplt.plot([0, time[-1]], [1+0.05, 1+0.05], linestyle='--', color='k')\n\n# Settling time within a desired +-0.05 interval\nplt.plot([t_s, t_s], [0, 1.1], linestyle='--')\nplt.text(t_s+0.2, 0.0, 't_s', fontsize=15) # textbox\n\n# Oscillation period (this is only to show it on the plot)\nplt.plot([t_max, t_max+Tp], [y_t[-1]+S+0.1, y_t[-1]+S+0.1], \n         marker='.', linestyle='--', color='k', markersize=10)\nplt.plot([t_max+Tp, t_max+Tp], [y_t[-1], y_t[-1]+S+0.1], linestyle='--', color='k')\nplt.text((t_max+t_max+Tp)/2, y_t[-1]+S, 'Tp', fontsize=15)\n\nfig.legend(['xi:{:.1f}'.format(xi)], loc='upper left')\n\nplt.title('Step response')\nplt.xlabel('time (s)')\nplt.grid()\n\n\n\n\nHow do you find (some of) the previous expressions:\n\nEvaluation of \\(t_{max}\\), differentiating \\(y(t)\\) and finding the first zero crossing after \\(t = 0\\).\nEvaluation of \\(S\\%\\): max value is found by evaluating \\(y(t)\\) at the peak time: \\(y(t_{max})\\) and comparing it with \\(y_{final}=1\\) (for unit step input)\nEvaluation of \\(t_s\\): find the time for which \\(y(t)\\) reaches and stays within \\(5\\%\\) of the steady-state value \\(y_{final}=1\\) (note that our expression is an approximation)."
  },
  {
    "objectID": "part_1_recap.html",
    "href": "part_1_recap.html",
    "title": "Part 1 Recap",
    "section": "",
    "text": "Please fill up the questionnaire at: https://esami.unipi.it"
  },
  {
    "objectID": "part_1_recap.html#jupyter-and-python",
    "href": "part_1_recap.html#jupyter-and-python",
    "title": "Part 1 Recap",
    "section": "Jupyter and Python",
    "text": "Jupyter and Python\n\nInstalling Anaconda and dependencies to run the notebooks\n\nInstall Anaconda: Anaconda Download Page\nRefer to the Anaconda documentation for more information here\nOpen a terminal window:\n\nWindows: From the Start menu, search for and open “Anaconda Prompt”\nMacOS/Linux: Click the terminal icon.\n\nOnce in a terminal,\n\ncreate an environment: conda create --name env-automatic-control python=3.9\nactivate it: conda activate env-automatic-control\n\nInstall everything that is needed\nJupyter notebook:\nconda install -c conda-forge notebook\nStandard engineering libraries:\npython -m pip install matplotlib numpy pandas\nconda install numpy scipy matplotlib # if not yet installed\nconda install -c conda-forge control\nconda install -c anaconda sympy\n\nYou are set up!\n\n\nGoing back to work (opening your Jupyter notebooks)\n\nOpen a terminal window:\n\nWindows: From the Start menu, search for and open “Anaconda Prompt”\nMacOS/Linux: Click the terminal icon.\n\nMove to yur project folder (e.g. cd /Users/<USERNAME>/MyProject/)\n(if you need to find out how to find your folder path in Windows read below: “Note: To find out where is your folder in Windows”)\nActivate your conda environment: conda activate env-automatic-control\nRun: jupyter notebook\n\nA new tab in your browser should open up automatically showing the jupyter home folder (your current folder). If it does not open, open your favourite brower and type: localhost:8888 in the browser address bar. Sometime jupyter requires a token - this is automatically created when jupyter is started. Look back to your terminal window right after the command jupyter notebook there should be a line similar to:\nTo access the notebook, open this file in a browser:\n    file:///Users/<YOUR_USERNAME>/Library/Jupyter/runtime/nbserver-73848-open.html\nOr copy and paste one of these URLs:\n    http://localhost:8888/?token=66781622f3ed474ccf1f0f61e5708b035acfcf4e41cc7c8a\n or http://127.0.0.1:8888/?token=66781622f3ed474ccf1f0f61e5708b035acfcf4e41cc7c8a\nCopy and paste one of the last two addresses in your browser to start your jupyter session.\nYou are ready to start coding!\n\n\n\n\n\n\nNote: To find out where is your folder in Windows: - Open Windows Explorer and go inside the folder you need - Right click on the address path and right-click - Select “copy address as text” (see picture below) - Go back to your Anaconda prompt and paste it (remember to type cd first!)\n\n\n\n\n\n\n\nReferences\n\nInstalling Jupyter Software\nPython Control Library"
  },
  {
    "objectID": "part_1_recap.html#the-control-problem",
    "href": "part_1_recap.html#the-control-problem",
    "title": "Part 1 Recap",
    "section": "The Control Problem",
    "text": "The Control Problem\n\nSystems and control systems\nThree problems:\n\nsystem identification problem\nsimulation problem\ncontrol Problem\n\nWhy do we need feedback control"
  },
  {
    "objectID": "part_1_recap.html#lti-systems",
    "href": "part_1_recap.html#lti-systems",
    "title": "Part 1 Recap",
    "section": "LTI systems",
    "text": "LTI systems\n\nHomogeneity\nSuperposition (Additivity)\nTime Invariance\nImpulse Response > LTI systems can be characterised by their response to an impulse function (the output of the system when presented with a brief input signal)"
  },
  {
    "objectID": "part_1_recap.html#transfer-functions",
    "href": "part_1_recap.html#transfer-functions",
    "title": "Part 1 Recap",
    "section": "Transfer Functions",
    "text": "Transfer Functions\n\nLaplace domain representation of a system\n\n\nDefinition: Transfer function is the Laplace transform of the impulse response of a linear, time-invariant system with a single input and single output when you set the initial conditions to zero.\n\n\nState space vs Transfer functions\nLaplace Transform\n\nThe Laplace transform is important because it is a tool for solving differential equations: it transforms linear differential equations into algebraic equations, and convolution into multiplication!\n\nDerivatives and integrals become algebric operations\n\nFrom a system perspective, the Transfer function expresses the relation between the Laplace Transform of the input and that of the output:\n\\[ Y(s)=\\mathbf{c}^T(s\\mathbf{I}-\\mathbf{A})^{-1} \\mathbf{b} U(s) \\]\n\nProperties of the Laplace Transform (relationships between time and frequency)\n\n\\(\\frac{df}{dt}(t) \\rightarrow sF(s)\\)\n\\(af(t)\\) + \\(bg(t) \\rightarrow aF(s)+bG(s)\\)\n\nLaplace transforms of known functions:\n\n\\(\\delta(t) \\rightarrow 1\\)\n\\(1(t) \\rightarrow \\frac{1}{s}\\)\n\\(e^{-at} \\rightarrow \\frac{1}{s+a}\\)\n\n…"
  },
  {
    "objectID": "part_1_recap.html#stability-and-other-properties",
    "href": "part_1_recap.html#stability-and-other-properties",
    "title": "Part 1 Recap",
    "section": "Stability and other properties",
    "text": "Stability and other properties\n\nSystem \\(G(s)\\) is asymptotically stable if all its poles have \\(Re<0\\)\nSystem \\(G(s)\\) is stable if all its poles have G(s) \\(Re \\le 0\\) and if those with \\(Re=0\\) have multiplicity one.\nSystem \\(G(s)\\) is unstable if it has at least one pole with \\(Re>0\\) or with \\(Re=0\\) and multiplicity \\(\\ge 2\\).\nBIBO stability:\n\nA system is BIBO stable (Bounded Input, Bounded Output) if for each limited input, there is a limited output\nFor linear systems: BIBO stability if and only if poles of the transfer functions have \\(Re<0\\)\n\nStability of a system in a state space representation\nControllability (Reachability)\nObservability"
  },
  {
    "objectID": "part_1_recap.html#block-diagrams",
    "href": "part_1_recap.html#block-diagrams",
    "title": "Part 1 Recap",
    "section": "Block diagrams",
    "text": "Block diagrams\n\nStandard representation of interconnected systems and subsystems using Transfer Functions\nBlock diagram algebra (serie, parallel, feedback) and equivalent block diagrams\nZero-poles cancellations\nImpact on stability and other properties"
  },
  {
    "objectID": "part_1_recap.html#system-response",
    "href": "part_1_recap.html#system-response",
    "title": "Part 1 Recap",
    "section": "System Response",
    "text": "System Response\n\nDominant pole approximation\nFirst-order and second-order systems\n\nFirst-order systems: step response and performance\ntime constant (\\(\\tau\\)) - Characterises completely the response of a first order system\nsettling time: \\(t_s = -\\tau ln(0.05)\\) (time to get to 95% of steady state)\nSecond order systems: step response and performance\nMore parameters to define performance: max overshoot, time of max overshoot, settling time, oscillation period"
  },
  {
    "objectID": "part_1_recap.html#frequency-response",
    "href": "part_1_recap.html#frequency-response",
    "title": "Part 1 Recap",
    "section": "Frequency response",
    "text": "Frequency response\n\nAssuming that the system is asymptotically stable,\nIf we input a sinusoid \\(Asin(\\omega t)\\)\nThe output of the system converges $y(t)=A|G(j)|sin(t + G(j) ) $\n\\(G(j\\omega)\\) is the frequency response\nIf we know magnitude and phase of \\(G(j\\omega)\\) (for varying \\(\\omega\\)) then we know how the system behaves for all sinusoidal inputs with different driving frequencies."
  },
  {
    "objectID": "part_1_recap.html#bode-plots",
    "href": "part_1_recap.html#bode-plots",
    "title": "Part 1 Recap",
    "section": "Bode plots",
    "text": "Bode plots\n\nFrequency Response and Bode Plots\n\nanalysing the gain and the phase shift across the full frequency spectrum\ngain diagram and phase diagram\n\nLeverages the properties of transfer functions and of logarithms:\n\nThe transfer function of linear systems are polynomial fractions in \\(s\\)\nIt is always possible to factor polynomials in terms of their roots (zeros and poles)\nEach polynomial is the product of first-order or second-order (potentially with multiplicity \\(>\\) 1)\nTo build the frequency response of the system is useful to have simple rules to represent each term\nThe logarithm of a product is equal to a sum of logarithms\nremember to convert gain to dB: \\(|G(j\\omega)|_{dB}=20log_{10}|G(j\\omega)|\\)\n\nSketching Bode Plots\n\nGiven a transfer function \\(G(s)\\) we can obtain the (steady state) frequency response setting \\(s=j\\omega \\rightarrow G(j\\omega)\\)\n$G(j) = + j $\nGain: \\(\\sqrt{\\text{real}^2 + \\text{img}^2} = |G(j\\omega)|\\)\nPhase: \\(atan2(\\text{img}, \\text{real}) = arg(G(j\\omega)) = \\angle G(j\\omega)\\)\nOur building blocks:\n\nconstant gain\nintegrator\nsimple pole\ncomplex poles\nspecular behaviour for the zeros (properties of the logarithms)\nTransform the trasnfer function to the Bode form before drawing the Bode plots!"
  },
  {
    "objectID": "part_1_recap.html#final-value-theorem-and-steady-state-error",
    "href": "part_1_recap.html#final-value-theorem-and-steady-state-error",
    "title": "Part 1 Recap",
    "section": "Final value theorem and steady state error",
    "text": "Final value theorem and steady state error\n\nMeaning of final value\nTheorem: for an asymptotically stable LTI system: $ {t } x(t) = {s } sX(s)$\nSystem type: number of poles at the origin\nAnalysis of the final value for different system types and different inputs\nFor a feedback system: the final value of the error is a much better indicator of the performance of our controller"
  },
  {
    "objectID": "part_1_recap.html#diving-deeper-into-the-feedback-loop",
    "href": "part_1_recap.html#diving-deeper-into-the-feedback-loop",
    "title": "Part 1 Recap",
    "section": "Diving deeper into the feedback loop",
    "text": "Diving deeper into the feedback loop\n\nIncluded load disturbances and measurement noise in the feedback loop\nSensitivity function (feedback and disturbances / low frequency specifications)\nComplementary sensitivity function (feedback and measurements / high frequency specifications)\nDesign requirements on the loop gain function \\(|R(s)G(s)|\\)"
  },
  {
    "objectID": "part_1_recap.html#routh-hurwitz-criterion",
    "href": "part_1_recap.html#routh-hurwitz-criterion",
    "title": "Part 1 Recap",
    "section": "Routh-Hurwitz Criterion",
    "text": "Routh-Hurwitz Criterion\n\nNecessary and sufficient condition to analyse the stability of the systems without solving for the roots explicitely\nUsage can be extended to beyond stability\n\nGain margins\nDeal with requirement on the speed of the response (time to half)"
  },
  {
    "objectID": "part_1_recap.html#coming-up-next",
    "href": "part_1_recap.html#coming-up-next",
    "title": "Part 1 Recap",
    "section": "Coming up next",
    "text": "Coming up next\n\nPolar plots and the Nyquist plot\nStability margins\nRoot locus\nPID controllers\nLead-lag/lad-lead compensators"
  },
  {
    "objectID": "routh_hurwitz_criterion.html",
    "href": "routh_hurwitz_criterion.html",
    "title": "Routh-Hurwitz Criterion",
    "section": "",
    "text": "Given a system with transfer function \\(G(s) = \\frac{N(s)}{D(s)}\\): in order for a system to be stable, all the roots of its characteristic equation \\(D(s)=0\\) must have \\(Re < 0\\)\nThe roots of the characteristic equation \\(D(s)=0\\) are the poles of the transfer function \\(G(s)\\).\n\nFor example:\n\\[G(s) = \\frac{1}{s+a}\\]\nwhose inverse Laplace transform to get the time domain representation is:\n\\[\\mathcal{L}^{-1}(G(s)) = e^{-at} = e^{st}\\]\n\nif \\(a > 0\\), the system is stable: signals go to zero as time goes to infinity.\nif \\(a < 0\\), the system is unstable: the response of the system goes to infinity.\n\nWe can have more complex transfer functions:\n\\[G(s) = \\frac{1}{s+a}\\frac{1}{s+b}\\frac{1}{s+c}...\\]\nand we know that we can always simplify a transfer function using partial fraction expansion:\n\\[G(s) = \\frac{A}{s+a} + \\frac{B}{s+b} + \\frac{C}{s+c}...\\]\nand\n\\[\\mathcal{L}^{-1}(G(s)) = Ae^{-at} + Be^{-bt} + Ce^{-ct}...\\]\nIf there is one root that is unstable, the system is unstable.\n\nWe know that we can determine the stability of the system calculating the roots of the characteristic equation\nCalculating the roots of a polynomial for \\(n>2\\) is time consuming, and possibly even impossible in closed form\n\n\\[12s^5+14s^4+3s^3+s^2+16s+11=0\\]\n\nWe would like to determine stability (hence the roots of \\(D(s)=0\\)) without solving for the roots directly\nThis is where the Routh-Hurwitz criterion can help us\n\n\nRouth–Hurwitz stability criterion > All the roots of a polinomial have \\(Re<0\\) if and only if a certain set of algebraic combinations (i.e. fill out the RH array) of its coefficients have the same signs\n\nThe Routh–Hurwitz stability criterion is a mathematical test that is a necessary and sufficient condition for the stability of a linear time invariant (LTI) control system\nDetermine whether all the roots of the characteristic polynomial of a linear system have negative real parts\nThe importance of the criterion is that the roots \\(p\\) of the characteristic equation of a linear system with negative real parts represent solutions \\(e^{pt}\\) of the system that are stable (bounded).\nThe criterion provides a way to determine if the equations of motion of a linear system have only stable solutions, without solving the system directly\n\nGiven:\n\\[G(s) = \\frac{N(s)}{D(s)}\\]\n\nIf all the signs of the coefficients are NOT the same, then the system is unstable\n\ne.g. \\(s^5+3s^3-4s^2+s+1\\) \\(\\rightarrow\\) unstable\n\nIf all the signs are the same, then the system can be stable or unstable\n\nFor example:\n\\[G(s) = \\frac{1}{(s^2-s+4)(s+2)(s+1)}\\]\nthe roots are \\(0.5\\pm j1.9365\\) (unstable roots), \\(-2, -1 \\Rightarrow\\) we have two roots with \\(Re > 0\\).\nIf we write the characteristic equation however:\n\\[s^4+2s^3+3s^2+10s+8 = 0\\]\n\nAll coefficients have the same sign. We need to use the RHC and populate the Routh array"
  },
  {
    "objectID": "routh_hurwitz_criterion.html#routh-hurwitz-array",
    "href": "routh_hurwitz_criterion.html#routh-hurwitz-array",
    "title": "Routh-Hurwitz Criterion",
    "section": "Routh-Hurwitz array",
    "text": "Routh-Hurwitz array\nA tabular method can be used to determine the stability when the roots of a higher order characteristic polynomial are difficult to obtain.\nFor an \\(n\\)th-degree polynomial\n\\[\nD(s)=a_{n}s^{n}+a_{{n-1}}s^{{n-1}}+\\cdots +a_{1}s+a_{0}\n\\]\nthe table has \\(n + 1\\) rows and the following structure:\n\n\n\n\\(s^n\\)\n\\(a_{n}\\)\n\\(a_{n-2}\\)\n\\(a_{n-4}\\)\n\\(...\\)\n\n\n\\(s^{n-1}\\)\n\\(a_{n-1}\\)\n\\(a_{n-3}\\)\n\\(a_{n-5}\\)\n\\(...\\)\n\n\n\\(...\\)\n\\(b_1\\)\n\\(b_2\\)\n\\(b_3\\)\n\\(...\\)\n\n\n\\(...\\)\n\\(c_1\\)\n\\(c_2\\)\n\\(c_3\\)\n\\(...\\)\n\n\n\\(s^{1}\\)\n\\(...\\)\n\\(...\\)\n\\(...\\)\n\\(...\\)\n\n\n\\(s^{0}\\)\n\\(...\\)\n\\(...\\)\n\\(...\\)\n\\(...\\)\n\n\n\nwhere the elements \\(b_{i}\\) and \\(c_{i}\\) can be computed as follows:\n\\[\n{\\displaystyle b_{i}={\\frac {a_{n-1}\\times {a_{n-2i}}-a_{n}\\times {a_{n-(2i+1)}}}{a_{n-1}}}.}\n\\]\n\\[\n{\\displaystyle c_{i}={\\frac {b_{1}\\times {a_{n-(2i+1)}}-a_{n-1}\\times {b_{i+1}}}{b_{1}}}.}\n\\]\n\nWhen completed, the number of sign changes in the first column will be the number of non-negative roots.\n\n\nExamples\n\\[G(s) = \\frac{1}{(s^2-s+4)(s+2)(s+1)} = \\frac{1}{s^4+2s^3+3s^2+10s+8}\\]\n\n\n\n\n\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(3\\)\n\\(8\\)\n\n\n\\(s^{3}\\)\n\\(2\\)\n\\(10\\)\n\n\n\n\\(s^2\\)\n\\(\\frac{2\\cdot3-1\\cdot10}{2}=-2\\)\n\\(\\frac{2\\cdot8-1\\cdot0}{2}=8\\)\n\n\n\n\\(s^{1}\\)\n\\(\\frac{-2\\cdot10-2\\cdot8}{-2}=18\\)\n\\(0\\)\n\n\n\n\\(s^{0}\\)\n\\(\\frac{18\\cdot8-2\\cdot0}{2}=8\\)\n\n\n\n\n\n\nDetermine the number of roots in RHP by counting the number of sign changes in the first column: the are two sign changes, hence there are two roots with \\(Re>0\\)\nThe system is unstable.\n\n\\[G(s) = \\frac{1}{s^4+2s^3+3s^2+4s+5}\\]\n\n\n\n\n\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(3\\)\n\\(5\\)\n\n\n\\(s^{3}\\)\n\\(2\\)\n\\(4\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(\\frac{2\\cdot3-1\\cdot4}{2}=1\\)\n\\(\\frac{2\\cdot5-1\\cdot0}{2}=5\\)\n\n\n\n\\(s^{1}\\)\n\\(\\frac{1\\cdot4-2\\cdot5}{1}=-6\\)\n\\(0\\)\n\n\n\n\\(s^{0}\\)\n\\(\\frac{-6\\cdot5-1\\cdot0}{-6}=5\\)\n\n\n\n\n\n\nWe have two roots with \\(Re>0\\)\nRoots: \\(-1.28\\pm j0.858, 0.28\\pm j1.416\\)\n\n\n\nRouth-Hurwitz criterion: special cases\n\n\nSpecial case 1)\n\nA zero in a row with at least one non-zero appearing later in the row\nThe system is always unstable\nWe can still fill out the table to know how many are unstable\nExample 1:\n\n\\[G(s) = \\frac{1}{1s^4+2s^3+0s^2+3s+4}\\]\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(0\\)\n\\(4\\)\n\n\n\\(s^{3}\\)\n\\(2\\)\n\\(3\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(\\frac{2\\cdot0-1\\cdot3}{2}=\\frac{-3}{2}\\)\n\n\n\n\n\\(s^{1}\\)\n\n\n\n\n\n\\(s^{0}\\)\n\n\n\n\n\n\nWe can already say that the system is unstable\n\nExample 2: \\[G(s) = \\frac{1}{1s^4+2s^3+2s^2+4s+5}\\]\n\n\n\n\n\n\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(2\\)\n\\(5\\)\n\n\n\\(s^{3}\\)\n\\(2\\)\n\\(4\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(0 \\rightarrow \\epsilon\\)\n\\(5\\)\n\n\n\n\\(s^{1}\\)\n\\(\\frac{\\epsilon\\cdot4-2\\cdot5}{\\epsilon}\\)\n\\(0\\)\n\n\n\n\\(s^{0}\\)\n\\(5\\)\n\n\n\n\n\n\nTo calculate the coefficients we need to calculate the values for $_{} $\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(2\\)\n\\(5\\)\n\n\n\\(s^{3}\\)\n\\(2\\)\n\\(4\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(0^+\\)\n\\(5\\)\n\n\n\n\\(s^{1}\\)\n\\(4-\\frac{10}{\\epsilon} = -\\infty\\)\n\\(0\\)\n\n\n\n\\(s^{0}\\)\n\\(5\\)\n\n\n\n\n\n\nWe have two roots with \\(Re > 0\\)\n\n\n\nSpecial case 2)\n\nAn entire row is zeros\n\n\\[G(s) = \\frac{1}{1s^5+2s^4+6s^3+10s^2+8s+12}\\]\n\n\n\n\\(s^5\\)\n\\(1\\)\n\\(6\\)\n\\(8\\)\n\n\n\\(s^4\\)\n\\(2\\)\n\\(10\\)\n\\(12\\)\n\n\n\\(s^{3}\\)\n\\(1\\)\n\\(2\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(6\\)\n\\(12\\)\n\\(0\\)\n\n\n\\(s^{1}\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(s^{0}\\)\n\\(?\\)\n\n\n\n\n\n\nThere are only 3 possible conditions that can lead to a Routh array with all zeros in a row:\n\ntwo real roots, equal and opposite in sign \\(\\Rightarrow\\) unstable\ntwo imaginary roots, that are complex conjugate of each other \\(\\Rightarrow\\) marginally stable - response is oscillatory\nfour roots that are all equal distance from the origin \\(\\Rightarrow\\) unstable\n\n\n\n\n\n\n\n\nTo determine the system stability:\nHere is the original table: | | | | | | —— | —— | - | - | | \\(s^5\\) | \\(1\\) | \\(6\\) | \\(8\\) | | \\(s^4\\) | \\(2\\) | \\(10\\) | \\(12\\) | | \\(s^{3}\\) | \\(1\\) | \\(2\\) | \\(0\\) | | \\(s^2\\) | \\(6\\) | \\(12\\) | \\(0\\) | | \\(s^{1}\\) | \\(0\\) | \\(0\\) | \\(0\\) | | \\(s^{0}\\) | \\(?\\) | | |\n\nWe build the auxiliary polinomial using the row right above the one that is zero:\n\nThose are the coefficients of the auxiliary polinomial\n\\(p(s)=6s^2+12s^0=0 \\rightarrow p(s)=s^2+2\\)\nNote that we are skipping every other power\n\nTake the derivative of \\(p(s)\\): \\(\\frac{d}{ds}p(s)=2s\\) and replace the all zero row with the coefficient of \\(\\frac{d}{ds}p(s)\\)\nComplete the table as we would normally\n\n\n\n\n\n\\(s^5\\)\n\\(1\\)\n\\(6\\)\n\\(8\\)\n\n\n\\(s^4\\)\n\\(2\\)\n\\(10\\)\n\\(12\\)\n\n\n\\(s^{3}\\)\n\\(1\\)\n\\(2\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(6\\)\n\\(12\\)\n\\(0\\)\n\n\n\\(s^{1}\\)\n\\(2\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(s^{0}\\)\n\\(12\\)\n\n\n\n\n\n\nNo sign changes in the first column, hence no roots with \\(Re >0\\).\nThis means that the system must have two imaginary roots and is marginally stable.\n\n\n\nAdditional comments:\n\nThe auxiliary polinomial \\(p(s)\\) exists if and only if there is an all zero row in the routh array, and it is a factor of the original polinomial \\(q(s)\\) (it divides the original polinomial with no reminder):\n\n\\[p(s)r(s)=q(s)\\]\n\nThe original polinomial \\(q(s)\\) is the one we started from (e.g., denominator of \\(G(s)\\))\nThis makes it possible to calculate how many roots have \\(Re<0\\), how many \\(Re=0\\) and how many \\(Re>0\\)\nWe can determine \\(r(s) = \\frac{q(s)}{p(s)}\\) (polinomial division)\n\nIn our case:\n\\[\n(s^5+2s^4+6s^3+10s^2+8s+12)\\;\\; :\\;\\; (s^2+2)\n\\]\n\\[\\Downarrow\\]\n\\[\nr(s) = s^3 + 2s^2+4s+6\n\\]\n\nwith no remainder.\n\nand this means: \\[\nq(s)= (s^2+2)(s^3 + 2s^2+4s+6)\n\\]\n\nOnly true if we have a row of all zeros, or we will have some non zero reminder when we do the division\n\nIf we now re-write our table here:\n\n\n\n\\(s^5\\)\n\\(1\\)\n\\(6\\)\n\\(8\\)\n\n\n\\(s^4\\)\n\\(2\\)\n\\(10\\)\n\\(12\\)\n\n\n\\(s^{3}\\)\n\\(1\\)\n\\(2\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(6\\)\n\\(12\\)\n\\(0\\)\n\n\n\\(s^{1}\\)\n\\(2\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(s^{0}\\)\n\\(12\\)\n\n\n\n\n\n\nAny part of the table above the auxiliary polinomial \\(p(s)=6s^2+12 = s^2+2\\) is due to the factor \\(r(s)=s^3 + 2s^2+4s+6\\), and since there are not sign changes we can say that \\(r(s)\\) is stable.\nThe other part of the table is due to the auxiliary polinomial \\(p(s)\\): the number of sign changes after \\(p(s)\\) in the table predicts the number of \\(Re>0\\) roots for \\(p(s)\\)\nGiven that we have \\(p(s) = s^2+2 \\Rightarrow s = \\sqrt{-2} = \\pm j\\sqrt{2} \\Rightarrow\\) Two complex conjugate roots, the system is marginally stable (what we expected)"
  },
  {
    "objectID": "routh_hurwitz_criterion.html#practical-uses-beyond-stability",
    "href": "routh_hurwitz_criterion.html#practical-uses-beyond-stability",
    "title": "Routh-Hurwitz Criterion",
    "section": "Practical uses: beyond stability",
    "text": "Practical uses: beyond stability\n\nHow can we use the Routh Criterion for more than just assessing stability\n\nSuppose you have an open loop system:\n\\[\nG(s) =\\frac{1}{s^4+6s^3+11s^2+6s+2}\n\\]\nAnd we can verify its step response:\n\n# Import relevant libraries (we have imported them already so we comment these out)\n# import control\n# import matplotlib.pylab as plt\n\n\nimport numpy as np\nnp.roots([1, 6, 11, 6, 2])\n\narray([-2.6938972+0.41879653j, -2.6938972-0.41879653j,\n       -0.3061028+0.41879653j, -0.3061028-0.41879653j])\n\n\n\nsys = control.tf([1], [1, 6, 11, 6, 2])\nT, yout = control.step_response(sys)\n\n\nfig = plt.figure()\n\nplt.plot(T, yout)\nplt.grid()\nplt.xlabel('Time (s)')\nplt.ylabel('Response');\n\n\n\n\nDesign requirement: - improve system response time\nWe can add feedback:\n\n\n\n\n\n\n\n\n\n\n\\(U(s)\\) is the reference signal (e.g. desired angle, etc.)\nNote that the feedback path is unitary: perfect sensor (it always knows the output signal (e.g. system angle) with no delay or error)\nWe want to tune the gain \\(K\\) to get the response that we desire\n\nAdjusting the gain increases the amplitude of the signal (err)\nIn an open loop linear system we can increase the amplitude “as much as we want” (it does not affect stability)\nThe feedback loop however changes the dynamics\n\nTypical requirement: increase the gain large enough to meet our requirements (e.g. fastest response) while keeping the system stable\nLet’s assess the closed loop stability using RHC\n\nSimplify the block diagram: \\[\n  \\hat{G}(s) = \\frac{\\frac{K}{s^4+6s^3+11s^2+6s+2}}{1+\\frac{K}{s^4+6s^3+11s^2+6s+2}} = \\frac{K}{s^4+6s^3+11s^2+6s+2 + K}\n  \\]\n\nThe new characteristic equation for the closed loop system is: \\[  \ns^4+6s^3+11s^2+6s+2 + K = 0\n\\]\n\nBuild the Routh array\n\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(11\\)\n\\(24+K\\)\n\n\n\\(s^{3}\\)\n\\(6\\)\n\\(6\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(10\\)\n\\(K+2\\)\n\n\n\n\\(s^{1}\\)\n\\(48-6K\\)\n\n\n\n\n\\(s^{0}\\)\n\\(K+2\\)\n\n\n\n\n\n\nTo have asymptotic stability:\n\n\\(48-6K > 0 \\rightarrow K < 8\\)\n\\(K+2>0 \\rightarrow K > -2\\)\nStability range is \\(-2 < K < 8\\)\n\nWhen \\(K=-2\\), we have a pole at the origin\nWhen \\(K=8\\), we have a pair of imaginary poles and the system is marginally stable\nWe know the gain margin that we have before the system becomes unstable\nWe do not know where the poles are exactly (we only know when they cross the imaginary axis)\n\n\nK = 2 #8\n\n\nsys = control.tf([K], [1, 6, 11, 6, 2+K])\nT_k, yout_k = control.step_response(sys)\n\n\nfig = plt.figure()\n\nplt.plot(T, yout, label='original')\nplt.plot(T_k, yout_k, label='feedback')\nplt.grid()\nplt.legend()\nplt.xlabel('Time (s)')\nplt.ylabel('Response');\n\n\n\n\n\nExample 2\nSuppose you have an open loop system:\n\\[\nG(s) =\\frac{1}{s^4+10s^3+35s^2+50s+24} = \\frac{1}{(s+1)(s+2)(s+3)(s+4)}\n\\]\nAnd we can plot its step response:\n\nsys = control.tf([1], [1, 10, 35, 50, 24])\nT, yout = control.step_response(sys)\n\n\nfig = plt.figure()\n\nplt.plot(T, yout, label='original')\nplt.grid()\nplt.xlabel('Time (s)')\nplt.ylabel('Response')\n\nText(0, 0.5, 'Response')\n\n\n\n\n\nDesign requirement: - improve system response time\nWe can add feedback:\n\n\n\n\n\n\n\n\n\n\n\\(U(s)\\) is the reference signal (e.g. desired angle, etc.)\nNote that the feedback path is unitary: perfect sensor (it always knows the output signal (e.g. system angle) with no delay or error)\nWe want to tune the gain \\(K\\) to get the response that we desire\n\nAdjusting the gain increases the amplitude of the signal (err)\nIn an open loop linear system we can increase the amplitude as much as we want\nThe feedback loop however changes the dynamics\n\nTypical requirement: increase the gain large enough to meet our requirements (e.g. fastest response) while keeping the system stable\nLet’s assess the closed loop stability using RHC\n\nSimplify the block diagram: \\[\n  \\hat{G}(s) = \\frac{\\frac{K}{s^4+10s^3+35s^2+50s+24}}{1+\\frac{K}{s^4+10s^3+35s^2+50s+24}} = \\frac{K}{s^4+10s^3+35s^2+50s+24 + K}\n  \\]\n\nThe new characteristic equation for the closed loop system is: \\[  \ns^4+10s^3+35s^2+50s+24 + K = 0\n\\]\n\nBuild the Routh array\n\n\n\n\n\n\\(s^4\\)\n\\(1\\)\n\\(35\\)\n\\(24+K\\)\n\n\n\\(s^{3}\\)\n\\(10\\)\n\\(50\\)\n\\(0\\)\n\n\n\\(s^2\\)\n\\(\\frac{10\\cdot35-50}{10}=30\\)\n\\(24+K\\)\n\n\n\n\\(s^{1}\\)\n\\(42-\\frac{K}{3}\\)\n\n\n\n\n\\(s^{0}\\)\n\\(24+K\\)\n\n\n\n\n\n\nHow many sign changes?\n\n\\(42-K/3 > 0 \\rightarrow K < 126\\)\n\\(K > -24\\)\nand solving the system of equations: \\(K < 126\\)\n\nWhen \\(K=-24\\) we have a pole at the origin\nWhen \\(K = 126\\) the poles would be going from \\(Re<0\\) to \\(Re>0\\) (imaginary poles) and the system would be marginally stable\n\nAgain, - We know the gain margin that we have - We do not know where the poles are exactly (we only know when they cross the imaginary axis)\n\nK = 126\n\n\nsys = control.tf([K], [1, 10, 35, 50, 24+K])\nT_k, yout_k = control.step_response(sys)\n\n\nfig = plt.figure()\n\nplt.plot(T, yout, label='original')\nplt.plot(T_k, yout_k, label='feedback')\nplt.grid()\nplt.legend()\nplt.xlabel('Time (s)')\nplt.ylabel('Response');\n\n\n\n\n\n\nHaving additional requirements: time to half\n\nSpeed of the response depends on the position of the poles on the real axis\n\nThe further from the imaginary axis, the faster the response (true for both stable and unstable poles)\nWhen \\(s=j\\omega\\) (imaginary poles), the response is a sinusoid (\\(e^{j\\omega}\\) is a sinusoid)\nthe closer the poles are to the real axis, the slower the frequency of the response (i.e., slower oscillation)\n\nThe time to half is the time for a signal to half (or double, if unstable) the initial signal in magnitude\nTypical requirement, especially when dealing with systems that interact with humans: the system should not be too fast or too slow to respond\nA time to half requirements is a requirement on the real part of the poles\nWe would like the poles to be to the left of a line called the z-line\nSet \\(s=z+\\sigma_{des}\\)\n\nWhen \\(z=0\\), \\(s=+\\sigma_{des}\\), and the poles have the desired real part.\n\nGiven a characteristic equation \\(D(s)=0\\) \\[\ns^3+5s^2+25s+30=0\n\\]\n\nwe replace \\(s=z+\\sigma_{des}\\) (e.g \\(\\sigma_{des}\\)=-1)\n\\[\n(z-1)^3+5(z-1)^2+25(z-1)+30=0\n\\]\n\\[\n\\Downarrow\n\\]\n\\[\nz^3+2z^2+18s+9=0\n\\]\n\nWe can now use the Routh array to verify if the system has any roots to the right of the z-line\n\n\n\n\n\\(s^3\\)\n\\(1\\)\n\\(18\\)\n\n\n\n\\(s^{2}\\)\n\\(2\\)\n\\(9\\)\n\n\n\n\\(s^1\\)\n\\(\\frac{27}{2}\\)\n\n\n\n\n\\(s^{0}\\)\n\\(9\\)\n\n\n\n\n\n\nNo sign changes in the first column: all roots are to the left side of \\(s=-1\\).\n\n\n\nRHC Final Comments\n\nRHC provides necessary and sufficient conditions to analyse the stability of a system\nThe stability analysis can be done with respect to any parameters of the system (including the gain \\(K\\))\nBuilding the routh array:\n\nwhen we have an all zero row, the roots of the auxiliary polynomial are a subset of the roots of the original polinomial (i.e., of the characteristic equation of the system)\nthe roots of the auxiliary polynomial are symmetric with respect to the origin (i.e., wrt real and imaginary axis)."
  },
  {
    "objectID": "b_assignment.html#nyquist-plot",
    "href": "b_assignment.html#nyquist-plot",
    "title": "Assignment Part 2",
    "section": "Nyquist Plot",
    "text": "Nyquist Plot\n\nQ1. Consider a close loop system with unity feedback.\n\nFor the \\(G(s)=s-1\\), hand sketch the Nyquist diagram\nDetermine \\(Z = N+P\\), algebraically find the closed-loop pole location, and show that the closed loop pole location is consistent with the Nyquist diagram calculation.\nUse the controller \\(D(s) = k = 2\\).\n\n\n\nQ2. Consider the following controller D and system G\nFor controller \\(D(s) = k\\) and \\(G(s) = \\frac{s + 1}{s^2(s + 10)}\\)\n\nHand sketch the asymptotes of the Bode plot magnitude and phase for the open-loop transfer function\nHand sketch Nyquist diagram.\nDiscuss stability margins"
  },
  {
    "objectID": "b_assignment.html#gain-and-phase-margins",
    "href": "b_assignment.html#gain-and-phase-margins",
    "title": "Assignment Part 2",
    "section": "Gain and Phase margins",
    "text": "Gain and Phase margins\n\nQ3. Given a closed loop system with unity gain with the following loop transfer function:\n\\[G(s) = \\frac{125(s + 1)}{(s+5)(s^2 +4s+25)}\\]\n\nPlot the Bode magnitude and phase plots for the open loop system\nDetermine the gain and phase margin."
  },
  {
    "objectID": "b_assignment.html#root-locus",
    "href": "b_assignment.html#root-locus",
    "title": "Assignment Part 2",
    "section": "Root Locus",
    "text": "Root Locus\n\nQ4. Properties of the Root Locus\nGiven\n\\[G(s) = \\frac{K(s+2)}{(s^2+4s+13)}\\]\n\nCalculate the angle of G(s) at the point \\((-3 + j0)\\) by finding the algebraic sum of angles of the vectors drawn from the zeros and poles of G(s) to the given point.\nDetermine if the point \\((-3 + j0)\\) is on the root locus.\nIf the point specified in a is on the root locus, find the gain, K, using the lengths of the vectors.\n\n\n\nQ5. Sketching the Root Locus\n\nSketch the root locus for the system shown"
  },
  {
    "objectID": "b_assignment.html#lead-lag-compensation-design",
    "href": "b_assignment.html#lead-lag-compensation-design",
    "title": "Assignment Part 2",
    "section": "Lead-Lag Compensation Design",
    "text": "Lead-Lag Compensation Design\n\nQ6. Design a lead compensator\nGiven a unity feedback system where the plant \\[G(s) =  \\frac{K}{s(s+50)(s+120)}\\]\nDesign a lag-lead compensator to have: - 20% overshoot - \\(\\Phi_m > 48^o\\) - \\(T_s=0.2s\\) - Steady state requirement \\(K_v=\\lim_{s\\rightarrow0}sG(s)=50\\)"
  },
  {
    "objectID": "b_assignment.html#putting-everything-together",
    "href": "b_assignment.html#putting-everything-together",
    "title": "Assignment Part 2",
    "section": "Putting everything together",
    "text": "Putting everything together\nConsider the DC motor transfer function between of the motor-load combination which is given by:\n\\[\n\\frac{\\theta(s)}{V_f(s)} = \\frac{K_m}{s(Js+f)(L_fs+R_f)} = \\frac{\\frac{K_m}{JL_f}}{s(s+\\frac{f}{J})(s+\\frac{R_f}{L_f})}. \\;\\;\\;\\;(1)\n\\]\nThe system above can also be written as:\n\\[\n\\frac{\\theta(s)}{V_f(s)} = \\frac{\\frac{K_m}{JL_f}}{s(s+\\frac{f}{J})(s+\\frac{R_f}{L_f})} = \\frac{\\frac{K_m}{fR_f}}{s(\\tau_f s+1)(\\tau_L s+1)}\n\\]\nwhere \\(\\tau_f=\\frac{L_f}{R_f}\\) and \\(\\tau_L=\\frac{J}{f}\\).\nWhen \\(\\tau_L > \\tau_f\\), the field time constant \\(\\tau_f\\) can be neglected.\nLet’s choose the following values:\n\n\nmoment of inertia of the rotor     0.01 kg.m^2\n\n\nmotor viscous friction constant    0.1 N.m.s\n\n(Ke) electromotive force constant 0.01 V/rad/sec\n(Kt) motor torque constant 0.01 N.m/Amp\n\nelectric resistance                1 Ohm\n\n\nelectric inductance                0.5 H\n\n\nNote that in SI units, \\(K_e = K_t = K\\)\n\nQ7. Close the loop with a unitary feedback and discuss the system performance.\n\n\nQ8 Calculate the stability margins\n\n\nQ9. Increase the inertia of the rotor and discuss what happens\n\nUse values J=[0.1, 1, 2]\n\n\n\nQ10. Designing a Controller for the Motor\nDesign requirements:\n\n$t_r s $ (90%)\nMax overshoot \\(20\\%\\) at \\(5.75s\\)\nSettling time to \\(\\pm0.05\\%\\) \\(\\approx 11.15s\\)\nZero steady state error\nChoose any method you prefer.\nDiscuss the performance\n\n\n\nQ11. Optional\n\nWhat happens if you add noise or load disturbances?\nHow would you implement a PID controller?\nUse notebook 99_workspace and design a controller for the pendulum attached to the DC motor. You might need to look at notebooks 90_simple_pendulum and 91_DC_motor to understand the model of the pendulum and of the motor used."
  },
  {
    "objectID": "workspace/dc_motor.html",
    "href": "workspace/dc_motor.html",
    "title": "DC Motor",
    "section": "",
    "text": "This notebook models a DC motor based on equations presented here and here.\nAdditional explanation on the motor are presented here\nMore information can be found:\nsource"
  },
  {
    "objectID": "workspace/dc_motor.html#adding-a-motor-controller",
    "href": "workspace/dc_motor.html#adding-a-motor-controller",
    "title": "DC Motor",
    "section": "Adding a motor controller",
    "text": "Adding a motor controller\nThe next step is to have a motor controller so that we can use it in our workspace.\n\nsource\n\nPID\n\n PID (Kp, Kd, Ki)\n\nPID controller.\n\npendulum = Pendulum(theta_0=np.radians(0), \n                    theta_dot_0=0, \n                    params=PendulumParameters(b=0.2))\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), \n                params=DCMotorParams())\n\npid = PID(Kp=1000, \n          Kd=500, \n          Ki=1000)\n\n# connect the motor to the pendulum\nmotor.connect_to(pendulum)\n\nt0, tf, dt = 0, 20, 0.01\ntime_vector = np.arange(t0, tf, dt)\n\nangle_des = 5 # desired final value\n\npendulum_angles = []\nfor t in time_vector:  \n    u = pid.control(dt, y_des=np.radians(angle_des), \n                        y=pendulum.position())\n    torque = motor.step(dt, u)\n    pendulum.step(dt, u=torque)\n    pendulum_angles.append(np.degrees(pendulum.position()))\n                                             \nprint('Reference position: {:.4f} rad/{:.1f} deg'.format(np.radians(angle_des), angle_des))\nprint('Pendulum position (tf): {:.4f} rad/{:.1f} deg'.format(np.radians(pendulum_angles[-1]), pendulum_angles[-1]))\n\nReference position: 0.0873 rad/5.0 deg\nPendulum position (tf): 0.0873 rad/5.0 deg\n\n\n\nplt.plot(time_vector, pendulum_angles)\nplt.plot(time_vector, angle_des*np.ones((len(pendulum_angles))));\nplt.xlabel('time (s)')\nplt.ylabel('pendulum position (deg)');\nplt.ylim(0, angle_des+1);\nplt.grid()\n\n\n\n\n\nsource\n\n\nSimulator\n\n Simulator (pendulum, motor, controller)\n\nInitialize self. See help(type(self)) for accurate signature.\n\npendulum = Pendulum(theta_0=np.radians(0), \n                    theta_dot_0=0, \n                    params=PendulumParameters(b=0.2))\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), \n                params=DCMotorParams())\n\npid = PID(Kp=1000, \n          Kd=500, \n          Ki=1000)\n\n# connect the motor to the pendulum\nmotor.connect_to(pendulum)\n\nt0, tf, dt = 0, 20, 0.01\nsim = Simulator(pendulum, motor, pid)\nsim.y_des(10) # desired final value in degrees\n\nresults = sim.run(t0, tf, dt)\n\n\nfig, axs = plt.subplots(len(results.keys())-1, 1, figsize=(10,10))\n\nfor index, key in enumerate(results.keys()):\n    if key == 'time (s)': continue\n    if key == 'position (rad)':\n        axs[index].plot(results['time (s)'], np.degrees(results[key]), linewidth=3)\n        axs[index].plot(results['time (s)'], sim.y_des*np.ones(results['time (s)'].shape), color='r', linewidth=3)\n    else:\n        axs[index].plot(results['time (s)'], results[key], linewidth=3)\n    axs[index].set_xlabel('time (s)')\n    axs[index].set_ylabel(key)\n    axs[index].grid()\n\n    \n\nplt.tight_layout()"
  },
  {
    "objectID": "workspace/dc_motor.html#animate",
    "href": "workspace/dc_motor.html#animate",
    "title": "DC Motor",
    "section": "Animate",
    "text": "Animate\n\nsource\n\nAnimateControlledPendulum\n\n AnimateControlledPendulum (sim)\n\nSee also: https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\n\n#%%capture\npendulum = Pendulum(theta_0=np.radians(0), \n                    theta_dot_0=0, \n                    params=PendulumParameters(b=0.2))\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), \n                params=DCMotorParams())\n\npid = PID(Kp=1000, \n          Kd=500, \n          Ki=1000)\n\n# connect the motor to the pendulum\nmotor.connect_to(pendulum)\n\nt0, tf, dt = 0, 10, 1./30 # time\nsim = Simulator(pendulum, motor, pid)\n\nsim.y_des(10) # desired final value in degrees\nresults = sim.run(t0, tf, dt)\n\nap = AnimateControlledPendulum(sim)\nap.start_animation(t0, tf, dt)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "workspace/simple_pendulum.html",
    "href": "workspace/simple_pendulum.html",
    "title": "Simulating a simple pendulum",
    "section": "",
    "text": "We first draw the free-body diagram where the forces acting on the pendulum are its weight and the reaction at the rotational joint. We also include a moment due to the friction in the joint (and the rotary potentiometer). The simplest approach to modeling assumes the mass of the bar is negligible and that the entire mass of the pendulum is concentrated at the center of the end weight.\n\n\n\n\n\n\n\n\n\n\nThe equation of motion of the pendulum can then be derived by summing the moments.\nWe will choose to sum the moments about the attachment point \\(O\\) since that point is the point being rotated about and since the reaction force does not impart a moment about that point.\n\nWe can then write:\n\\[ \\sum{M} = (M+m)glsin(\\theta) - T_{fric} = I_O\\ddot\\theta\\]\nAssuming that the mass of the pendulum is concentrated at its end mass, the mass moment of inertia is \\(I_O \\approx (M+m)l^2\\). A more accurate approach would be to consider the rod and end mass explicitly. In that case, the weight of the system could be considered to be located at the system’s mass center \\(l_G = (Ml+0.5ml))/(M+m)\\). In that case, the mass moment of inertia is \\(I_O = ml^2/3 + Ml^2\\). Depending on the parameters of your particular pendulum, you can assess if this added fidelity is necessary.\nWe will also initially assume a viscous model of friction, that is, \\(T_{Fric} = -b\\dot{\\theta}\\) where \\(b\\) is a constant. Such a model is nice because it is linear. We will assess the appropriateness of this model later. Sometimes the frictional moment is not linearly proportional to the angular velocity. Sometimes, the stiction in the joint is significant enough that it must be modeled too.\nTaking into account the above assumptions, our equation of motion becomes the following.\n\\[ -(M+m)gl_G\\sin\\theta - b\\dot{\\theta} = I_O\\ddot{\\theta} \\]\n\n\nWe will use the same parameters of this pendulum.\nThis is a simple pendulum that consists of: - a rod of length \\(l = 0.43\\ \\mbox{m}\\) and mass \\(m = 0.095\\ \\mbox{kg}\\) - an end mass of \\(0.380\\ \\mbox{kg}\\).\nAssuming that the mass of the pendulum is concentrated at its end mass, the mass moment of inertia is \\(I_O \\approx (M+m)l^2\\). A more accurate approach would be to consider the rod and end mass explicitly. In that case, the weight of the system could be considered to be located at the system’s mass center \\(l_G = (Ml+0.5ml))/(M+m)\\). In that case, the mass moment of inertia is \\(I_O = ml^2/3 + Ml^2\\). Depending on the parameters of your particular pendulum, you can assess if this added fidelity is necessary.\nTherefore, the difference between \\(l = 0.43\\ \\mbox{m}\\) and \\(l_G = 0.39\\ \\mbox{m}\\) is significant enough to include.\nThe difference between \\(I_O \\approx (M+m)l^2 = 0.088\\ \\mbox{kg-m}^2\\) and \\(I_O = ml^2/3 + Ml^2 = 0.076\\ \\mbox{kg-m}^2\\) is also significant enough to include.\n\n\n\nAfter model identification the following parameters are identified:\n\\[ I_O = 0.079\\ \\mbox{kg-m}^2 \\] \\[ b = 0.003\\ \\mbox{N-m-s}^2 \\]\nRecalling the model we derived from first principles earlier.\n\\[ I_O\\ddot{\\theta} + b\\dot{\\theta} + (M+m)gl_G\\sin\\theta = 0 \\]\nThe final equation is:\n\\[\n0.079\\ddot{\\theta} + 0.003\\dot{\\theta} + 1.82\\sin\\theta = 0\n\\]\nWe can write this as:\n\\[\\dot{x_1}= x_2\\] \\[\\dot{x_2}=\\frac{1}{0.079}\\big ( -1.82\\sin x_1 -0.003 x_2  \\big)\\]\n\nsource\n\n\n\n\n PendulumParameters (rod_length=0.43, rod_mass=0.095, bob_mass=0.38,\n                     b=0.003)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrod_length\nfloat\n0.43\nm\n\n\nrod_mass\nfloat\n0.095\nkg\n\n\nbob_mass\nfloat\n0.38\nkg\n\n\nb\nfloat\n0.003\nestimate of viscous friction coefficient (N-m-s)\n\n\n\n\nsource\n\n\n\n\n Load ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Pendulum (theta_0, theta_dot_0, params)\n\nInitialize self. See help(type(self)) for accurate signature.\nWe now create our pendulum with specific initial conditions\n\npendulum = Pendulum( theta_0 = 0.423, theta_dot_0 = 0, params=PendulumParameters())\n\nAnd finally we define the simulation parameters and run the simulation\n\nt0, tf, dt = 0, 15, 0.01 # time\n\nangle = []\ntime = np.arange(t0, tf, dt)\nfor t in time:\n    pendulum.step(dt, u=0.0)\n    angle.append(pendulum.sense_theta_deg())\n\nLet’s plot the results:\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 5))\nplt.plot(time, angle, linewidth=3)\nplt.grid()\nplt.yticks(np.arange(-30, 30, step=5))  # Set label locations.\nplt.ylabel('angle (deg)')\nplt.xlabel('time (s)');\n\n\n\n\nAs expected the pendulum oscillates, with decreasing amplitude which depends on the damping coefficient (try different values of the viscous friction coefficient \\(b\\) in the class parameter to see how it affects the response).\nWe can also verify what happens when we apply an esternal torque \\(u\\)\n\nparams = PendulumParameters()\nparams.b = 2\npendulum = Pendulum( theta_0 = np.radians(10), theta_dot_0 = 0, params=params)\nt0, tf, dt = 0, 15, 0.01 # time\n\nangle = []\ntime = np.arange(t0, tf, dt)\nfor t in time:\n    pendulum.step(dt, u=0.31)\n    angle.append(pendulum.sense_theta_deg())\n    \nfig, ax = plt.subplots(1, 1, figsize=(15, 5))\nplt.plot(time, angle, linewidth=3)\nplt.grid()\nplt.yticks(np.arange(-10, 30, step=5))  # Set label locations.\nplt.ylabel('angle (deg)')\nplt.xlabel('time (s)');"
  },
  {
    "objectID": "workspace/simple_pendulum.html#animating-a-pendulum",
    "href": "workspace/simple_pendulum.html#animating-a-pendulum",
    "title": "Simulating a simple pendulum",
    "section": "Animating a pendulum",
    "text": "Animating a pendulum\nLet’s now animate the pendulum so that we can get a better intuition of how it behaves.\nThe PendulumDrawer class does the animation manually, drawing each single frame explicitely. Using this method results in quite slow rendering performance. We will use the FuncAnimation to solve this issue. For now, let’s see how the PendulumDrawer class could be defined.\n\nsource\n\nPendulumDrawer\n\n PendulumDrawer (pendulum)\n\nInitialize self. See help(type(self)) for accurate signature.\n\npendulum_drawer = PendulumDrawer(pendulum)\nfig, ax = plt.subplots(1,1)\npendulum_drawer.draw(ax)\nplt.title('theta: {:.1f} deg'.format(pendulum_drawer._pendulum.sense_theta_deg()))\nplt.axis('equal');\n\n\n\n\nIf we used the PendulumDrawer class our rendering will be very slow. Let’s now leverage matplotlib functionalities directly to have a better animation. This is done through the class AnimatePendulum.\n\nsource\n\n\nAnimatePendulum\n\n AnimatePendulum (pendulum, t0, tf, dt)\n\nSee also: https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\nAnd now we can put everything together and animate our pendulum!\n\n#%%capture\nt0, tf, dt = 0, 10, 1./30 # time\nap = AnimatePendulum(Pendulum(theta_0=np.radians(10), theta_dot_0=0, params=PendulumParameters()), t0, tf, dt)\nap.simulate(u=0.31);\nap.start_animation()\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "workspace/simple_pendulum.html#dc-motor",
    "href": "workspace/simple_pendulum.html#dc-motor",
    "title": "Simulating a simple pendulum",
    "section": "DC Motor",
    "text": "DC Motor\nThis part is based on equations presented here and here.\nAdditional explanation on the motor are presented here\nMore references are - DC Motor Transfer Function - Transfer functions of DC Motors\n\nclass DCMotorParams():\n    def __init__(self, J=0.01, b=0.1, K=0.01, R=1, L=0.5):\n            #     (J)     moment of inertia of the rotor     0.01 kg.m^2\n            #     (b)     motor viscous friction constant    0.1 N.m.s\n            #     (Ke)    electromotive force constant       0.01 V/rad/sec\n            #     (Kt)    motor torque constant              0.01 N.m/Amp\n            #     (R)     electric resistance                1 Ohm\n            #     (L)     electric inductance                0.5 H\n            #      Note that in SI units, Ke = Kt = K\n        self.J = J\n        self.b = b\n        self.K = K\n        self.R = R\n        self.L = L\n                \nclass DCMotor():\n    def __init__(self, x0, params):\n        self._params  = params\n                \n        self._x0 = x0\n        self._x = x0\n        self._J_load = 0\n        self.update_motor_matrix()\n        \n    def update_motor_matrix(self):\n        # state variables are: rotation speed (w, or theta_dot) and current (i)\n        self._A = np.array([[-self._params.b/(self._params.J+self._J_load),  self._params.K/(self._params.J+self._J_load)],\n                            [-self._params.K/self._params.L,  -self._params.R/self._params.L]])\n        self._B = np.array([[0],[1/self._params.L]])\n        self._C = np.array([1,   0])\n        self._D = 0;\n        \n    def reset(self):\n        self._x = self._x0\n        \n    def set_load(self, J_load):\n        self._params.J += J_load   \n        self.update_motor_matrix()\n        \n    def step(self, dt, u):\n        self._x = self._x + dt*(self._A@self._x + self._B*u)\n              \n    def measure_speed(self):\n        self._y = self._C@self._x \n        return self._y\n\n\nmotor = DCMotor(np.array([[0], [0]]), DCMotorParams())\n\n\ny = []\nu = 1\n\ntime_vector = np.arange(0, 10, 0.01)\nfor t in time_vector:\n    motor.step(0.01, u)\n    y.append(motor.measure_speed())\n\n\nfig = plt.figure(figsize=(10,5))\n\nplt.plot(time_vector, y)\nplt.xlabel('time (s)')\nplt.ylabel('speed (w, rad/s)')\n\nText(0, 0.5, 'speed (w, rad/s)')\n\n\n\n\n\n\nmotor.reset()\nmotor.set_load(0.1)\n\n\ny = []\nu = 1\n\ntime_vector = np.arange(0, 10, 0.01)\nfor t in time_vector:\n    motor.step(0.01, u)\n    y.append(motor.measure_speed())\n\n\nfig = plt.figure(figsize=(10,5))\n\nplt.plot(time_vector, y)\nplt.xlabel('time (s)')\nplt.ylabel('speed (w, rad/s)')\n\nText(0, 0.5, 'speed (w, rad/s)')\n\n\n\n\n\n\nExtended DCMotor\nWe modify the usual definition of the motor (see above), adding one state to explicit the rotor position.\nWe also define a wrapping function to make sure the rotor position stays within -pi and pi.\n\nsource\n\n\nwrap\n\n wrap (angle)\n\nWraps an angle between -pi and pi.\n\nsource\n\n\nDCMotor\n\n DCMotor (x0, params)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDCMotorParams\n\n DCMotorParams (J=0.01, b=0.1, K=0.01, R=1, L=0.5)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), params=DCMotorParams())\ny = np.array([[0],[0]]) # initialise to 0. This is only here to have the correct size. Alternative: y = np.empty((2,1)) \n\nu = 1\n\ntime_vector = np.arange(0, 10, 0.01)\nfor t in time_vector:\n    motor.step(0.01, u)\n    y = np.append(y, motor.measure(), axis=1)   \n    \n# remove the first item which was there only to have the correct size\ny = np.delete(y, 0, axis=1)\n\nposition = y[0,:]\nvelocity = y[1,:]\n\n\nfig, axs = plt.subplots(2, 1, figsize=(10,5))\n\naxs[0].plot(time_vector, velocity)\naxs[0].set_xlabel('time (s)')\naxs[0].set_ylabel('speed (w, rad/s)')\n\naxs[1].plot(time_vector, position)\naxs[1].set_xlabel('time (s)')\naxs[1].set_ylabel('position (rad)');\n\n\n\n\nOr if we want to express it as a transfer function: \\[\nP(s) = \\frac{\\dot\\Theta(s)}{V(s)} = \\frac{K}{(Js+b)(Ls+R) + K^2}\n\\]\nand if we want to measure the position\n\\[\n\\tilde{P}(s) = \\frac{\\Theta(s)}{V(s)} = \\frac{K}{s(Js+b)(Ls+R) + K^2}\n\\]"
  },
  {
    "objectID": "workspace/simple_pendulum.html#adding-a-motor-controller",
    "href": "workspace/simple_pendulum.html#adding-a-motor-controller",
    "title": "Simulating a simple pendulum",
    "section": "Adding a motor controller",
    "text": "Adding a motor controller\nWe can now control the position of the motor\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), params=DCMotorParams())\nKp = 10\nKd = 3\nKi = .1\n\ny_history = np.array([[0],[0]]) # initialise to 0. This is only here to have the correct size. Alternative: y = np.empty((2,1)) \nerror_history = np.array([[0]])\n\nref_position = np.radians(1) # position reference\ntime_vector = np.arange(0, 10, 0.01)\n\nerror_old = 0\nerror_dot = 0\nerror_I  = 0\nfor t in time_vector:   \n    y = motor.measure()\n    position, velocity = y[0], y[1]\n    error = ref_position - position\n    error_dot = (error - error_old)/0.01\n    error_I   += error*0.01\n    error_old = error\n    # controller\n    u = Kp*error + Kd*error_dot + Ki*error_I\n    motor.step(0.01, u)\n\n    # save the last measurements\n    y_history = np.append(y_history, y, axis=1)  \n    error_history = np.append(error_history, [error], axis=1)\n\n\n    \n    \n# remove the first item which was there only to have the correct size\ny_history = np.delete(y_history, 0, axis=1)\nerror_history = np.delete(error_history, 0, axis=1)\n\nposition = y_history[0,:]\nvelocity = y_history[1,:]\nerror = error_history[0,:]\n\nprint('Reference (rad)/(deg): {:.5f}/{:.2f}'.format(np.radians(ref_position), ref_position))\n\nReference (rad)/(deg): 0.00030/0.02\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(10,5))\n\naxs[0].plot(time_vector, velocity)\naxs[0].set_xlabel('time (s)')\naxs[0].set_ylabel('speed (w, rad/s)')\n\n\naxs[1].plot(time_vector, position)\naxs[1].plot(time_vector, ref_position*np.ones(position.shape))\naxs[1].set_xlabel('time (s)')\naxs[1].set_ylabel('position (rad)');\n\naxs[2].plot(time_vector, np.degrees(error))\naxs[2].set_xlabel('time (s)')\naxs[2].set_ylabel('error (deg)');\n\n\n\n\nAnd now we can define it as a class.\nTo be honest, this class will do more than simply controlling the motor, but will do for now.\n\nsource\n\nMotorController\n\n MotorController (Kp, Kd, Ki, motor)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nPutting everything together\n\np_params = PendulumParameters()\np_params.b = 0.2\n\nm_params = DCMotorParams()\n\n\npendulum = Pendulum(theta_0=np.radians(0), theta_dot_0=0, params=p_params)\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), params=m_params)\nmotor_controller = MotorController(Kp=1000, Kd=500, Ki=1000, motor=motor)\n\n# connect the motor to the pendulum\nmotor.connect_to(pendulum)\n\nt0, tf, dt = 0, 20, 0.01\ntime_vector = np.arange(t0, tf, dt)\n\nref_position_rad=np.radians(5)\nangle = []\nfor t in time_vector:       \n    motor_controller.run(dt, \n                         ref_position_rad=ref_position_rad,\n                         y = np.array([[np.radians(pendulum.sense_theta_deg())], [pendulum.speed()]]))\n    T = motor_controller.motor.get_motor_torque()\n    pendulum.step(dt, u=T)\n    angle.append(pendulum.sense_theta_deg())\n\n\nposition, velocity, error, torque = motor_controller.get_results()\n\nprint('Reference position: {:.4f} rad/{:.1f} deg'.format(ref_position_rad, np.degrees(ref_position_rad)))\nprint('Pendulum position (tf): {:.4f} rad/{:.1f} deg'.format(np.radians(angle[-1]), angle[-1]))\n\nReference position: 0.0873 rad/5.0 deg\nPendulum position (tf): 0.0873 rad/5.0 deg\n\n\n\nfig, axs = plt.subplots(5, 1, figsize=(10,10))\n\naxs[0].plot(time_vector, velocity)\naxs[0].set_xlabel('time (s)')\naxs[0].set_ylabel('motor speed (w, rad/s)')\n\naxs[1].plot(time_vector, position)\naxs[1].plot(time_vector, ref_position_rad*np.ones(position.shape))\naxs[1].set_xlabel('time (s)')\naxs[1].set_ylabel('motor position (rad)');\n\naxs[2].plot(time_vector, np.degrees(error))\naxs[2].set_xlabel('time (s)')\naxs[2].set_ylabel('motor error (deg)');\n\naxs[3].plot(time_vector, torque)\naxs[3].set_xlabel('time (s)')\naxs[3].set_ylabel('torque (N.m)');\n\naxs[4].plot(time_vector, angle)\naxs[4].plot(time_vector, np.degrees(ref_position_rad)*np.ones(position.shape))\naxs[4].set_xlabel('time (s)')\naxs[4].set_ylabel('pendulum position (deg)');\naxs[4].set_ylim(0, 20)\n\nplt.tight_layout()\n\n\n\n\n\nsource\n\n\nAnimateControlledPendulum\n\n AnimateControlledPendulum (pendulum, motor, controller, t0, tf, dt)\n\nSee also: https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/\n\n#%%capture\np_params = PendulumParameters()\np_params.b = 0.2\nm_params = DCMotorParams()\npendulum = Pendulum(theta_0=np.radians(20), theta_dot_0=0, params=p_params)\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), params=m_params)\ncontroller = MotorController(Kp=1000, Kd=500, Ki=1000, motor=motor)\n\n\n\nt0, tf, dt = 0, 10, 1./30 # time\nap = AnimateControlledPendulum(pendulum, motor, controller, t0, tf, dt)\nap.simulate(angle_ref=10, controller_flag=True);\nap.start_animation()\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "workspace/workspace.html",
    "href": "workspace/workspace.html",
    "title": "Workspace",
    "section": "",
    "text": "You are welcome to use this coding environment to train your controller for the project. Follow the instructions below to get started!"
  },
  {
    "objectID": "workspace/workspace.html#start-the-environment",
    "href": "workspace/workspace.html#start-the-environment",
    "title": "Workspace",
    "section": "1. Start the Environment",
    "text": "1. Start the Environment\nThe environment is already saved in the Workspace and can be accessed at the file path provided below. Please run the next code cell without making any changes."
  },
  {
    "objectID": "workspace/workspace.html#simulate-an-untuned-pid-controller",
    "href": "workspace/workspace.html#simulate-an-untuned-pid-controller",
    "title": "Workspace",
    "section": "2. Simulate an untuned PID controller",
    "text": "2. Simulate an untuned PID controller\n\n#%%capture\n# %matplotlib notebook\npendulum = Pendulum(theta_0=np.radians(0), \n                    theta_dot_0=0, \n                    params=PendulumParameters(b=0.2))\n\nmotor = DCMotor(x0=np.array([[0], [0], [0]]), # theta, theta_dot, current\n                params=DCMotorParams())\n\npid = PID(Kp=1000, \n          Kd=0, \n          Ki=0)\n\n# connect the motor to the pendulum (effectively it sets the load on the motor)\nmotor.connect_to(pendulum)\n\nt0, tf, dt = 0, 10, 1./30 # time\nsim = Simulator(pendulum, motor, pid)\n\nsim.y_des(10) # desired final value in degrees\nresults = sim.run(t0, tf, dt)\n\n# show an animation of the results\nap = AnimateControlledPendulum(sim)\nap.start_animation(t0, tf, dt)\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\nPlot more information\nAnd we can also dig deeper and verify how the motor is behaving:\n\n# %matplotlib inline\nfig, axs = plt.subplots(len(results.keys())-1, 1, figsize=(10,10))\n\nfor index, key in enumerate(results.keys()):\n    if key == 'time (s)': continue\n    if key == 'position (rad)':\n        axs[index].plot(results['time (s)'], np.degrees(results[key]), linewidth=3)\n        axs[index].plot(results['time (s)'], sim.y_des*np.ones(results['time (s)'].shape), color='r', linewidth=3)\n        key = 'position (deg)'\n    else:\n        axs[index].plot(results['time (s)'], results[key], linewidth=3)\n    axs[index].set_xlabel('time (s)')\n    axs[index].set_ylabel(key)\n    axs[index].grid()\n\nplt.tight_layout()"
  },
  {
    "objectID": "workspace/workspace.html#its-your-turn",
    "href": "workspace/workspace.html#its-your-turn",
    "title": "Workspace",
    "section": "3. It’s Your Turn!",
    "text": "3. It’s Your Turn!\nNow it’s your turn to train your own controller to solve the project!\nA few important notes: - To structure your work, you’re welcome to work directly in this Jupyter notebook, or you might like to start over with a new file! You can see the list of files in the workspace by clicking on Jupyter in the top left corner of the notebook. - In this coding environment, you will be able to watch the results of using the controller through a simple animation. You will not be able to see how the pendulum reacts to control in real-time as the animation is played after the simulation is finished. - If you have have a TypeError: 'Line2D' object is not iterable error when running the animation, please make sure you have run %matplotlib notebook."
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html",
    "href": "getting_started_with_python_and_jupyter_notebook.html",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "",
    "text": "The purpose of this Jupyter Notebook is to get you started using Python and Jupyter Notebooks for routine engineering calculations. This introduction assumes this is your first exposure to Python or Jupyter notebooks.\nThis notebook composes information available here and here\nThe easiest way to use Jupyter notebooks is to use a cloud-based service such as Google Colaboratory. You will need continuous internet connectivity to access your work, but the advantages are there is no software to install or maintain."
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#installing-jupyterpython-on-your-laptop",
    "href": "getting_started_with_python_and_jupyter_notebook.html#installing-jupyterpython-on-your-laptop",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "Installing Jupyter/Python on your Laptop",
    "text": "Installing Jupyter/Python on your Laptop\nFor regular off-line use you should consider installing a Jupyter Notebook/Python environment directly on your laptop. This will provide you with reliable off-line access to a computational environment. This will also allow you to install additional code libraries to meet particular needs.\nChoosing this option will require an initial software installation and routine updates. For this course the recommended package is Anaconda available from Continuum Analytics. Downloading and installing the software is well documented and easy to follow. Allow about 10-30 minutes for the installation depending on your connection speed.\nAfter installing be sure to check for updates before proceeding further. With the Anaconda package this is done by executing the following two commands in a terminal window:\n> conda update conda\n> conda update anaconda\nAnaconda includes an ‘Anaconda Navigator’ application that simplifies startup of the notebook environment and manage the update process."
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#installing-the-course-environment",
    "href": "getting_started_with_python_and_jupyter_notebook.html#installing-the-course-environment",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "Installing the Course Environment",
    "text": "Installing the Course Environment\nInstrutions for the Anaconda Navigator are slightly different and are not covered in this notebook. We assume you use a terminal from hereon.\nIf you decide to use Anaconda, the first thing to do is to create a virtual environment for the course. This makes sure that you do not pollute your main OS python envinronment.\nYou can do this with:\nconda create --name feedback-control python=3.10\nYou only need to run the previous command once.\nYou then just need to activate your environment:\nconda activate feedback-control\ncreate a folder you want to use for this course. You can use your OS GUI or run:\nmkdir feedback-control\nThis creates the folder feedback-control in your current directory. Make sure you are in the correct folder before executing the previous commands.\nTo run the notebooks you need to install the following packages: fastcore pandas matplotlib control sympy numpy ffmpeg-python\nyou can do this running:\npython -m pip install fastcore pandas matplotlib control sympy numpy ffmpeg-python notebook\nYou are ready to go!\nRun:\njupyter notebook\nto start your notebook session."
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#start-a-jupyter-notebook-session",
    "href": "getting_started_with_python_and_jupyter_notebook.html#start-a-jupyter-notebook-session",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "1. Start a Jupyter Notebook Session",
    "text": "1. Start a Jupyter Notebook Session\nIf you are using a cloud-based service a Jupyter session will be started when you log on.\nIf you have installed a Jupyter/Python distribution on your laptop then you can open a Jupyter session in one of two different ways:\n\nUse the Anaconda Navigator App, or\nOpen a terminal window on your laptop and execute the following statement at the command line:\n> jupyter notebook\n\nEither way, once you have opened a session you should see a browser window.\nAt this point the browser displays a list of directories and files. You can navigate amoung the directories in the usual way by clicking on directory names or on the ‘breadcrumbs’ located just about the listing.\nJupyter notebooks are simply files in a directory with a .ipynb suffix."
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#simple-calculations-with-python",
    "href": "getting_started_with_python_and_jupyter_notebook.html#simple-calculations-with-python",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "2. Simple Calculations with Python",
    "text": "2. Simple Calculations with Python\nPython is an elegant and modern language for programming and problem solving that has found increasing use by engineers and scientists. In the next few cells we’ll demonstrate some basic Python functionality.\n\na = 12\nb = 2\n\nprint(a + b)\nprint(a**b)\nprint(a/b)\n\n14\n144\n6.0\n\n\n\nb\n\n2\n\n\n\nPython Libraries\nThe Python language has only very basic operations. Most math functions are in various math libraries. The numpy library is convenient library. This next cell shows how to import numpy with the prefix np, then use it to call a common mathematical function\n\nimport numpy as np\n\n# mathematical constants\nprint(np.pi)\nprint(np.e)\n\n# trignometric functions\nangle = np.pi/4\nprint(np.sin(angle))\nprint(np.cos(angle))\nprint(np.tan(angle))\n\n3.141592653589793\n2.718281828459045\n0.7071067811865475\n0.7071067811865476\n0.9999999999999999\n\n\n\n\nWorking with Lists\nLists are a versatile way of organizing your data in Python.\n\nxList = [1, 2, 3, 4]\nxList\n\n[1, 2, 3, 4]\n\n\nYou can join one list to another or concatentate them\n\n# Concatenation\nx = [1, 2, 3, 4];\ny = [5, 6, 7, 8];\n\nx + y\n\n[1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\nnp.sum(x)\n\n10\n\n\nElement by element operation\n\nprint(np.add(x,y))\nprint(np.multiply(x,y))\nprint(np.dot(x,y))\n\n[ 6  8 10 12]\n[ 5 12 21 32]\n70\n\n\nA for loop is a means for iterating over the elements of a list. The colon marks the start of code that will be executed for each element of a list. Indenting has meaning in Python. In this case, everything in the indented block will be executed on each iteration of the for loop. This example also demonstrates string formatting.\n\nfor x in xList:\n    print(\"sin({0}) = {1:8.5f}\".format(x,np.sin(x)))\n\nsin(1) =  0.84147\nsin(2) =  0.90930\nsin(3) =  0.14112\nsin(4) = -0.75680\n\n\n\n\nNumPy arrays\nNote that while you can do calculations on the lists, NumPy has a special object to represent math vectors or matrices called array.\nThis is NumPy’s main object and it is a homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of non-negative integers. In NumPy dimensions are called axes.\nNumPy arrays are much more powerful.\nCreating an array:\n\na = np.array([2, 3, 4])\n\narray transforms sequences of sequences into two-dimensional arrays, sequences of sequences of sequences into three-dimensional arrays, and so on.\n\nb = np.array([(1.5, 2, 3), (4, 5, 6)])\nprint(b)\n\n[[1.5 2.  3. ]\n [4.  5.  6. ]]\n\n\nThe type of the array can also be explicitly specified at creation time:\n\nc = np.array([[1, 2], [3, 4]], dtype=complex)\nprint(c)\n\n[[1.+0.j 2.+0.j]\n [3.+0.j 4.+0.j]]\n\n\nOften, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation.\n\nprint(np.zeros((3, 4)))\n\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n\n\nnp.ones((2, 3, 4), dtype=np.int16)\n\narray([[[1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1]],\n\n       [[1, 1, 1, 1],\n        [1, 1, 1, 1],\n        [1, 1, 1, 1]]], dtype=int16)\n\n\nArithmetic operators on arrays apply elementwise. A new array is created and filled with the result.\n\na = np.array([20, 30, 40, 50])\nb = np.arange(4)\nprint(a)\nprint(b)\n\n[20 30 40 50]\n[0 1 2 3]\n\n\n\nc = a - b\nprint(c)\n\n[20 29 38 47]\n\n\n\nb**2\n\narray([0, 1, 4, 9])\n\n\n\n10 * np.sin(a)\n\narray([ 9.12945251, -9.88031624,  7.4511316 , -2.62374854])\n\n\n\na < 35\n\narray([ True,  True, False, False])\n\n\nImportant Unlike in many matrix languages, the product operator * operates elementwise in NumPy arrays. The matrix product can be performed using the @ operator (in python >=3.5) or the dot function or method:\n\nA = np.array([[1, 1],\n              [0, 1]])\n\nB = np.array([[2, 0],\n              [3, 4]])\n\n\nA * B     # elementwise product\n\narray([[2, 0],\n       [0, 4]])\n\n\n\nA @ B     # matrix product\n\narray([[5, 4],\n       [3, 4]])\n\n\n\nA.dot(B)  # another matrix product\n\narray([[5, 4],\n       [3, 4]])\n\n\n\n\nWorking with Dictionaries\nDictionaries are useful for storing and retrieving data as key-value pairs.\n\nmw = {'CH4': 16.04, 'H2O': 18.02, 'O2':32.00, 'CO2': 44.01}\nmw\n\n{'CH4': 16.04, 'H2O': 18.02, 'O2': 32.0, 'CO2': 44.01}\n\n\nWe can retrieve a value from a dictionary:\n\nmw['CH4']\n\n16.04\n\n\nA for loop is a useful means of interating over all key-value pairs of a dictionary.\n\nfor values in mw.keys():\n    print(\"Value {:<s} is {}\".format(values, mw[values]))\n\nValue CH4 is 16.04\nValue H2O is 18.02\nValue O2 is 32.0\nValue CO2 is 44.01\n\n\nDictionaries can be sorted by key or by value\n\nfor values in sorted(mw):\n    print(\" {:<8s}  {}\".format(values, mw[values]))\n\n CH4       16.04\n CO2       44.01\n H2O       18.02\n O2        32.0\n\n\n\nfor values in sorted(mw, key = mw.get):\n    print(\" {:<8s}  {}\".format(values, mw[values]))\n\n CH4       16.04\n H2O       18.02\n O2        32.0\n CO2       44.01\n\n\n\n\nPlotting with Matplotlib\nImporting the matplotlib.pyplot library gives IPython notebooks plotting functionality very similar to Matlab’s. Here are some examples using functions from the\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0,10)\ny = np.sin(x)\nz = np.cos(x)\n\nplt.plot(x,y,'b',x,z,'r')\nplt.xlabel('Radians');\nplt.ylabel('Value');\nplt.title('Plotting Demonstration')\nplt.legend(['Sin','Cos'])\nplt.grid()\n\n\n\n\n\nplt.plot(y,z)\nplt.axis('equal')\n\n(-1.09972447591003,\n 1.0979832896606587,\n -1.0992804688576738,\n 1.0999657366122702)\n\n\n\n\n\n\nplt.subplot(2,1,1)\nplt.plot(x,y)\nplt.title('Sin(x)')\n\nplt.subplot(2,1,2)\nplt.plot(x,z)\nplt.title('Cos(x)')\n\nText(0.5, 1.0, 'Cos(x)')"
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#where-to-learn-more",
    "href": "getting_started_with_python_and_jupyter_notebook.html#where-to-learn-more",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "Where to Learn More",
    "text": "Where to Learn More\nPython offers a full range of programming language features, and there is a seemingly endless range of packages for scientific and engineering computations. Here are some suggestions on places you can go for more information on programming for engineering applications in Python.\n\nIntroduction to Python for Science\nThis excellent introduction to python is aimed at undergraduates in science with no programming experience. It is free and available at the following link.\n\nIntroduction to Python for Science\n\n\n\nTutorial Introduction to Python for Science and Engineering\nThe following text is available on Amazon. Resources for this book are available on github.\n\nA Primer on Scientific Programming with Python (Fourth Edition) by Hans Petter Langtangen. Resources for this book are available on github.\n\npycse is a package of python functions, examples, and document prepared by John Kitchin at Carnegie Mellon University.\n\npycse - Python Computations in Science and Engineering by John Kitchin at Carnegie Mellon. This is a link into the the github repository for pycse, click on the Raw button to download the .pdf file.\n\nAnd there is plenty more! Google it!"
  },
  {
    "objectID": "getting_started_with_python_and_jupyter_notebook.html#python-basics",
    "href": "getting_started_with_python_and_jupyter_notebook.html#python-basics",
    "title": "Getting Started with Python and Jupyter Notebooks",
    "section": "Python Basics",
    "text": "Python Basics\nThis second part of the notebook is to describe some more Python concepts that will be used during the class.\n\nVariables\n\n#A variable stores a piece of data and gives it a name\nanswer = 42\n\n#answer contained an integer because we gave it an integer!\n\nis_it_thursday = True\nis_it_wednesday = False\n\n#these both are 'booleans' or true/false values\n\npi_approx = 3.1415\n\n#This will be a floating point number, or a number containing digits after the decimal point\n\nmy_name = \"Andrea\"\n#This is a string datatype, the name coming from a string of characters\n\n#Data doesn't have to be a singular unit\n\n#p.s., we can print all of these with a print command. For Example:\nprint(answer)\nprint(pi_approx)\n\n42\n3.1415\n\n\n\n\nMore Complicated Data Types\n\n#What if we want to store many integers? We need a list!\nprices = [10, 20, 30, 40, 50]\n\n#This is a way to define a list in place. We can also make an empty list and add to it.\ncolors = []\n\ncolors.append(\"Green\")\ncolors.append(\"Blue\")\ncolors.append(\"Red\")\n\nprint(colors)\n\n#We can also add unlike data to a list\nprices.append(\"Sixty\")\n\n#As an exercise, look up lists in python and find out how to add in the middle of a list!\n\nprint(prices)\n#We can access a specific element of a list too:\n\nprint(colors[0])\nprint(colors[2])\n\n#Notice here how the first element of the list is index 0, not 1! \n#Languages like MATLAB are 1 indexed, be careful!\n\n#In addition to lists, there are tuples\n#Tuples behave very similarly to lists except that you can't change them \n# after you make them\n\n#An empty Tuple isn't very useful:\nempty_tuple = ()\n\n#Nor is a tuple with just one value:\none_tuple = (\"first\",)\n\n#But tuples with many values are useful:\nrosa_parks_info = (\"Rosa\", \"Parks\", 1913, \"February\", 4)\n\n#You can access tuples just like lists\nprint(rosa_parks_info[0] + \" \" + rosa_parks_info[1])\n\n# You cannot modify existing tuples, but you can make new tuples that extend \n# the information.\n# I expect Tuples to come up less than lists. So we'll just leave it at that.\n\n['Green', 'Blue', 'Red']\n[10, 20, 30, 40, 50, 'Sixty']\nGreen\nRed\nRosa Parks\n\n\n\n\nUsing variables\n\nfloat1 = 5.75\nfloat2 = 2.25\n#Addition, subtraction, multiplication, division are as you expect\n\nprint(float1 + float2)\nprint(float1 - float2)\nprint(float1 * float2)\nprint(float1 / float2)\n\n#Here's an interesting one that showed up in the first homework in 2017. Modulus: \nprint(5 % 2)\n\n8.0\n3.5\n12.9375\n2.5555555555555554\n1\n\n\n\n\nImporting in Python\n\n#Just about every standard math function on a calculator has a python equivalent pre made.\n#however, they are from the 'math' package in python. Let's add that package!\nimport math\nprint(math.log(float1))\nprint(math.exp(float2))\nprint(math.pow(2,5))\n# There is a quicker way to write exponents if you want:\nprint(2.0**5.0)\n\n#Like in MATLAB, you can expand the math to entire lists\nlist3 = [1, 2, 3, 4, 5]\nprint(2 * list3)\n\n1.749199854809259\n9.487735836358526\n32.0\n32.0\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\n\n# We can plot easily in Python like in matlab, just import the relevant package!\nimport matplotlib.pyplot as plt\n\nx_vals = [-2, -1, 0, 1, 2]\ny_vals = [-4, -2, 0, 2, 4]\nplt.plot(x_vals, y_vals)\n\n\n\n\n\n\nLoops\n\n#Repeat code until a conditional statement ends the loop\n\n#Let's try printing a list\nfib = [1, 1, 2, 3, 5, 8]\n\n#While loops are the basic type\ni = 0\nwhile(i < len(fib)):\n    print(fib[i])\n    i = i + 1\n    \n#In matlab, to do the same thing you would have the conditional as: counter < (length(fib) + 1)\n#This is because matlab starts indexing at 1, and python starts at 0.\n    \n#The above type of loop is so common that the 'for' loop is the way to write it faster.\n\nprint(\"Let's try that again\")\n#This is most similar to for loops in matlab\nfor i in range(0, len(fib)) :\n    print(fib[i])\n\nprint(\"One more time:\")\n#Or you can do so even neater\nfor e in fib:\n    print(e)\n\n1\n1\n2\n3\n5\n8\nLet's try that again\n1\n1\n2\n3\n5\n8\nOne more time:\n1\n1\n2\n3\n5\n8\n\n\n\n\nFunctions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\ndef my_function():\n    print(\"Hello from a function\")\n\nTo call a function, use the function name followed by parenthesis:\n\nmy_function()\n\nHello from a function\n\n\nInformation can be passed into functions as arguments.\nArguments are specified after the function name, inside the parentheses. You can add as many arguments as you want, just separate them with a comma.\nThe following example has a function with one argument (fname). When the function is called, we pass along a first name, which is used inside the function to print the full name:\n\ndef my_function(fname):\n    print(fname + \" Refsnes\")\n\nmy_function(\"Emil\")\nmy_function(\"Tobias\")\nmy_function(\"Linus\")\n\nEmil Refsnes\nTobias Refsnes\nLinus Refsnes\n\n\nYou can send any data types of argument to a function (string, number, list, dictionary etc.), and it will be treated as the same data type inside the function.\nE.g. if you send a List as an argument, it will still be a List when it reaches the function:\n\ndef my_function(food):\n    for x in food:\n        print(x)\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nmy_function(fruits)\n\napple\nbanana\ncherry\n\n\nTo let a function return a value, use the return statement:\n\ndef my_function(x):\n    return 5 * x\n\nprint(my_function(3))\nprint(my_function(5))\nprint(my_function(9))\n\n15\n25\n45\n\n\n\n\nClasses\nA class is a user-defined blueprint or prototype from which objects are created. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by their class) for modifying their state.\nTo understand the need for creating a class let’s consider an example, let’s say you wanted to track the number of dogs that may have different attributes like breed, age. If a list is used, the first element could be the dog’s breed while the second element could represent its age. Let’s suppose there are 100 different dogs, then how would you know which element is supposed to be which? What if you wanted to add other properties to these dogs? This lacks organization and it’s why we need classes.\nClass creates a user-defined data structure, which holds its own data members and member functions, which can be accessed and used by creating an instance of that class. A class is like a blueprint for an object.\nIt’s not hard to define Python class. To do so, you’ll need the class keyword:\nclass ClassName:     # Statement-1     .     .     .     # Statement-N\nFor example\n\nclass Example:    \n    variable = 123\n\nIf you run the above code in a Python environment, you’ll find you can call Example.variable to return an integer value.\n\nExample.variable\n\n123\n\n\nThis is an example of a class for data-only objects, but it’s equally easy to define a class that returns a function object by adding the def keyword to your code:\n\nclass Example:\n    def b(self):\n        return \"this is an example class\"\n\n\nExample.b # we are accessing the function...this is probably not what we want to do..\n\n<function __main__.Example.b(self)>\n\n\nWe need a few more concepts:\n\n\nSome more class concepts\nAn Object is an instance of a Class. A class is like a blueprint while an instance is a copy of the class with actual values. It’s not an idea anymore, it’s an actual dog, like a dog of breed pug who’s seven years old. You can have many dogs to create many different instances, but without the class as a guide, you would be lost, not knowing what information is required. An object consists of :\n\nState: It is represented by the attributes of an object. It also reflects the properties of an object.\nBehavior: It is represented by the methods of an object. It also reflects the response of an object to other objects.\nIdentity: It gives a unique name to an object and enables one object to interact with other objects.\n\n\n\nDeclaring Objects (Also called instantiating a class)\nWhen an object of a class is created, the class is said to be instantiated. All the instances share the attributes and the behavior of the class. But the values of those attributes, i.e. the state are unique for each object. A single class may have any number of instances.\nExample:\n\n\n\n\n\n\nclass Dog:\n     \n    # A simple class\n    # attribute\n    attr1 = \"mammal\"\n    attr2 = \"dog\"\n \n    # A sample method \n    def fun(self):\n        print(\"I'm a\", self.attr1)\n        print(\"I'm a\", self.attr2)\n\n# Object instantiation\nRodger = Dog()\n \n# Accessing class attributes\n# and method through objects\nprint(Rodger.attr1)\nRodger.fun()\n\nmammal\nI'm a mammal\nI'm a dog\n\n\n\n\nSelf\nClass methods must have an extra first parameter in the method definition. We do not give a value for this parameter when we call the method, Python provides it.\nIf we have a method that takes no arguments, then we still have to have one argument.\nWhen we call a method of this object as myobject.method(arg1, arg2), this is automatically converted by Python into MyClass.method(myobject, arg1, arg2).\nNote that this means that inside the function method (in our example) we now have access to the instance of the class! so we can access its variables, etc.\n\n\n__init__ method\nThe init method is similar to constructors in C++, it constructs the object and can be used to initialise the object’s state.\nLike methods, a constructor also contains a collection of statements (i.e. instructions) that are executed when the object is created.\nThe __init__ method runs as soon as an object of a class is instantiated. The method is useful to do any initialization you want to do with your object.\n\n# A Sample class with init method\nclass Person:\n\n    # init method or constructor\n    def __init__(self, name):\n        self.name = name\n\n    # Sample Method\n    def say_hi(self):\n        print('Hello, my name is', self.name)\n\np = Person('Nikhil') # as soon as we do this, the __init__ method is called.\np.say_hi()\n\nHello, my name is Nikhil\n\n\n\n\nClass and Instance Variables\n\nInstance variables are used to store data that is unique to each instance of the class. Instance variables are variables whose value is assigned inside the __init__ method or inside a class method (a method with the argument self)\nClass variables are for attributes and methods shared by all instances of the class. Class variables are variables whose value is assigned directly in the class.\n\n\n# Class for Dog\nclass Dog:\n   \n    # Class Variable\n    animal = 'dog'            \n   \n    # The init method or constructor\n    def __init__(self, breed, color):\n     \n        # Instance Variable    \n        self.breed = breed\n        self.color = color       \n    \n# Objects of Dog class\nRodger = Dog(\"Pug\", \"brown\")\nBuzo = Dog(\"Bulldog\", \"black\")\n \nprint('Rodger details:')  \nprint('Rodger is a', Rodger.animal)\nprint('Breed: ', Rodger.breed)\nprint('Color: ', Rodger.color)\n \nprint('\\nBuzo details:')  \nprint('Buzo is a', Buzo.animal)\nprint('Breed: ', Buzo.breed)\nprint('Color: ', Buzo.color)\n \n# Class variables can be accessed using class\n# name also\nprint(\"\\nAccessing class variable using class name\")\nprint(Dog.animal)\n\nRodger details:\nRodger is a dog\nBreed:  Pug\nColor:  brown\n\nBuzo details:\nBuzo is a dog\nBreed:  Bulldog\nColor:  black\n\nAccessing class variable using class name\ndog\n\n\n\n\nAdditional Resources\n\nCode Academy\nOfficial Python Reference\nReal Python\n\nGoogle Colab - An Introduction to Google Colab, McGraw Center for Teaching and Learning - Getting Started with Google Colab - Colab Walkthrough, Stanford University - Google Colab Tutorial for Data Scientists, Datacamp.com"
  }
]